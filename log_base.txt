/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor(310.7826)
tensor(310.7826)
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor(310.7826)
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor(310.7826)
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
[19:55:49.952727] job dir: /home/s2/youngjoonjeong/github/ltrp
[19:55:49.952907] Namespace(accum_iter=1,
asymmetric=True,
batch_size=128,
blr=0.0003,
burning_in=0,
data_path='/shared/s2/lab01/dataset/imagenet/images',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=400,
focused_rank_k=10,
gpu=0,
img_metric='',
input_size=224,
list_mle_k=20,
local_rank=0,
log_dir=None,
low_shot='',
lr=None,
ltr_loss='list_mleEx',
mask_all=False,
mask_ratio=0.9,
min_lr=0.0,
model='ltrp_base_and_vs',
model_ckpt='/shared/s2/lab01/youngjoonjeong/dino_wm_oc/outputs/wall_dinovits_full_nope/checkpoints/model_latest.pth',
norm_pix_loss=False,
num_workers=20,
output_dir='/shared/s2/lab01/youngjoonjeong/ltrp_test/output_dir',
pin_mem=True,
pretrained_from='',
rank=0,
rank_net_sigma=1,
rank_net_t=0.001,
resume='',
resume_from_mae='',
resume_score_net='',
save_ckpt_freq=20,
score_net='vit_small',
score_net_depth=12,
seed=0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.06,
world_size=4)
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[19:55:52.828184] INFO: dataset_train len:  1281167
[19:55:52.828288] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ff23e8860a0>
[19:55:52.828306] Sampler len: 320292
[19:55:52.902572] Sampler sample indices (first 10): [1069931, 899707, 337725, 1249538, 782342, 269008, 436576, 186495, 1001350, 968210]
[19:55:52.903071] INFO: len(data_loader_train) = 2503
[19:55:54.049049] get_score_net  ltrp_cluster_vs
[19:55:55.563583] score net is ltrp_cluster(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (pre_logits): Identity()
  (head): Linear(in_features=384, out_features=196, bias=True)
)
[19:55:55.564918] INFO: self.score_net ltrp_cluster(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (pre_logits): Identity()
  (head): Linear(in_features=384, out_features=196, bias=True)
)
[19:55:55.566441] Total trainable parameters: 21665476
[19:55:55.651715] Model = LearnToRankPatchMIM(
  (mim): MaskedAutoencoderViT(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (decoder_embed): Linear(in_features=768, out_features=512, bias=True)
    (decoder_blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=512, out_features=1536, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
    (decoder_pred): Linear(in_features=512, out_features=768, bias=True)
  )
  (score_net): ltrp_cluster(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=384, out_features=196, bias=True)
  )
  (criterion): list_mleEx()
)
[19:55:55.651805] base lr: 3.00e-04
[19:55:55.651818] actual lr: 6.00e-04
[19:55:55.651829] accumulate grad iterations: 1
[19:55:55.651839] effective batch size: 512
[19:55:55.651856] ngpus_per_node  4
[19:55:55.669378] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0006
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0006
    maximize: False
    weight_decay: 0.06
)
[19:55:55.669486] Start training for 400 epochs
[19:56:26.723211] INFO: samples:  tensor([[[[ 1.0844,  0.6563, -0.1314,  ...,  0.9817,  0.4508,  0.5707],
          [ 0.4166,  1.0331, -0.8164,  ...,  0.9988,  0.9474,  0.5022],
          [-0.3541, -0.3883, -0.0116,  ...,  0.5707,  0.8618,  0.8618],
          ...,
          [ 1.0844,  1.2899,  1.1187,  ...,  0.7762,  0.8276,  0.8447],
          [ 1.3070,  1.5468,  0.5364,  ...,  0.6734,  0.6906,  0.5878],
          [ 1.4954,  1.0159,  1.1187,  ...,  0.8276,  0.5878,  0.6392]],

         [[ 0.7479,  0.3102, -0.4426,  ...,  0.8529,  0.2927,  0.3627],
          [ 0.0651,  0.7129, -1.1604,  ...,  0.8179,  0.7829,  0.2927],
          [-0.7227, -0.7402, -0.3375,  ...,  0.3452,  0.6429,  0.6254],
          ...,
          [ 0.9405,  1.1681,  1.0105,  ...,  0.4503,  0.5028,  0.5203],
          [ 1.1856,  1.4482,  0.4153,  ...,  0.3627,  0.3627,  0.2577],
          [ 1.3957,  0.9055,  1.0105,  ...,  0.5203,  0.2577,  0.3102]],

         [[ 0.2522, -0.1661, -0.8458,  ...,  0.3568, -0.2358, -0.0964],
          [-0.4101,  0.2522, -1.2641,  ...,  0.2173,  0.1302, -0.3404],
          [-1.1770, -1.1596, -0.7413,  ..., -0.3055, -0.0615, -0.0615],
          ...,
          [ 0.5834,  0.8099,  0.6879,  ..., -0.2881, -0.2358, -0.2184],
          [ 0.8274,  1.1062,  0.1128,  ..., -0.3753, -0.3753, -0.4798],
          [ 1.0714,  0.5834,  0.6879,  ..., -0.2184, -0.4798, -0.4275]]],


        [[[ 1.0159,  1.0331,  1.0844,  ...,  1.2385,  1.2385,  1.2385],
          [ 1.0673,  1.0844,  1.1529,  ...,  1.1872,  1.2043,  1.2385],
          [ 1.1358,  1.1358,  1.2043,  ...,  1.2214,  1.2557,  1.2214],
          ...,
          [ 1.0159,  0.9817,  0.9646,  ...,  1.3755,  1.3927,  1.4269],
          [ 0.9474,  0.9817,  1.0673,  ...,  1.4440,  1.4783,  1.5125],
          [ 0.9646,  1.0502,  1.1700,  ...,  1.5125,  1.5468,  1.5468]],

         [[ 1.3256,  1.3431,  1.3431,  ...,  1.5007,  1.5007,  1.5007],
          [ 1.4307,  1.3957,  1.4307,  ...,  1.5007,  1.5182,  1.5007],
          [ 1.5007,  1.4657,  1.4832,  ...,  1.5182,  1.5007,  1.4832],
          ...,
          [ 1.5007,  1.4657,  1.5007,  ...,  1.6758,  1.6408,  1.6408],
          [ 1.4657,  1.4482,  1.5007,  ...,  1.7108,  1.6933,  1.6933],
          [ 1.4832,  1.5182,  1.5882,  ...,  1.7283,  1.7458,  1.7458]],

         [[ 1.4548,  1.4722,  1.4722,  ...,  1.6465,  1.6465,  1.6465],
          [ 1.4897,  1.4897,  1.5245,  ...,  1.6640,  1.6814,  1.6465],
          [ 1.5420,  1.5245,  1.5420,  ...,  1.6814,  1.6465,  1.6291],
          ...,
          [ 1.7511,  1.7511,  1.7860,  ...,  1.8034,  1.7860,  1.8034],
          [ 1.7337,  1.7511,  1.7860,  ...,  1.8208,  1.8208,  1.8383],
          [ 1.7511,  1.8034,  1.8731,  ...,  1.8557,  1.8731,  1.8731]]],


        [[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.1462,  2.0948],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.1975,  2.1462,  2.1290],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.0092,  2.1462,  2.2147],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],

         [[ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.3235,  2.2710],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.3761,  2.3235,  2.3060],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.1835,  2.3235,  2.3936],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],

         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.5354,  2.4831],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.5877,  2.5354,  2.5180],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.3960,  2.5354,  2.6051],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]],


        ...,


        [[[-0.9363,  0.9817,  0.2453,  ...,  0.7077,  0.8447,  0.9817],
          [-0.6452, -0.0458,  0.0227,  ...,  0.9474,  0.9132,  0.8447],
          [-0.5082, -0.1486, -0.7650,  ...,  0.8104,  0.8104,  0.8104],
          ...,
          [-0.8507, -0.9705, -1.0048,  ...,  1.6153,  1.6153,  1.5468],
          [-0.7479, -0.8849, -0.9534,  ...,  1.6838,  1.6667,  1.5982],
          [-0.6623, -0.6794, -0.7822,  ...,  1.6153,  1.6324,  1.6153]],

         [[-0.8277,  1.1331,  0.3803,  ...,  0.1877,  0.2052,  0.2752],
          [-0.5301,  0.0826,  0.1527,  ...,  0.5378,  0.5553,  0.5028],
          [-0.3901, -0.0224, -0.6527,  ...,  0.3978,  0.4678,  0.5378],
          ...,
          [-1.1429, -1.2129, -1.1954,  ...,  1.6057,  1.6232,  1.5532],
          [-1.0728, -1.0903, -1.1078,  ...,  1.6408,  1.6583,  1.6057],
          [-1.0028, -1.0203, -0.9328,  ...,  1.5707,  1.5707,  1.5532]],

         [[-0.6018,  1.3502,  0.6008,  ...,  0.0605, -0.0441, -0.0615],
          [-0.3230,  0.2696,  0.3393,  ...,  0.3916,  0.3916,  0.2871],
          [-0.2010,  0.1651, -0.4624,  ...,  0.2522,  0.3045,  0.3045],
          ...,
          [-0.9156, -1.0027, -1.0376,  ...,  1.6640,  1.6814,  1.5942],
          [-0.8458, -0.8981, -0.9678,  ...,  1.6988,  1.7163,  1.6640],
          [-0.7936, -0.7413, -0.6541,  ...,  1.5942,  1.6465,  1.6640]]],


        [[[ 0.0398,  0.0398,  0.0227,  ..., -0.0629, -0.0629, -0.0972],
          [ 0.0569,  0.0741,  0.0227,  ..., -0.0458, -0.0629, -0.0972],
          [ 0.0569,  0.0741,  0.0569,  ..., -0.0458, -0.0801, -0.0629],
          ...,
          [ 1.3413,  1.3070,  1.3242,  ...,  0.8961,  1.0331,  1.1700],
          [ 1.3413,  1.3413,  1.3070,  ...,  1.2043,  1.1872,  1.2043],
          [ 1.4098,  1.3413,  1.2557,  ...,  1.2043,  1.2043,  1.2385]],

         [[ 0.4853,  0.4853,  0.4678,  ...,  0.3978,  0.3978,  0.3627],
          [ 0.5203,  0.5203,  0.4678,  ...,  0.4153,  0.3803,  0.3803],
          [ 0.5203,  0.5378,  0.5028,  ...,  0.3803,  0.3803,  0.3978],
          ...,
          [ 1.0805,  1.0280,  1.0630,  ...,  0.4503,  0.6429,  0.8354],
          [ 1.0980,  1.1155,  1.0455,  ...,  0.9230,  0.8880,  0.9405],
          [ 1.1856,  1.1155,  1.0280,  ...,  0.9405,  0.9055,  0.9580]],

         [[ 1.4025,  1.4025,  1.4025,  ...,  1.3677,  1.3677,  1.3851],
          [ 1.3851,  1.4200,  1.4025,  ...,  1.3851,  1.3851,  1.3677],
          [ 1.3851,  1.4025,  1.3677,  ...,  1.3677,  1.3502,  1.3502],
          ...,
          [ 0.8448,  0.7576,  0.8274,  ...,  0.1651,  0.4265,  0.6008],
          [ 0.8622,  0.8797,  0.8099,  ...,  0.7402,  0.6879,  0.7228],
          [ 1.0191,  0.9668,  0.8274,  ...,  0.7751,  0.7402,  0.8099]]],


        [[[-2.0494, -2.0494, -2.0323,  ..., -1.4672, -1.1760, -0.9534],
          [-2.0152, -2.0152, -1.9980,  ..., -1.0048, -1.0733, -1.4672],
          [-2.0152, -2.0152, -1.9809,  ..., -0.9877, -1.5870, -1.6384],
          ...,
          [-2.0665, -2.0323, -1.9467,  ...,  1.7352,  1.9749,  2.0605],
          [-2.0323, -2.0494, -1.9980,  ...,  2.1804,  2.1633,  2.1290],
          [-2.0323, -2.0323, -2.0323,  ...,  2.1633,  2.0948,  2.0605]],

         [[-1.8782, -1.8782, -1.8606,  ..., -1.5805, -1.2479, -1.0203],
          [-1.8431, -1.8431, -1.8256,  ..., -1.0728, -1.1429, -1.5105],
          [-1.8431, -1.8431, -1.8081,  ..., -1.0378, -1.6331, -1.7031],
          ...,
          [-1.9832, -1.9482, -1.8606,  ...,  1.5007,  1.7633,  1.8333],
          [-1.9482, -1.9832, -1.9307,  ...,  2.0959,  2.0084,  1.8859],
          [-1.9657, -1.9657, -1.9657,  ...,  2.0959,  1.9384,  1.8158]],

         [[-1.7173, -1.7173, -1.6999,  ..., -1.5604, -1.2119, -1.0027],
          [-1.6824, -1.6824, -1.6650,  ..., -1.0724, -1.1073, -1.4733],
          [-1.6824, -1.6824, -1.6476,  ..., -1.0201, -1.6127, -1.6476],
          ...,
          [-1.7696, -1.7347, -1.6650,  ...,  1.1759,  1.4200,  1.5071],
          [-1.7522, -1.7870, -1.7522,  ...,  1.8208,  1.7337,  1.6117],
          [-1.7870, -1.7870, -1.8044,  ...,  1.8905,  1.6988,  1.5420]]]])
[19:56:26.777577] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[19:56:29.819413] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:56:31.143316] INFO: mask:  tensor([[ True,  True, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True, False,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:56:31.354276] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.354588] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.354956] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.355412] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.355877] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.356368] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.356824] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.357287] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.357746] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.358213] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.358679] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.359150] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.359613] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.360076] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.360541] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.361007] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.361526] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.362001] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:31.362520] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[19:56:52.821584] Epoch: [0]  [   0/2503]  eta: 1 day, 15:44:06  lr: 0.000000  loss: 48.4919 (48.4919)  time: 57.1500  data: 31.0520  max mem: 3169
[19:56:52.821861] INFO: samples:  tensor([[[[-1.1418, -0.8507,  0.4679,  ...,  0.8276,  0.8618,  0.9303],
          [-0.1314, -0.6109, -0.7822,  ...,  0.8104,  0.6906,  0.3652],
          [-0.4568,  0.2796, -0.6452,  ...,  0.8447,  0.9132,  0.4679],
          ...,
          [ 0.4851,  0.3309,  0.0569,  ...,  0.0056, -1.7412, -2.0494],
          [ 0.2796,  0.3481,  0.2453,  ...,  0.2796, -1.1760, -2.0323],
          [ 0.5364,  0.2967,  0.2967,  ...,  0.6563, -0.5253, -2.0494]],

         [[-1.0903, -0.8102,  0.5378,  ...,  0.7479,  0.7129,  0.8529],
          [-0.1099, -0.5826, -0.7052,  ...,  0.6954,  0.5728,  0.1352],
          [-0.4251,  0.3102, -0.6176,  ...,  0.6429,  0.7304,  0.1702],
          ...,
          [ 0.1001, -0.0574, -0.3375,  ..., -0.1275, -1.6331, -2.0357],
          [-0.1099, -0.0399, -0.1450,  ...,  0.3102, -1.2129, -1.9132],
          [ 0.1527, -0.0924, -0.0924,  ...,  0.4853, -0.2850, -1.9307]],

         [[-1.0027, -0.6715,  0.8448,  ...,  0.1128,  0.3045,  0.3568],
          [ 0.2696, -0.3578, -0.6367,  ...,  0.0779,  0.0605, -0.6541],
          [-0.1487,  0.6531, -0.3753,  ...,  0.0431,  0.3916, -0.5147],
          ...,
          [ 0.1825, -0.0092, -0.3055,  ..., -0.7413, -1.6476, -1.5779],
          [-0.0441,  0.0082, -0.1138,  ..., -0.5844, -1.5779, -1.5953],
          [ 0.1999, -0.0441, -0.0790,  ..., -0.1835, -0.0615, -1.6824]]],


        [[[ 0.5707,  0.4337,  1.6667,  ..., -0.4397, -0.3027, -0.2513],
          [ 0.8276,  0.6392,  0.3481,  ..., -0.4911, -0.4739, -0.5253],
          [ 1.0844,  0.7248, -0.5424,  ..., -0.4054, -0.4226, -0.4739],
          ...,
          [-0.3541,  0.0398,  0.2624,  ..., -0.5938, -0.5767, -0.3198],
          [-0.3712, -0.0116,  0.2967,  ..., -0.6452, -0.6452, -0.6794],
          [-0.3541, -0.0458,  0.2967,  ..., -0.5596, -0.7822, -0.5938]],

         [[ 0.5203,  0.5378,  1.7458,  ...,  0.0301,  0.1001,  0.0651],
          [ 0.7129,  0.5028,  0.1702,  ...,  0.0126,  0.0126, -0.0399],
          [ 0.5203,  0.3277, -0.7227,  ..., -0.0049, -0.0224, -0.0749],
          ...,
          [ 0.2402,  0.7654,  0.7479,  ..., -0.8277, -0.8452, -0.6001],
          [ 0.2052,  0.6254,  0.9230,  ..., -0.8452, -0.8277, -0.7927],
          [ 0.2752,  0.5203,  0.9405,  ..., -0.8277, -1.0203, -0.8102]],

         [[ 0.5834,  0.4962,  1.9080,  ..., -0.8633, -0.8284, -0.8458],
          [ 0.7228,  0.5659,  0.4265,  ..., -0.8284, -0.8458, -0.8633],
          [ 0.3393,  0.0431, -0.7238,  ..., -0.8284, -0.9330, -0.9330],
          ...,
          [-0.5670, -0.2184,  0.3568,  ..., -0.8633, -0.9853, -0.7936],
          [-0.3927, -0.2184,  0.3219,  ..., -0.9156, -0.9330, -0.9504],
          [-0.4450, -0.3404,  0.2348,  ..., -0.8807, -1.1073, -0.8284]]],


        [[[-0.9705, -0.9363, -0.8849,  ..., -1.0904, -0.8849, -1.0390],
          [-0.9877, -0.9192, -0.8678,  ..., -1.1247, -0.8849, -1.0562],
          [-0.9192, -0.8849, -0.8507,  ..., -1.0904, -0.8849, -1.0733],
          ...,
          [-1.3987, -1.3987, -1.3987,  ..., -1.4843, -1.3302, -1.3987],
          [-1.4158, -1.4158, -1.4158,  ..., -1.4672, -1.3130, -1.3987],
          [-1.4158, -1.4158, -1.4158,  ..., -1.4158, -1.2788, -1.3987]],

         [[-0.8803, -0.8452, -0.7927,  ..., -0.9853, -0.7752, -0.9328],
          [-0.8978, -0.8277, -0.7752,  ..., -1.0203, -0.7752, -0.9503],
          [-0.8277, -0.7927, -0.7577,  ..., -0.9853, -0.7752, -0.9678],
          ...,
          [-1.3179, -1.3179, -1.3179,  ..., -1.3880, -1.2304, -1.3004],
          [-1.3354, -1.3354, -1.3354,  ..., -1.3704, -1.2129, -1.3004],
          [-1.3354, -1.3354, -1.3354,  ..., -1.3179, -1.1779, -1.3004]],

         [[-0.5670, -0.5321, -0.4798,  ..., -0.7238, -0.5147, -0.6715],
          [-0.5844, -0.5147, -0.4624,  ..., -0.7761, -0.5147, -0.6890],
          [-0.5147, -0.4798, -0.4450,  ..., -0.7413, -0.5147, -0.7064],
          ...,
          [-1.1596, -1.1596, -1.1596,  ..., -1.1247, -0.9678, -1.0376],
          [-1.1770, -1.1770, -1.1770,  ..., -1.1073, -0.9504, -1.0376],
          [-1.1770, -1.1770, -1.1770,  ..., -1.0550, -0.9156, -1.0376]]],


        ...,


        [[[-0.3198, -0.4568, -0.4911,  ..., -0.3712, -0.4397, -0.4397],
          [-0.3198, -0.4397, -0.5767,  ..., -0.3883, -0.4568, -0.4397],
          [-0.6109, -0.6452, -0.5938,  ..., -0.3369, -0.3712, -0.3712],
          ...,
          [-1.2959, -1.2959, -1.2617,  ...,  1.4098,  1.3242,  1.4783],
          [-0.7650, -1.4329, -1.2274,  ...,  1.7009,  1.5125,  1.5810],
          [-0.7479, -1.5014, -1.2959,  ...,  1.7352,  1.6838,  1.6153]],

         [[-0.5826, -0.6527, -0.6352,  ..., -1.3529, -1.3179, -1.2654],
          [-0.6176, -0.6702, -0.7577,  ..., -1.3529, -1.3880, -1.2654],
          [-0.9328, -0.8627, -0.7927,  ..., -1.3354, -1.4055, -1.3004],
          ...,
          [-1.3004, -1.3354, -1.4230,  ...,  0.4153,  0.3627,  0.4153],
          [-0.8277, -1.5105, -1.4055,  ...,  0.9055,  0.6779,  0.6078],
          [-0.8627, -1.6331, -1.5105,  ...,  1.0630,  0.8529,  0.6954]],

         [[-0.8110, -0.9156, -0.9330,  ..., -1.7522, -1.7347, -1.7522],
          [-0.8458, -0.8981, -0.9678,  ..., -1.7870, -1.7870, -1.7347],
          [-1.1073, -1.0376, -0.9330,  ..., -1.7696, -1.8044, -1.7522],
          ...,
          [-1.3861, -1.4210, -1.4733,  ..., -0.7936, -0.8110, -0.5670],
          [-0.9156, -1.5779, -1.4036,  ..., -0.3927, -0.4973, -0.4275],
          [-1.0376, -1.6302, -1.4384,  ..., -0.1312, -0.1487, -0.2532]]],


        [[[ 0.3994,  0.9132,  0.7933,  ...,  1.7694,  1.4098,  0.5022],
          [-0.5596,  0.0056,  0.3652,  ...,  2.0092,  1.8722,  1.5468],
          [-0.0801, -0.3369, -0.2342,  ...,  2.0777,  1.9749,  1.7694],
          ...,
          [ 1.1015,  1.1187,  0.7591,  ..., -0.6623, -0.7479, -0.7479],
          [ 1.3584,  0.8104,  0.5878,  ..., -0.5938, -0.6794, -0.7308],
          [ 0.9988,  0.8447,  0.5022,  ..., -0.5253, -0.6109, -0.6965]],

         [[ 0.2577,  0.7829,  0.6779,  ...,  1.7983,  1.2906,  0.3102],
          [-0.7752, -0.1625,  0.2227,  ...,  2.0609,  1.8158,  1.4307],
          [-0.2500, -0.5476, -0.4251,  ...,  2.2185,  2.0084,  1.7108],
          ...,
          [ 1.0105,  0.9580,  0.6078,  ..., -0.7752, -0.8627, -0.8627],
          [ 1.2381,  0.6429,  0.4153,  ..., -0.7052, -0.7927, -0.8452],
          [ 0.7304,  0.6779,  0.3277,  ..., -0.6176, -0.7227, -0.8102]],

         [[ 0.5136,  0.9319,  0.8797,  ...,  1.8557,  1.3851,  0.4614],
          [-0.1835,  0.1999,  0.4788,  ...,  2.1868,  1.9080,  1.5420],
          [ 0.1825, -0.0790,  0.0779,  ...,  2.3263,  2.0997,  1.7860],
          ...,
          [ 1.0714,  0.9668,  0.6008,  ..., -0.6890, -0.7413, -0.7413],
          [ 1.3328,  0.6705,  0.4614,  ..., -0.6193, -0.6890, -0.7238],
          [ 0.7925,  0.7228,  0.3916,  ..., -0.5147, -0.6018, -0.6890]]],


        [[[-1.5185, -1.4158, -1.6727,  ...,  1.3413,  0.9817,  0.7248],
          [-0.3712, -0.6281, -1.4329,  ...,  1.3584,  0.9474,  1.0844],
          [ 0.2967, -0.8335, -1.1932,  ...,  1.0159,  0.3481,  0.8447],
          ...,
          [ 1.3070,  0.8276,  1.2385,  ...,  1.2899,  1.2899,  1.2899],
          [ 1.3413,  1.3070,  1.2728,  ...,  1.2899,  1.2899,  1.2899],
          [ 1.1358,  1.3242,  1.2557,  ...,  1.2899,  1.2899,  1.2899]],

         [[-1.6155, -1.4580, -1.6506,  ...,  1.4482,  1.0805,  0.8179],
          [-0.3375, -0.6527, -1.5280,  ...,  1.4657,  1.0630,  1.2206],
          [ 0.3102, -0.8978, -1.3354,  ...,  1.1155,  0.4328,  0.9055],
          ...,
          [ 1.3606,  0.8880,  1.3782,  ...,  1.4307,  1.4307,  1.4307],
          [ 1.4307,  1.4132,  1.4132,  ...,  1.4307,  1.4307,  1.4307],
          [ 1.2906,  1.4832,  1.4307,  ...,  1.4307,  1.4307,  1.4307]],

         [[-0.6193, -0.3578, -0.7238,  ...,  1.6465,  1.1585,  0.7576],
          [ 0.5485,  0.5311, -0.0441,  ...,  1.5768,  1.0191,  1.0191],
          [ 1.6465,  0.7054,  0.4788,  ...,  1.1062,  0.4265,  0.8622],
          ...,
          [ 1.9254,  1.5245,  1.7163,  ...,  1.5594,  1.5594,  1.5594],
          [ 2.0125,  2.0474,  1.7511,  ...,  1.5594,  1.5594,  1.5594],
          [ 1.7163,  1.9428,  1.7163,  ...,  1.5594,  1.5594,  1.5594]]]])
[19:56:52.828318] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:56:52.929644] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:56:54.264505] INFO: mask:  tensor([[False,  True,  True,  ...,  True,  True,  True],
        [ True, False,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:56:54.476231] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.476502] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.476923] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.477394] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.477859] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.478324] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.478813] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.479282] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.479750] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.480214] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.480683] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.481149] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.481617] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.482083] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.482548] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.483024] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.483492] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.483956] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.484422] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:54.601601] INFO: samples:  tensor([[[[ 0.4508,  0.3994,  0.3481,  ..., -0.6623, -0.6452, -0.7822],
          [ 0.4851,  0.4508,  0.3652,  ..., -0.4911, -0.5767, -0.6965],
          [ 0.4679,  0.4337,  0.3652,  ..., -0.4226, -0.5253, -0.6109],
          ...,
          [ 0.0912,  0.0569,  0.0227,  ..., -0.5082, -0.5253, -0.4911],
          [ 0.1939,  0.2453,  0.1426,  ..., -0.4911, -0.4054, -0.3712],
          [ 0.3481,  0.3652,  0.3138,  ..., -0.4397, -0.4226, -0.3027]],

         [[ 1.4832,  1.5007,  1.5182,  ...,  0.6779,  0.6954,  0.6954],
          [ 1.5182,  1.5357,  1.5532,  ...,  0.6429,  0.6604,  0.6604],
          [ 1.4657,  1.5007,  1.5182,  ...,  0.6078,  0.6254,  0.6604],
          ...,
          [ 1.2031,  1.1681,  1.1506,  ...,  1.2206,  1.1681,  1.1681],
          [ 1.3782,  1.2731,  1.2556,  ...,  1.2381,  1.2731,  1.2731],
          [ 1.4482,  1.4307,  1.3782,  ...,  1.2381,  1.3081,  1.3606]],

         [[ 1.9080,  1.9254,  1.9080,  ..., -0.0615, -0.2010, -0.2532],
          [ 1.8383,  1.9254,  1.9080,  ...,  0.0779, -0.0615, -0.1835],
          [ 1.7860,  1.8905,  1.9080,  ...,  0.1999,  0.1128, -0.0615],
          ...,
          [ 0.7751,  0.6879,  0.6356,  ...,  1.4722,  1.3502,  1.3851],
          [ 0.8797,  0.8448,  0.7751,  ...,  1.4374,  1.5071,  1.5420],
          [ 1.0365,  1.0365,  0.9494,  ...,  1.3502,  1.5420,  1.6465]]],


        [[[ 1.7009,  1.7523,  1.7352,  ..., -0.4054, -0.3712, -0.3369],
          [ 1.7009,  1.7523,  1.7694,  ..., -0.4397, -0.3198, -0.4397],
          [ 1.7009,  1.8037,  1.7694,  ..., -0.4397, -0.3198, -0.4568],
          ...,
          [ 0.0056, -0.2856, -0.3883,  ...,  0.6906,  0.6906,  0.7419],
          [ 1.2385,  1.1529,  1.0673,  ...,  0.2796,  0.2282,  0.2282],
          [ 0.5536,  0.5878,  0.3823,  ...,  0.4508,  0.5878,  0.4337]],

         [[ 1.5882,  1.6232,  1.6057,  ..., -0.2850, -0.1975, -0.1800],
          [ 1.5707,  1.6232,  1.5882,  ..., -0.2850, -0.2150, -0.2500],
          [ 1.5882,  1.6232,  1.5882,  ..., -0.2500, -0.3200, -0.2675],
          ...,
          [ 0.3102,  0.0476, -0.0924,  ...,  0.9055,  0.8704,  0.9405],
          [ 1.4657,  1.4307,  1.3256,  ...,  0.4503,  0.4328,  0.4678],
          [ 0.7304,  0.8004,  0.5903,  ...,  0.6604,  0.8004,  0.6604]],

         [[ 1.3502,  1.3851,  1.3328,  ...,  0.0779, -0.1487, -0.0092],
          [ 1.2980,  1.3502,  1.3328,  ...,  0.0431,  0.0082, -0.0615],
          [ 1.3154,  1.3677,  1.3328,  ..., -0.0267,  0.1651, -0.0092],
          ...,
          [ 0.7054,  0.3916,  0.1999,  ...,  1.0888,  1.0888,  1.1411],
          [ 1.7337,  1.6814,  1.6465,  ...,  0.7576,  0.6705,  0.7054],
          [ 0.9319,  1.0365,  0.8622,  ...,  1.0191,  1.0714,  0.9319]]],


        [[[-0.5596, -0.5596, -0.5596,  ..., -0.3027, -0.3027, -0.3027],
          [-0.5767, -0.5767, -0.5767,  ..., -0.3198, -0.3198, -0.3198],
          [-0.5767, -0.5767, -0.5767,  ..., -0.3198, -0.3198, -0.3198],
          ...,
          [-1.0562, -1.0904, -1.1075,  ..., -0.5082, -0.2513, -0.2342],
          [-1.2274, -1.2617, -1.2959,  ..., -0.2684, -0.2684, -0.2856],
          [-1.1589, -1.1247, -1.0733,  ..., -0.0287, -0.1657, -0.5767]],

         [[ 0.3277,  0.3277,  0.3277,  ...,  0.5728,  0.5728,  0.5728],
          [ 0.3102,  0.2927,  0.2927,  ...,  0.5553,  0.5553,  0.5553],
          [ 0.3102,  0.3102,  0.3102,  ...,  0.5553,  0.5553,  0.5553],
          ...,
          [-0.3901, -0.4076, -0.4076,  ..., -0.9328, -0.6527, -0.5301],
          [-0.5826, -0.5826, -0.6001,  ..., -0.6527, -0.6352, -0.5651],
          [-0.5126, -0.4426, -0.3725,  ..., -0.4426, -0.5476, -0.8803]],

         [[ 1.4025,  1.4025,  1.4025,  ...,  1.5420,  1.5420,  1.5420],
          [ 1.3851,  1.4025,  1.4025,  ...,  1.5245,  1.5245,  1.5245],
          [ 1.3851,  1.4025,  1.4025,  ...,  1.5245,  1.5245,  1.5245],
          ...,
          [ 0.3742,  0.3219,  0.3045,  ..., -1.3164, -1.0550, -0.9330],
          [ 0.1825,  0.1476,  0.1128,  ..., -0.9853, -1.0201, -0.9156],
          [ 0.2522,  0.2871,  0.3393,  ..., -0.7587, -0.9156, -1.1770]]],


        ...,


        [[[ 1.3584,  1.3584,  1.3584,  ...,  1.3755,  1.3755,  1.3755],
          [ 1.3927,  1.3755,  1.3755,  ...,  1.3927,  1.3927,  1.3927],
          [ 1.3584,  1.3755,  1.3755,  ...,  1.3927,  1.3927,  1.3927],
          ...,
          [ 2.0777,  2.0092,  2.0092,  ..., -1.8268, -1.7925, -1.7240],
          [ 1.9578,  1.8550,  1.8208,  ..., -1.7583, -1.8097, -1.7069],
          [ 1.7865,  1.6838,  1.6667,  ..., -1.8097, -1.8097, -1.7240]],

         [[ 2.0609,  2.0609,  2.0609,  ...,  2.0784,  2.0784,  2.0784],
          [ 2.0609,  2.0434,  2.0434,  ...,  2.0959,  2.0959,  2.0959],
          [ 2.0434,  2.0609,  2.0609,  ...,  2.0959,  2.0959,  2.0959],
          ...,
          [ 2.0959,  2.0259,  1.9909,  ..., -1.7031, -1.6856, -1.6506],
          [ 1.9734,  1.8683,  1.8158,  ..., -1.6331, -1.7031, -1.6155],
          [ 1.8333,  1.7283,  1.7108,  ..., -1.6856, -1.6856, -1.5630]],

         [[ 2.6226,  2.6226,  2.6226,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6226,  2.6226,  ...,  2.6400,  2.6226,  2.6226],
          [ 2.6051,  2.6226,  2.6400,  ...,  2.6226,  2.6226,  2.6226],
          ...,
          [ 1.9951,  1.8905,  1.8557,  ..., -1.4907, -1.4733, -1.4210],
          [ 1.8383,  1.7337,  1.6988,  ..., -1.4210, -1.4733, -1.3861],
          [ 1.7163,  1.6117,  1.6117,  ..., -1.4384, -1.4733, -1.3687]]],


        [[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 1.4612,  1.5639,  1.6667,  ...,  2.2489,  2.2489,  2.2489],
          [ 0.8447,  0.9646,  1.0844,  ...,  2.2489,  2.2489,  2.2489],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],

         [[ 2.2885,  2.3410,  2.3936,  ...,  2.4286,  2.4286,  2.4286],
          [ 1.4307,  1.5532,  1.6933,  ...,  2.4286,  2.4286,  2.4286],
          [ 0.8179,  0.9405,  1.0805,  ...,  2.4286,  2.4286,  2.4286],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],

         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 1.7337,  1.8383,  1.9428,  ...,  2.6400,  2.6400,  2.6400],
          [ 1.0017,  1.1062,  1.2108,  ...,  2.6400,  2.6400,  2.6400],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]],


        [[[-0.1486, -0.7479,  0.0912,  ..., -0.4568, -0.7308, -0.7137],
          [-0.4911, -0.4397,  0.3309,  ..., -0.6281, -0.6281, -0.5938],
          [-0.8507, -0.2684,  0.7419,  ..., -0.5938, -0.5082, -0.6452],
          ...,
          [ 1.1015,  1.2214,  1.2043,  ...,  2.0777,  2.1119,  2.1633],
          [ 1.3755,  0.9988,  0.6392,  ...,  2.0434,  2.0777,  2.0948],
          [ 0.6221,  0.8618,  0.9132,  ...,  1.9749,  1.9407,  2.0777]],

         [[-1.1604, -1.3704, -1.0028,  ..., -0.4601, -0.6702, -0.6352],
          [-1.0028, -0.9503, -0.9153,  ..., -0.6352, -0.6176, -0.5826],
          [-1.1954, -0.8803, -0.6877,  ..., -0.6001, -0.5126, -0.6352],
          ...,
          [ 1.6232,  1.8683,  1.8333,  ...,  2.2710,  2.3060,  2.3410],
          [ 1.9384,  1.6583,  1.2731,  ...,  2.2360,  2.2885,  2.3060],
          [ 1.2906,  1.4657,  1.4657,  ...,  2.1485,  2.1835,  2.3761]],

         [[-0.9330, -1.1073, -0.7587,  ..., -0.4624, -0.7064, -0.6715],
          [-0.6193, -0.6541, -0.7936,  ..., -0.6890, -0.6715, -0.6367],
          [-0.8633, -0.6193, -0.5321,  ..., -0.6541, -0.5321, -0.6715],
          ...,
          [ 2.2914,  2.4657,  2.4831,  ...,  2.5703,  2.5877,  2.6226],
          [ 2.5354,  2.4831,  2.0648,  ...,  2.5529,  2.6051,  2.5703],
          [ 1.9951,  2.2740,  2.4657,  ...,  2.5703,  2.5529,  2.6400]]]])
[19:56:54.608424] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:56:54.625567] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:56:55.933320] INFO: mask:  tensor([[ True,  True, False,  ...,  True,  True,  True],
        [ True,  True, False,  ...,  True,  True,  True],
        [ True,  True, False,  ...,  True,  True,  True],
        ...,
        [False,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True, False,  True,  ..., False,  True,  True]], device='cuda:0')
[19:56:56.185245] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.185514] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.185936] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.186394] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.186859] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.187311] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.187767] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.188231] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.188701] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.189273] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.189731] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.190187] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.190648] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.191108] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.191564] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.192029] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.192483] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.192939] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.193400] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:56.252689] INFO: samples:  tensor([[[[-2.1179, -2.1179, -2.1179,  ...,  1.0159,  0.9474,  0.9132],
          [-2.1179, -2.1179, -2.1179,  ...,  1.0159,  0.9303,  0.8961],
          [-2.1179, -2.1179, -2.1179,  ...,  1.0159,  0.9474,  0.8961],
          ...,
          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],
          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],
          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],

         [[-2.0357, -2.0357, -2.0357,  ...,  0.9230,  0.8354,  0.7654],
          [-2.0357, -2.0357, -2.0357,  ...,  0.9055,  0.8179,  0.7479],
          [-2.0357, -2.0357, -2.0357,  ...,  0.9055,  0.8179,  0.7479],
          ...,
          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],
          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],
          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],

         [[-1.8044, -1.8044, -1.8044,  ...,  0.7925,  0.7228,  0.6705],
          [-1.8044, -1.8044, -1.8044,  ...,  0.7925,  0.7228,  0.6531],
          [-1.8044, -1.8044, -1.8044,  ...,  0.7751,  0.6879,  0.6705],
          ...,
          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],
          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],
          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],


        [[[-1.0390, -1.0904, -1.1760,  ..., -1.4329, -1.4672, -1.3815],
          [-1.0904, -1.1418, -1.1418,  ..., -1.5014, -1.5699, -1.5357],
          [-1.1418, -1.2445, -1.2103,  ..., -1.5185, -1.5528, -1.5185],
          ...,
          [-1.6727, -1.5870, -1.5528,  ..., -1.7583, -1.8439, -1.8953],
          [-1.6727, -1.6384, -1.5699,  ..., -1.8610, -1.7583, -1.7583],
          [-1.6727, -1.6384, -1.6042,  ..., -1.6555, -1.8610, -1.9467]],

         [[-0.9678, -1.0203, -1.0028,  ..., -1.1429, -1.2129, -1.1604],
          [-0.9853, -1.0203, -1.0378,  ..., -1.2304, -1.2829, -1.3004],
          [-1.0203, -0.9853, -1.0553,  ..., -1.2479, -1.2129, -1.3004],
          ...,
          [-1.4930, -1.4755, -1.4405,  ..., -1.5630, -1.5980, -1.6681],
          [-1.4755, -1.4580, -1.4055,  ..., -1.7731, -1.6331, -1.5280],
          [-1.5105, -1.4755, -1.4405,  ..., -1.2654, -1.5980, -1.7731]],

         [[-1.0027, -1.0201, -1.0376,  ..., -0.8458, -0.8981, -0.8458],
          [-0.9678, -1.0027, -1.0201,  ..., -0.9678, -1.0201, -1.0201],
          [-0.9678, -0.9678, -1.0201,  ..., -0.9853, -0.9678, -0.9678],
          ...,
          [-1.2119, -1.1770, -1.1421,  ..., -1.2293, -1.2641, -1.3164],
          [-1.2467, -1.1770, -1.1247,  ..., -1.4733, -1.3339, -1.2467],
          [-1.2990, -1.2293, -1.1944,  ..., -0.8110, -1.2990, -1.6127]]],


        [[[ 2.0948,  1.9749,  1.8893,  ...,  2.1462,  2.1633,  2.1975],
          [ 2.0948,  1.9920,  1.8722,  ...,  2.1804,  2.1975,  2.1975],
          [ 2.0605,  2.0263,  1.9235,  ...,  2.1975,  2.1975,  2.2318],
          ...,
          [ 1.2214,  1.1187,  0.9646,  ...,  2.0434,  1.9749,  1.9578],
          [ 1.2899,  1.1529,  0.9303,  ...,  1.9920,  1.9749,  1.9578],
          [ 1.3413,  1.1529,  0.8961,  ...,  1.9407,  1.9749,  1.9064]],

         [[ 2.2710,  2.1485,  2.0609,  ...,  2.2185,  2.2360,  2.2710],
          [ 2.2710,  2.1660,  2.0434,  ...,  2.2535,  2.2710,  2.2710],
          [ 2.2360,  2.2010,  2.0959,  ...,  2.2710,  2.2710,  2.3060],
          ...,
          [ 1.3606,  1.2556,  1.0980,  ...,  1.5882,  1.5707,  1.6057],
          [ 1.4307,  1.2906,  1.0630,  ...,  1.6232,  1.6057,  1.5707],
          [ 1.4832,  1.2906,  1.0280,  ...,  1.6583,  1.6408,  1.5357]],

         [[ 2.5180,  2.3960,  2.3088,  ...,  2.5006,  2.5180,  2.5529],
          [ 2.5180,  2.4134,  2.2914,  ...,  2.5354,  2.5529,  2.5529],
          [ 2.4831,  2.4483,  2.3437,  ...,  2.5529,  2.5529,  2.5877],
          ...,
          [ 1.5420,  1.4374,  1.2805,  ...,  1.6640,  1.6640,  1.7163],
          [ 1.5942,  1.4548,  1.2457,  ...,  1.6465,  1.6117,  1.5942],
          [ 1.6291,  1.4374,  1.1759,  ...,  1.6291,  1.5594,  1.4200]]],


        ...,


        [[[ 0.7419,  0.9303,  1.4612,  ...,  2.1119,  2.1119,  2.1119],
          [ 0.6392,  0.8447,  1.3927,  ...,  2.1119,  2.1119,  2.1119],
          [ 0.4337,  0.6563,  1.2385,  ...,  2.1119,  2.1119,  2.1119],
          ...,
          [ 0.9303,  0.9303,  0.9303,  ...,  0.2624,  0.2624,  0.2282],
          [ 0.9474,  0.9474,  0.9474,  ..., -0.0116, -0.0801, -0.1314],
          [ 0.9474,  0.9474,  0.9474,  ..., -0.1143, -0.1999, -0.2684]],

         [[-0.3375, -0.1800,  0.3102,  ...,  0.6254,  0.6254,  0.6254],
          [-0.4251, -0.2500,  0.2402,  ...,  0.6254,  0.6254,  0.6254],
          [-0.6176, -0.4251,  0.1176,  ...,  0.6254,  0.6254,  0.6254],
          ...,
          [-0.7927, -0.7927, -0.7927,  ..., -0.5126, -0.6001, -0.6527],
          [-0.7927, -0.7927, -0.7927,  ..., -0.7227, -0.8803, -0.9678],
          [-0.7927, -0.7927, -0.7927,  ..., -0.8102, -0.9678, -1.0728]],

         [[-0.0790, -0.0092,  0.2871,  ...,  0.0779,  0.0779,  0.0779],
          [-0.1487, -0.0615,  0.2522,  ...,  0.0779,  0.0779,  0.0779],
          [-0.2881, -0.1835,  0.1651,  ...,  0.0779,  0.0779,  0.0779],
          ...,
          [-1.2641, -1.2641, -1.2641,  ...,  0.1128, -0.1487, -0.2707],
          [-1.2467, -1.2467, -1.2467,  ..., -0.0615, -0.3927, -0.5670],
          [-1.2467, -1.2467, -1.2467,  ..., -0.1312, -0.4973, -0.6541]]],


        [[[-1.8610, -1.8953, -1.8782,  ..., -2.1008, -2.1008, -2.1179],
          [-1.8268, -1.8610, -1.8439,  ..., -2.0837, -2.1008, -2.1008],
          [-1.7583, -1.8097, -1.8097,  ..., -2.1008, -2.1008, -2.1008],
          ...,
          [-1.1589, -1.1932, -1.1418,  ..., -1.5185, -1.5014, -1.4672],
          [-1.2103, -1.1760, -1.1418,  ..., -1.5014, -1.5357, -1.5699],
          [-1.1418, -1.0733, -1.0904,  ..., -1.4158, -1.4329, -1.5528]],

         [[-1.8606, -1.8957, -1.8782,  ..., -2.0182, -2.0182, -2.0357],
          [-1.8256, -1.8606, -1.8431,  ..., -2.0007, -2.0182, -2.0182],
          [-1.7731, -1.8431, -1.8431,  ..., -2.0182, -2.0182, -2.0182],
          ...,
          [-1.1429, -1.1779, -1.1253,  ..., -1.5105, -1.4930, -1.4580],
          [-1.1429, -1.1253, -1.1078,  ..., -1.4930, -1.5280, -1.5280],
          [-1.0728, -1.0203, -1.0553,  ..., -1.4055, -1.4055, -1.5105]],

         [[-1.7522, -1.7696, -1.7522,  ..., -1.7870, -1.7870, -1.8044],
          [-1.7347, -1.7347, -1.7173,  ..., -1.7696, -1.7870, -1.7870],
          [-1.6999, -1.7173, -1.7173,  ..., -1.7870, -1.7870, -1.7870],
          ...,
          [-1.0724, -1.1073, -1.0376,  ..., -1.3861, -1.3687, -1.3339],
          [-1.0724, -1.0550, -1.0376,  ..., -1.3687, -1.4036, -1.4210],
          [-1.0027, -0.9504, -0.9853,  ..., -1.2816, -1.2990, -1.4036]]],


        [[[-0.5767, -0.7308, -1.1760,  ...,  0.9817,  0.9817,  0.9817],
          [-0.5082, -0.7137, -1.2274,  ...,  0.9646,  0.9474,  0.9817],
          [-0.5253, -0.7650, -1.2959,  ...,  0.9303,  0.9132,  0.9646],
          ...,
          [ 0.9474,  0.8789,  0.8789,  ...,  0.3481,  0.4851,  0.5878],
          [ 0.8789,  0.8618,  0.8447,  ..., -0.1828, -0.1828, -0.2684],
          [ 0.8961,  0.8789,  0.8447,  ...,  0.1768,  0.1426,  0.1597]],

         [[ 0.0651, -0.1625, -0.8627,  ...,  1.2731,  1.2731,  1.2731],
          [ 0.1176, -0.1275, -0.8978,  ...,  1.2906,  1.2906,  1.2731],
          [ 0.1001, -0.1975, -0.9853,  ...,  1.2731,  1.2731,  1.3081],
          ...,
          [ 1.0105,  0.9580,  0.9580,  ..., -0.6527, -0.4776, -0.2500],
          [ 0.9405,  0.9230,  0.9405,  ..., -1.4930, -1.4580, -1.5280],
          [ 0.9580,  0.9405,  0.9230,  ..., -1.2304, -1.2304, -1.2479]],

         [[ 0.4091,  0.1825, -0.5147,  ...,  1.5245,  1.5245,  1.5245],
          [ 0.4614,  0.2173, -0.6193,  ...,  1.5245,  1.5420,  1.5245],
          [ 0.3916,  0.0779, -0.7413,  ...,  1.5071,  1.5245,  1.5594],
          ...,
          [ 1.0539,  1.0017,  0.9668,  ..., -1.0550, -0.9330, -0.7413],
          [ 1.0539,  1.0539,  0.9842,  ..., -1.5256, -1.5604, -1.6127],
          [ 1.1062,  1.0191,  0.9494,  ..., -1.3513, -1.3513, -1.3339]]]])
[19:56:56.258978] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:56:56.275448] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:56:57.584843] INFO: mask:  tensor([[ True, False,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True, False,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False,  True,  True]], device='cuda:0')
[19:56:57.796696] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.796995] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.797401] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.797856] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.798322] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.798816] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.799280] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.799749] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.800215] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.800680] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.801148] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.801628] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.802088] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.802553] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.803024] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.803495] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.803955] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.804420] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.804890] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:57.867059] INFO: samples:  tensor([[[[ 0.7419,  0.7419,  0.7419,  ...,  1.3070,  1.2385,  0.9988],
          [ 0.7248,  0.7419,  0.7419,  ...,  1.3070,  1.2557,  0.9817],
          [ 0.7591,  0.7419,  0.7419,  ...,  1.2899,  1.2557,  0.9646],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ...,  0.6221,  0.3138,  0.5364],
          [ 2.2489,  2.2489,  2.2489,  ...,  0.5193,  0.4337,  0.3481],
          [ 2.2489,  2.2489,  2.2489,  ...,  0.3823,  0.5878,  0.4337]],

         [[ 0.7479,  0.7479,  0.7479,  ...,  1.4132,  1.3606,  1.1856],
          [ 0.7304,  0.7479,  0.7479,  ...,  1.4132,  1.3782,  1.1681],
          [ 0.7654,  0.7479,  0.7479,  ...,  1.3957,  1.3606,  1.1506],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ...,  0.7829,  0.4153,  0.6429],
          [ 2.4286,  2.4286,  2.4286,  ...,  0.6429,  0.4853,  0.4678],
          [ 2.4286,  2.4286,  2.4286,  ...,  0.4503,  0.6254,  0.5553]],

         [[ 0.9145,  0.9145,  0.9145,  ...,  1.8208,  1.8034,  1.6291],
          [ 0.8971,  0.9145,  0.9145,  ...,  1.8208,  1.8034,  1.6117],
          [ 0.9319,  0.9145,  0.9145,  ...,  1.8034,  1.7685,  1.5768],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ...,  1.0888,  0.7576,  0.9668],
          [ 2.6400,  2.6400,  2.6400,  ...,  0.9668,  0.8274,  0.7925],
          [ 2.6400,  2.6400,  2.6400,  ...,  0.7751,  0.9668,  0.8797]]],


        [[[ 0.1083,  0.1083,  0.1083,  ...,  2.2489,  2.2489,  2.2489],
          [ 0.1083,  0.1083,  0.1083,  ...,  2.2489,  2.2489,  2.2489],
          [ 0.1083,  0.1083,  0.1083,  ...,  2.2489,  2.2489,  2.2489],
          ...,
          [-0.2342, -0.2342, -0.2342,  ...,  2.2489,  2.2489,  2.2489],
          [-0.2513, -0.2513, -0.2513,  ...,  2.2489,  2.2489,  2.2489],
          [-0.2684, -0.2684, -0.2684,  ...,  2.2489,  2.2489,  2.2489]],

         [[ 0.7479,  0.7479,  0.7479,  ...,  2.4111,  2.4286,  2.4286],
          [ 0.7479,  0.7479,  0.7479,  ...,  2.4111,  2.4286,  2.4286],
          [ 0.7479,  0.7479,  0.7479,  ...,  2.4111,  2.4286,  2.4286],
          ...,
          [ 0.3627,  0.3627,  0.3627,  ...,  2.4286,  2.4286,  2.4286],
          [ 0.3803,  0.3803,  0.3803,  ...,  2.4286,  2.4286,  2.4286],
          [ 0.3803,  0.3803,  0.3803,  ...,  2.4286,  2.4286,  2.4286]],

         [[ 2.3263,  2.3263,  2.3263,  ...,  2.5354,  2.6051,  2.6400],
          [ 2.3263,  2.3263,  2.3263,  ...,  2.5354,  2.6051,  2.6400],
          [ 2.3263,  2.3263,  2.3263,  ...,  2.5354,  2.6051,  2.6400],
          ...,
          [ 1.7511,  1.7511,  1.7860,  ...,  2.5529,  2.6051,  2.6400],
          [ 1.7860,  1.7860,  1.8208,  ...,  2.5703,  2.6051,  2.6400],
          [ 1.8208,  1.8208,  1.8208,  ...,  2.6051,  2.6400,  2.6400]]],


        [[[ 0.7077,  0.6906,  0.6734,  ...,  0.9132,  0.9132,  0.9303],
          [ 0.6906,  0.7077,  0.6906,  ...,  0.9303,  0.9132,  0.9303],
          [ 0.6392,  0.7248,  0.7077,  ...,  0.9646,  0.9303,  0.9303],
          ...,
          [-0.8164, -0.6794, -0.6794,  ..., -0.5424,  0.7248,  1.2043],
          [-1.5870, -1.8610, -1.9980,  ..., -0.9363,  0.5878,  0.8447],
          [-1.3644, -0.9534, -1.2617,  ..., -0.9877,  0.5193,  0.8789]],

         [[ 1.0805,  1.0630,  1.0805,  ...,  1.3256,  1.3256,  1.3256],
          [ 1.0980,  1.0805,  1.0980,  ...,  1.3081,  1.3256,  1.3256],
          [ 1.0805,  1.0980,  1.1155,  ...,  1.3081,  1.3081,  1.3081],
          ...,
          [-1.2829, -1.1253, -1.3004,  ..., -0.8978,  0.0826,  0.4328],
          [-1.6155, -1.8256, -1.8606,  ..., -1.0553, -0.0924,  0.2927],
          [-1.5805, -1.3880, -1.3704,  ..., -1.1954, -0.0399,  0.2752]],

         [[ 1.4374,  1.4200,  1.4200,  ...,  1.6291,  1.5942,  1.5942],
          [ 1.4374,  1.4374,  1.4374,  ...,  1.6291,  1.5942,  1.5942],
          [ 1.4200,  1.4548,  1.4548,  ...,  1.6291,  1.6291,  1.6291],
          ...,
          [-1.2293, -1.4036, -1.3513,  ..., -1.1073, -0.5321, -0.1312],
          [-1.6127, -1.5256, -1.4907,  ..., -1.0550, -0.5844, -0.3753],
          [-1.4907, -1.3687, -1.5779,  ..., -1.4384, -0.5495, -0.4275]]],


        ...,


        [[[ 0.5536,  0.5707,  0.5878,  ...,  2.0434,  2.0434,  2.2147],
          [ 0.5536,  0.5536,  0.5707,  ...,  2.0948,  2.0777,  2.2489],
          [ 0.5878,  0.5878,  0.6049,  ...,  2.0263,  2.0434,  2.2147],
          ...,
          [-0.6452, -1.0048, -1.1932,  ...,  1.1187,  1.7865,  1.8893],
          [-1.0048, -1.1589, -1.0390,  ...,  1.0331,  1.1700,  1.5297],
          [-1.2103, -1.0904, -0.6109,  ...,  0.7419,  0.9303,  2.2318]],

         [[-0.4951, -0.4776, -0.4776,  ...,  0.9230,  1.0455,  1.1506],
          [-0.4951, -0.4951, -0.4776,  ...,  0.9230,  0.9930,  1.1506],
          [-0.4601, -0.4776, -0.4601,  ...,  0.8529,  0.9230,  1.0455],
          ...,
          [-1.3004, -1.5980, -1.7381,  ..., -0.2500,  0.4328,  0.4503],
          [-1.2479, -1.5980, -1.6331,  ..., -0.4776, -0.4251,  0.4153],
          [-1.6506, -1.5980, -1.2129,  ..., -0.6702, -0.0749,  2.1660]],

         [[-1.4036, -1.3861, -1.3861,  ...,  0.3916,  0.3742,  0.5834],
          [-1.4036, -1.4036, -1.3861,  ...,  0.4439,  0.3393,  0.6008],
          [-1.3687, -1.3861, -1.3687,  ...,  0.2871,  0.2173,  0.5136],
          ...,
          [-1.6127, -1.7173, -1.7522,  ..., -0.9330, -0.4101, -0.4275],
          [-1.6824, -1.7870, -1.7347,  ..., -1.0898, -1.2467, -0.3230],
          [-1.7870, -1.7347, -1.4210,  ..., -1.3513, -0.9678,  1.2108]]],


        [[[ 1.1700,  1.1187,  1.1358,  ...,  0.5878,  0.6049,  0.5878],
          [ 1.0331,  0.9817,  1.0844,  ...,  0.6049,  0.6392,  0.5878],
          [ 1.1187,  1.1529,  1.2385,  ...,  0.5878,  0.6563,  0.6392],
          ...,
          [ 1.1529,  1.2728,  1.3413,  ...,  1.4098,  1.4269,  1.2899],
          [ 1.3070,  1.4098,  1.4783,  ...,  1.2043,  1.4269,  0.7248],
          [ 1.3070,  1.5125,  1.5125,  ...,  1.0844,  1.2728,  0.7077]],

         [[ 1.4657,  1.4132,  1.4832,  ...,  0.6779,  0.6954,  0.6779],
          [ 1.3256,  1.2731,  1.4132,  ...,  0.6954,  0.7304,  0.6779],
          [ 1.4132,  1.4482,  1.5182,  ...,  0.6779,  0.7479,  0.7304],
          ...,
          [ 1.1331,  1.2556,  1.3256,  ...,  1.2206,  1.2381,  1.1155],
          [ 1.2556,  1.3782,  1.4482,  ...,  1.1155,  1.3431,  0.6254],
          [ 1.2556,  1.4657,  1.4482,  ...,  1.0980,  1.2731,  0.6779]],

         [[ 2.0125,  1.9603,  2.0125,  ...,  1.0191,  1.0365,  1.0191],
          [ 1.8383,  1.7860,  1.9080,  ...,  1.0365,  1.0714,  1.0191],
          [ 1.8731,  1.9080,  1.9951,  ...,  1.0191,  1.0888,  1.0714],
          ...,
          [ 1.3502,  1.4722,  1.5420,  ...,  1.5768,  1.5071,  1.3154],
          [ 1.5071,  1.6291,  1.6814,  ...,  1.4897,  1.6814,  1.0017],
          [ 1.5071,  1.7163,  1.6988,  ...,  1.2805,  1.4897,  0.9668]]],


        [[[ 0.3481,  0.3481,  0.3481,  ...,  1.3584,  1.2214,  0.8789],
          [ 0.3481,  0.3481,  0.3481,  ...,  1.3070,  1.2557,  1.0844],
          [ 0.3481,  0.3481,  0.3481,  ...,  1.2728,  1.2214,  1.1872],
          ...,
          [ 0.8961,  0.8789,  0.8789,  ..., -0.7993, -0.6281, -0.2513],
          [ 0.9817,  0.9303,  0.8789,  ..., -0.8678, -0.5082, -0.0629],
          [ 0.9817,  0.9303,  0.9474,  ..., -0.8678, -0.4226, -0.0629]],

         [[ 0.6254,  0.6254,  0.6254,  ...,  0.7304,  0.6779,  0.3452],
          [ 0.6254,  0.6254,  0.6254,  ...,  0.6779,  0.7129,  0.5028],
          [ 0.6254,  0.6254,  0.6254,  ...,  0.6604,  0.6954,  0.5903],
          ...,
          [ 1.1856,  1.2206,  1.2031,  ..., -0.6352, -0.5126, -0.1800],
          [ 1.1506,  1.2031,  1.1681,  ..., -0.6702, -0.3725, -0.0049],
          [ 1.1506,  1.1681,  1.2556,  ..., -0.6527, -0.2850,  0.0301]],

         [[ 0.8971,  0.8971,  0.8971,  ...,  0.3568,  0.2871, -0.0964],
          [ 0.8971,  0.8971,  0.8971,  ...,  0.3393,  0.4091,  0.1302],
          [ 0.8971,  0.8971,  0.8971,  ...,  0.4091,  0.4962,  0.2522],
          ...,
          [ 1.4374,  1.4374,  1.3677,  ..., -1.2119, -1.1073, -0.7761],
          [ 1.4025,  1.4025,  1.3677,  ..., -1.3339, -1.0201, -0.6193],
          [ 1.4025,  1.4025,  1.4722,  ..., -1.3164, -0.9330, -0.6018]]]])
[19:56:57.873416] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:56:57.890509] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:56:59.197739] INFO: mask:  tensor([[ True,  True,  True,  ...,  True, False,  True],
        [ True,  True, False,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:56:59.409216] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.409499] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.409906] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.410373] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.410842] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.411308] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.411775] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.412239] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.412711] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.413170] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.413632] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.414102] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.414564] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.415031] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.415500] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.415970] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.416428] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.416891] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.417359] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:56:59.473558] INFO: samples:  tensor([[[[ 0.8447,  0.7933,  0.8276,  ...,  1.9235,  1.9235,  1.9578],
          [ 0.8447,  0.8276,  0.8276,  ...,  1.8550,  1.8722,  1.9235],
          [ 0.8276,  0.8276,  0.7933,  ...,  1.9064,  1.9235,  1.9235],
          ...,
          [ 1.5810,  1.5639,  1.6324,  ...,  1.7180,  1.7865,  1.7694],
          [ 1.6324,  1.6324,  1.6153,  ...,  1.7352,  1.7009,  1.7523],
          [ 1.6495,  1.7180,  1.6667,  ...,  1.5982,  1.5639,  1.6495]],

         [[ 1.0805,  1.0980,  0.9755,  ...,  2.0084,  1.9909,  1.9909],
          [ 1.0980,  1.0630,  0.9930,  ...,  2.0084,  1.9909,  2.0084],
          [ 1.1331,  1.0630,  1.0280,  ...,  2.0084,  2.0084,  2.0084],
          ...,
          [ 1.4132,  1.3782,  1.3957,  ...,  0.9755,  1.0455,  0.9580],
          [ 1.4657,  1.4482,  1.3957,  ...,  0.7129,  0.6254,  0.7479],
          [ 1.5182,  1.5357,  1.5182,  ...,  0.3277,  0.2752,  0.5028]],

         [[ 1.0017,  0.9145,  0.8099,  ...,  1.9254,  1.9603,  1.9777],
          [ 1.0365,  0.9319,  0.8448,  ...,  1.9254,  1.9254,  1.9777],
          [ 1.0539,  0.9668,  0.8971,  ...,  1.9603,  1.9777,  1.9777],
          ...,
          [ 0.9145,  0.8274,  0.7751,  ..., -1.1421, -0.9156, -1.0376],
          [ 0.9842,  0.8797,  0.7751,  ..., -1.2293, -1.2467, -1.2293],
          [ 1.1062,  1.0539,  1.0017,  ..., -1.2293, -1.2119, -1.1944]]],


        [[[-0.1486, -0.6281, -0.8678,  ...,  0.9474,  0.9132,  0.4337],
          [-0.5596, -0.6623, -0.6109,  ...,  1.0844,  0.8618,  0.5364],
          [-0.8335, -0.5596, -0.1314,  ...,  1.1872,  1.0331,  0.6563],
          ...,
          [-2.0837, -2.1008, -2.0837,  ...,  0.1768,  0.0741,  0.1254],
          [-2.0494, -2.0494, -2.1179,  ...,  0.3823,  0.1597,  0.1597],
          [-2.0665, -2.1179, -2.1179,  ...,  0.3309,  0.1083,  0.3823]],

         [[-0.3901, -0.8277, -1.1604,  ...,  0.2052,  0.1702, -0.3025],
          [-0.9853, -0.9328, -0.9853,  ...,  0.4153,  0.1877, -0.2500],
          [-1.2129, -1.0378, -0.6877,  ...,  0.4853,  0.2577, -0.1800],
          ...,
          [-2.0357, -2.0182, -1.9657,  ..., -1.9307, -2.0007, -2.0007],
          [-2.0357, -2.0182, -1.9482,  ..., -1.9307, -2.0182, -1.9132],
          [-2.0182, -2.0182, -2.0007,  ..., -1.9832, -2.0007, -1.8256]],

         [[-0.2184, -0.6367, -0.8458,  ..., -0.3753, -0.3753, -1.0724],
          [-0.5844, -0.7761, -0.9330,  ..., -0.2010, -0.2010, -0.8110],
          [-0.8807, -1.0201, -0.7587,  ..., -0.0964, -0.2358, -0.7587],
          ...,
          [-1.7173, -1.7173, -1.7347,  ..., -1.7522, -1.8044, -1.7696],
          [-1.8044, -1.7870, -1.7696,  ..., -1.7870, -1.7347, -1.7870],
          [-1.6999, -1.7696, -1.8044,  ..., -1.7870, -1.7696, -1.6824]]],


        [[[ 1.5468,  1.4954,  1.5810,  ...,  0.7762,  0.6392,  1.8550],
          [ 1.8893,  1.4954,  1.5297,  ...,  0.3138,  0.9303,  1.2214],
          [ 0.9817,  1.4783,  1.5468,  ..., -0.1314,  0.4508,  0.4337],
          ...,
          [-1.2959, -1.5357, -0.9534,  ...,  0.0741, -0.5082, -0.0972],
          [-1.6727, -1.6042, -0.4568,  ..., -0.0287, -0.3712,  0.0912],
          [-1.5870, -1.0390, -0.2856,  ..., -0.4568, -0.1657, -0.3027]],

         [[ 1.4307,  1.4307,  1.3957,  ...,  0.7829,  0.5028,  1.9559],
          [ 1.8683,  1.5007,  1.3256,  ...,  0.3452,  0.7129,  1.2031],
          [ 0.9405,  1.2906,  1.4132,  ..., -0.2500,  0.4853,  0.1527],
          ...,
          [-0.8452, -1.1779, -0.6352,  ...,  0.3803, -0.3550,  0.0651],
          [-1.4930, -1.2479,  0.0476,  ...,  0.2927, -0.1450,  0.3277],
          [-1.4230, -0.4251,  0.3102,  ..., -0.0924,  0.2227, -0.0574]],

         [[ 1.1759,  1.2631,  1.3328,  ...,  1.0191,  0.6705,  2.2566],
          [ 1.8557,  1.3677,  1.3677,  ...,  0.4788,  0.7402,  1.5245],
          [ 1.1062,  1.2805,  1.3328,  ...,  0.1825,  0.7402,  0.3742],
          ...,
          [-1.4036, -1.5430, -1.5779,  ...,  0.1999, -0.7587, -0.1487],
          [-1.4733, -1.4210, -0.9504,  ..., -0.1835, -0.6367, -0.0790],
          [-1.2816, -1.0898, -0.8284,  ..., -0.5670, -0.2881, -0.6367]]],


        ...,


        [[[ 0.8104,  0.7933,  0.8104,  ...,  0.7077,  0.7077,  0.6734],
          [ 0.7933,  0.7762,  0.7762,  ...,  0.7419,  0.7077,  0.6734],
          [ 0.7762,  0.7762,  0.7591,  ...,  0.7419,  0.6906,  0.7077],
          ...,
          [ 0.5707,  0.5707,  0.4851,  ...,  0.7248,  0.7591,  0.7419],
          [ 0.6049,  0.5536,  0.4679,  ...,  0.6906,  0.7591,  0.7419],
          [ 0.6734,  0.5536,  0.6734,  ...,  0.6734,  0.7248,  0.7762]],

         [[ 1.5357,  1.5182,  1.5532,  ...,  1.3256,  1.3431,  1.3256],
          [ 1.5532,  1.5357,  1.5357,  ...,  1.3606,  1.3431,  1.3081],
          [ 1.5357,  1.5357,  1.5007,  ...,  1.3256,  1.3081,  1.3081],
          ...,
          [ 0.3978,  0.3803,  0.2927,  ...,  1.4132,  1.3957,  1.3431],
          [ 0.3803,  0.3277,  0.2577,  ...,  1.3957,  1.4307,  1.3606],
          [ 0.4328,  0.3102,  0.4153,  ...,  1.3606,  1.3957,  1.3957]],

         [[ 2.3088,  2.2740,  2.2566,  ...,  1.9254,  1.8905,  1.8208],
          [ 2.2740,  2.2740,  2.2391,  ...,  1.8731,  1.8383,  1.8034],
          [ 2.2914,  2.3263,  2.3088,  ...,  1.8383,  1.8557,  1.8208],
          ...,
          [-0.3753, -0.3927, -0.4450,  ...,  2.1171,  2.1694,  2.0997],
          [-0.2881, -0.3927, -0.4624,  ...,  2.1694,  2.2043,  2.0997],
          [-0.1835, -0.3230, -0.2532,  ...,  2.1868,  2.1520,  2.1171]]],


        [[[ 2.2489,  2.2489,  2.2489,  ..., -1.0219, -1.0390, -1.0562],
          [ 2.2489,  2.2489,  2.2489,  ..., -1.0219, -1.0390, -1.0562],
          [ 2.2489,  2.2489,  2.2489,  ..., -1.0219, -1.0390, -1.0562],
          ...,
          [-1.8097, -1.8439, -1.7069,  ..., -1.4158, -1.5699, -1.3644],
          [-1.8953, -1.7240, -1.8439,  ..., -1.2788, -1.4500, -1.5528],
          [-1.8439, -1.7412, -1.7240,  ..., -1.3815, -1.5185, -1.5870]],

         [[ 2.4286,  2.4286,  2.4286,  ..., -1.0203, -1.0553, -1.0728],
          [ 2.4286,  2.4286,  2.4286,  ..., -1.0203, -1.0378, -1.0553],
          [ 2.4286,  2.4286,  2.4286,  ..., -1.0378, -1.0378, -1.0553],
          ...,
          [-1.7906, -1.8256, -1.6681,  ..., -1.4230, -1.5805, -1.3704],
          [-1.8606, -1.7031, -1.8431,  ..., -1.2829, -1.4580, -1.5630],
          [-1.8431, -1.7556, -1.7031,  ..., -1.3880, -1.5280, -1.5980]],

         [[ 2.6400,  2.6400,  2.6400,  ..., -0.9678, -0.9853, -1.0027],
          [ 2.6400,  2.6400,  2.6400,  ..., -1.0376, -1.0550, -1.0724],
          [ 2.6400,  2.6400,  2.6400,  ..., -1.0027, -1.0027, -1.0201],
          ...,
          [-1.6302, -1.6302, -1.5430,  ..., -1.4036, -1.5604, -1.3513],
          [-1.7173, -1.5430, -1.6824,  ..., -1.2641, -1.4384, -1.5430],
          [-1.6999, -1.6302, -1.5779,  ..., -1.3687, -1.5081, -1.5779]]],


        [[[-1.4329, -1.5528, -1.2959,  ..., -2.1008, -2.1008, -2.1179],
          [-1.9295, -1.9638, -1.8097,  ..., -2.0323, -2.1179, -1.9638],
          [-2.1179, -2.1008, -2.1008,  ..., -1.9638, -1.9638, -1.7754],
          ...,
          [-1.9809, -1.9809, -1.8268,  ...,  0.5193, -1.6898, -2.1179],
          [-2.0323, -2.0837, -2.1179,  ...,  0.0741, -1.5014, -2.1179],
          [-2.0494, -2.1008, -2.1179,  ..., -0.1999, -1.5699, -2.1008]],

         [[-1.1779, -1.4230, -1.0728,  ..., -2.0357, -1.9832, -2.0357],
          [-1.8256, -1.8431, -1.6856,  ..., -1.9307, -2.0007, -1.9482],
          [-2.0357, -2.0182, -2.0182,  ..., -2.0182, -2.0182, -1.9832],
          ...,
          [-1.8957, -1.7381, -1.5805,  ...,  0.6254, -1.5280, -2.0357],
          [-2.0357, -2.0357, -2.0357,  ...,  0.1176, -1.3529, -1.9832],
          [-2.0007, -2.0007, -1.9307,  ..., -0.0749, -1.3704, -2.0007]],

         [[ 0.3916,  0.1302,  0.2696,  ..., -1.8044, -1.8044, -1.8044],
          [-0.7413, -0.6715, -0.4101,  ..., -1.7522, -1.7696, -1.6302],
          [-1.7173, -1.6824, -1.6650,  ..., -1.7347, -1.6999, -1.5604],
          ...,
          [-0.7936, -0.6541, -0.3927,  ...,  0.7054, -0.9678, -1.2816],
          [-1.6476, -1.7870, -1.8044,  ...,  0.3742, -0.6367, -1.4036],
          [-1.6824, -1.8044, -1.7347,  ...,  0.2871, -0.4798, -1.0027]]]])
[19:56:59.479541] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:56:59.496218] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:00.799707] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True, False]], device='cuda:0')
[19:57:01.010153] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.010419] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.010837] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.011305] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.011771] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.012239] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.012704] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.013172] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.013640] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.014108] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.014572] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.015046] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.015509] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.016090] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.016682] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.017018] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.017385] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.017848] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.018321] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:01.080904] INFO: samples:  tensor([[[[-1.2788, -1.2788, -1.2274,  ..., -1.0390, -0.9877, -1.0904],
          [-1.2617, -1.2788, -1.1932,  ..., -1.0562, -1.0219, -1.0562],
          [-1.2103, -1.2445, -1.2103,  ..., -1.0904, -1.0390, -1.0219],
          ...,
          [-0.9705, -0.9877, -0.9877,  ..., -0.6281, -0.5424, -0.5767],
          [-1.0048, -1.0904, -1.0562,  ..., -0.5938, -0.6452, -0.6794],
          [-1.1247, -1.1589, -1.1418,  ..., -0.6109, -0.5938, -0.5938]],

         [[-1.0553, -1.0553, -1.0728,  ..., -0.8452, -0.8277, -0.8803],
          [-1.0203, -1.0378, -1.0378,  ..., -0.7927, -0.8277, -0.7927],
          [-1.0728, -1.0378, -1.0378,  ..., -0.7927, -0.7752, -0.7752],
          ...,
          [-1.1954, -1.1779, -1.1604,  ..., -0.7927, -0.7577, -0.8102],
          [-1.1954, -1.1954, -1.1954,  ..., -0.8102, -0.8102, -0.8277],
          [-1.2479, -1.1779, -1.1954,  ..., -0.8277, -0.8627, -0.8102]],

         [[-0.6715, -0.7064, -0.6367,  ..., -0.5495, -0.4624, -0.4275],
          [-0.6890, -0.6367, -0.5670,  ..., -0.4973, -0.4450, -0.3927],
          [-0.6541, -0.5844, -0.5844,  ..., -0.4275, -0.4101, -0.4798],
          ...,
          [-1.1247, -1.0724, -1.0550,  ..., -0.9156, -0.8458, -0.8633],
          [-1.1247, -1.0898, -1.1073,  ..., -0.7761, -0.8981, -0.9853],
          [-1.0898, -1.1247, -1.1421,  ..., -0.8458, -0.8458, -0.8458]]],


        [[[ 1.1529,  0.3994, -0.5596,  ..., -0.8164, -0.5767, -0.3369],
          [ 1.1358, -0.0801, -0.5938,  ..., -0.5424, -0.4568, -0.3027],
          [-0.9020, -0.4226, -0.4397,  ..., -0.7822, -0.7137, -0.3198],
          ...,
          [ 0.6049,  0.6734, -0.1657,  ...,  0.0569, -0.0458,  0.1426],
          [ 0.6392,  0.3994,  0.4508,  ...,  0.3652,  0.1254, -0.0629],
          [ 2.0434,  0.7591,  0.2967,  ..., -0.1143,  0.0912,  0.2111]],

         [[ 1.5007,  0.6954, -0.3025,  ..., -0.7752, -0.5826, -0.4076],
          [ 1.3957,  0.1702, -0.3901,  ..., -0.4951, -0.4601, -0.3725],
          [-0.7577, -0.2500, -0.3200,  ..., -0.7577, -0.7052, -0.3550],
          ...,
          [ 0.5378,  0.5728, -0.3025,  ..., -0.1625, -0.2850, -0.1450],
          [ 0.5378,  0.2402,  0.2752,  ...,  0.1527, -0.1450, -0.3901],
          [ 2.0434,  0.5728,  0.1001,  ..., -0.3375, -0.1800, -0.0749]],

         [[ 1.4374,  0.3219, -1.0550,  ..., -1.0724, -0.8807, -0.6715],
          [ 1.1585, -0.4101, -1.2641,  ..., -0.8110, -0.7936, -0.7064],
          [-1.2293, -1.0027, -1.1596,  ..., -1.0376, -1.0201, -0.7064],
          ...,
          [ 0.0082,  0.0431, -0.7936,  ..., -0.4101, -0.4798, -0.3055],
          [ 0.0953, -0.2010, -0.1312,  ..., -0.1487, -0.3753, -0.5670],
          [ 1.6988,  0.1999, -0.3055,  ..., -0.6890, -0.4624, -0.3230]]],


        [[[-1.8953, -1.8439, -1.7240,  ..., -1.7412, -1.7412, -1.7412],
          [-1.7069, -1.7925, -1.7583,  ..., -1.7240, -1.7240, -1.7412],
          [-1.6555, -1.7240, -1.7583,  ..., -1.7069, -1.7240, -1.7240],
          ...,
          [-1.7069, -1.7412, -1.6727,  ..., -1.9467, -1.9124, -1.9295],
          [-1.7069, -1.6727, -1.6555,  ..., -1.8439, -1.8610, -1.9124],
          [-1.7754, -1.7754, -1.7240,  ..., -1.8610, -1.9809, -2.0665]],

         [[-0.0749, -0.0224,  0.0826,  ...,  0.0301,  0.0126, -0.0049],
          [ 0.0826,  0.0476,  0.1001,  ...,  0.0301,  0.0301,  0.0301],
          [ 0.1877,  0.1877,  0.1352,  ...,  0.0651,  0.0301,  0.0301],
          ...,
          [ 0.1527,  0.2402,  0.2577,  ..., -0.7402, -0.7402, -0.7402],
          [ 0.0651,  0.2227,  0.2227,  ..., -0.5826, -0.6001, -0.6527],
          [-0.0224,  0.1001,  0.1352,  ..., -0.4951, -0.6176, -0.7577]],

         [[ 0.9494,  1.0017,  1.1411,  ...,  1.5942,  1.6117,  1.5942],
          [ 1.1585,  1.0888,  1.1585,  ...,  1.5942,  1.6117,  1.6291],
          [ 1.3677,  1.2631,  1.2282,  ...,  1.6291,  1.6117,  1.6117],
          ...,
          [ 1.3154,  1.4025,  1.3851,  ...,  0.1476,  0.1999,  0.2173],
          [ 1.2282,  1.3851,  1.3677,  ...,  0.2348,  0.2348,  0.2348],
          [ 1.1411,  1.2805,  1.2805,  ...,  0.2871,  0.1651,  0.0779]]],


        ...,


        [[[-0.3027, -0.1999, -0.2171,  ..., -0.3541, -0.5082, -0.4054],
          [-0.4226, -0.3369, -0.2342,  ..., -0.0116, -0.0629, -0.1143],
          [-0.4911, -0.3541, -0.2684,  ...,  0.2111, -0.1999, -0.3541],
          ...,
          [-0.5938, -0.5253, -0.4739,  ...,  0.2796,  0.2796,  0.2967],
          [-0.4739, -0.4911, -0.4911,  ...,  0.0398,  0.0227,  0.0398],
          [-0.1828, -0.2342, -0.2342,  ...,  0.0569,  0.0741,  0.0912]],

         [[-0.4251, -0.2850, -0.3901,  ..., -0.5651, -0.6702, -0.7577],
          [-0.4951, -0.3375, -0.3200,  ..., -0.4426, -0.5126, -0.6702],
          [-0.7577, -0.4426, -0.3550,  ..., -0.4951, -0.9853, -1.1078],
          ...,
          [-0.6877, -0.5826, -0.5301,  ...,  0.1352,  0.1352,  0.1877],
          [-0.4076, -0.4076, -0.3725,  ..., -0.1450, -0.1450, -0.1275],
          [ 0.0126,  0.0301,  0.0826,  ..., -0.1800, -0.1450, -0.1099]],

         [[-1.3861, -1.2641, -1.1247,  ..., -0.8633, -0.9330, -1.0027],
          [-1.3513, -1.2293, -1.0550,  ..., -0.7587, -0.7587, -0.8110],
          [-1.3164, -1.1944, -1.1247,  ..., -0.5670, -0.9504, -1.0201],
          ...,
          [-1.2816, -1.2816, -1.2816,  ..., -0.4624, -0.4973, -0.5147],
          [-1.2293, -1.2641, -1.2816,  ..., -0.6367, -0.6890, -0.6890],
          [-1.0376, -1.0724, -1.0550,  ..., -0.5321, -0.5670, -0.5844]]],


        [[[ 1.7009,  1.7180,  1.7352,  ...,  1.4098,  1.3927,  1.4098],
          [ 1.6667,  1.6838,  1.6838,  ...,  1.3927,  1.3755,  1.3927],
          [ 1.6324,  1.6495,  1.6495,  ...,  1.3927,  1.3755,  1.3927],
          ...,
          [ 1.9578,  1.9407,  1.7865,  ...,  0.2111,  0.2453,  0.2282],
          [ 1.9578,  1.9578,  1.9749,  ...,  0.5364,  0.5364,  0.5193],
          [ 1.9920,  1.9920,  1.9920,  ...,  0.8276,  0.8104,  0.8104]],

         [[ 1.9384,  1.9559,  1.9734,  ...,  1.6232,  1.6057,  1.5882],
          [ 1.9034,  1.9209,  1.9209,  ...,  1.6057,  1.5882,  1.5707],
          [ 1.8683,  1.8859,  1.8859,  ...,  1.6057,  1.5882,  1.5707],
          ...,
          [ 1.8333,  1.8158,  1.6583,  ...,  0.3277,  0.3102,  0.2927],
          [ 1.8683,  1.8683,  1.8859,  ...,  0.6779,  0.6429,  0.6254],
          [ 1.9384,  1.9384,  1.9384,  ...,  0.9755,  0.9580,  0.9580]],

         [[ 2.3611,  2.3786,  2.3960,  ...,  1.5420,  1.5245,  1.5594],
          [ 2.3088,  2.3263,  2.3263,  ...,  1.5245,  1.5071,  1.5420],
          [ 2.2740,  2.2914,  2.2914,  ...,  1.5245,  1.5071,  1.5420],
          ...,
          [ 2.1520,  2.1346,  1.9777,  ...,  0.5136,  0.5136,  0.4962],
          [ 2.2043,  2.2043,  2.2217,  ...,  0.8971,  0.8797,  0.8622],
          [ 2.2740,  2.2740,  2.2740,  ...,  1.1934,  1.1759,  1.1759]]],


        [[[ 1.8722,  1.3413,  1.0673,  ..., -1.1418, -1.0562, -1.1760],
          [ 1.7009,  1.3755,  1.1700,  ..., -1.1589, -1.1418, -1.0048],
          [ 1.4269,  1.2728,  1.2043,  ..., -1.2445, -1.1589, -1.0048],
          ...,
          [ 0.5707,  0.9988,  1.0673,  ...,  0.9474,  0.0227, -0.1828],
          [ 0.7591,  0.6906,  0.9646,  ...,  0.8961,  0.7762, -0.3198],
          [ 0.9303,  0.9646,  1.0159,  ...,  0.9474,  0.8276,  0.9988]],

         [[ 1.6232,  1.1681,  0.6779,  ..., -0.7227, -0.6352, -0.7577],
          [ 1.5357,  1.0805,  0.6604,  ..., -0.7402, -0.7227, -0.5826],
          [ 1.3081,  0.8179,  0.6604,  ..., -0.8277, -0.7402, -0.5826],
          ...,
          [ 0.3452,  0.8179,  0.8354,  ...,  0.6604, -0.4076, -0.4251],
          [ 0.6078,  0.5028,  0.7479,  ...,  0.7129,  0.3452, -0.8102],
          [ 0.7829,  0.8004,  0.8179,  ...,  0.8179,  0.3627,  0.4503]],

         [[ 2.1694,  1.5594,  0.4265,  ..., -0.4973, -0.4101, -0.5321],
          [ 1.9777,  1.3677,  0.4962,  ..., -0.5147, -0.4973, -0.3578],
          [ 1.6291,  1.0539,  0.4439,  ..., -0.6018, -0.5147, -0.3578],
          ...,
          [ 0.3045,  0.4439,  0.1999,  ...,  0.4788, -0.7761, -0.4275],
          [ 0.4962,  0.2696,  0.1999,  ...,  0.2348, -0.1487, -1.1247],
          [ 0.8274,  0.6705,  0.4788,  ...,  0.1128, -0.0441,  0.1651]]]])
[19:57:01.087060] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:01.103979] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:02.406169] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [False,  True,  True,  ...,  True,  True,  True],
        [ True,  True, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:02.616101] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.616363] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.616788] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.617253] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.617718] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.618179] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.618646] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.619119] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.619584] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.620050] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.620510] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.620983] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.621443] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.621907] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.622375] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.622849] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.623305] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.623772] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.624237] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:02.689197] INFO: samples:  tensor([[[[-0.1999,  0.2111,  0.5364,  ..., -0.3198, -0.0801,  1.6153],
          [ 0.0056,  0.1768,  0.4679,  ..., -0.3027, -0.1999,  1.4269],
          [ 0.2453,  0.1939,  0.6221,  ..., -0.4911,  0.1254,  1.2214],
          ...,
          [-1.9980, -2.0152, -2.0494,  ..., -2.0837, -2.1008, -1.9980],
          [-2.0494, -2.0494, -2.0665,  ..., -1.8953, -1.9638, -2.0665],
          [-2.0665, -2.0837, -2.0837,  ..., -1.7069, -2.0323, -2.0665]],

         [[-0.2500,  0.3803,  0.7129,  ..., -0.4426, -0.0399,  1.7458],
          [ 0.0126,  0.4153,  0.6954,  ..., -0.3725, -0.3375,  1.4832],
          [ 0.3978,  0.3627,  0.9055,  ..., -0.4951, -0.0749,  1.1331],
          ...,
          [-2.0357, -2.0357, -2.0357,  ..., -1.9657, -1.9132, -1.2829],
          [-2.0357, -2.0357, -2.0182,  ..., -1.8081, -1.7206, -1.1779],
          [-2.0182, -2.0182, -2.0182,  ..., -1.5980, -1.8081, -1.1954]],

         [[ 0.4962,  1.1759,  1.4374,  ...,  0.0431,  0.2696,  1.7337],
          [ 0.8448,  1.2108,  1.5594,  ...,  0.0779,  0.0082,  1.4374],
          [ 1.2282,  1.2282,  1.7511,  ..., -0.0092,  0.1999,  1.0191],
          ...,
          [-1.8044, -1.8044, -1.8044,  ..., -1.6999, -1.7173, -1.1247],
          [-1.8044, -1.8044, -1.8044,  ..., -1.5256, -1.5430, -1.1421],
          [-1.8044, -1.8044, -1.8044,  ..., -1.3339, -1.6127, -1.1247]]],


        [[[-1.5185, -1.5357, -1.3987,  ..., -1.2103, -1.3302, -1.5870],
          [-1.3473, -1.4158, -1.2959,  ..., -1.0562, -1.0733, -1.1589],
          [-1.2103, -1.3130, -1.3473,  ..., -1.2617, -1.2274, -0.9877],
          ...,
          [-1.7925, -1.8097, -1.7412,  ..., -0.4911, -0.7308, -1.1589],
          [-1.5357, -1.6555, -1.7583,  ..., -1.3130, -1.5185, -1.6898],
          [-1.5699, -1.4672, -1.4843,  ..., -1.6042, -1.7240, -1.8097]],

         [[-1.3704, -1.3354, -1.1779,  ..., -0.9503, -1.0728, -1.3354],
          [-1.1954, -1.2129, -1.0728,  ..., -0.7927, -0.8102, -0.8978],
          [-1.0553, -1.1078, -1.1078,  ..., -1.0203, -0.9853, -0.7402],
          ...,
          [-1.6506, -1.6681, -1.5980,  ..., -0.4601, -0.7052, -1.1604],
          [-1.3880, -1.5105, -1.6155,  ..., -1.3004, -1.4930, -1.6856],
          [-1.4230, -1.3179, -1.3354,  ..., -1.5805, -1.7031, -1.7906]],

         [[-1.2641, -1.3339, -1.1770,  ..., -1.0898, -1.2119, -1.4733],
          [-1.1073, -1.2293, -1.1073,  ..., -0.9330, -0.9504, -1.0027],
          [-1.0201, -1.1247, -1.1596,  ..., -1.1073, -1.0724, -0.8284],
          ...,
          [-1.3164, -1.3164, -1.2119,  ..., -0.2881, -0.5321, -0.9853],
          [-1.0724, -1.1596, -1.2293,  ..., -1.1247, -1.3164, -1.5081],
          [-1.0898, -0.9678, -0.9678,  ..., -1.4036, -1.5256, -1.6127]]],


        [[[-0.3712, -0.6109, -0.3883,  ..., -2.0837, -2.0494, -2.0323],
          [-0.2342, -0.4054, -0.2171,  ..., -2.0665, -2.0152, -2.0323],
          [-0.6109, -0.5253, -0.1657,  ..., -2.0665, -2.0323, -2.0665],
          ...,
          [-0.0801,  0.0912,  0.0227,  ..., -1.8268, -1.8782, -1.7069],
          [-0.1486,  0.1939,  0.3138,  ..., -1.7754, -1.6555, -1.1418],
          [-0.2513,  0.0741,  0.3994,  ..., -0.9192, -0.8335, -0.6281]],

         [[-0.7752, -1.0903, -0.8277,  ..., -1.8431, -1.8081, -1.7381],
          [-0.8102, -0.9678, -0.6352,  ..., -1.8256, -1.7556, -1.7206],
          [-1.1779, -1.1078, -0.7577,  ..., -1.8081, -1.7731, -1.7731],
          ...,
          [-0.5826, -0.4601, -0.6176,  ..., -1.3880, -1.5105, -1.6331],
          [-0.6176, -0.3025, -0.3550,  ..., -1.7031, -1.7206, -1.4055],
          [-0.7227, -0.4251, -0.2675,  ..., -1.2654, -1.1253, -0.8978]],

         [[-1.3687, -1.5779, -1.4036,  ..., -1.5953, -1.5953, -1.5430],
          [-1.4559, -1.5430, -1.2990,  ..., -1.5604, -1.5604, -1.5081],
          [-1.5953, -1.6302, -1.4733,  ..., -1.5256, -1.5430, -1.5430],
          ...,
          [-1.4036, -1.4559, -1.5256,  ..., -1.2816, -1.2293, -1.4384],
          [-1.3861, -1.2990, -1.4733,  ..., -1.6302, -1.7347, -1.6650],
          [-1.4733, -1.3513, -1.4036,  ..., -1.6650, -1.6302, -1.5779]]],


        ...,


        [[[-1.8610, -1.9124, -1.9809,  ..., -1.9638, -1.8953, -1.8610],
          [-1.9295, -1.9809, -2.0152,  ..., -1.8439, -1.9809, -1.9124],
          [-2.0665, -2.0494, -2.0494,  ..., -1.9638, -1.9467, -1.8268],
          ...,
          [-1.9980, -2.0152, -1.9980,  ..., -0.2684, -0.3027, -0.3883],
          [-1.9980, -1.9295, -1.9295,  ...,  0.0569,  0.0398, -0.0972],
          [-2.0323, -1.9809, -1.9467,  ...,  0.4166,  0.2453,  0.1768]],

         [[-1.1078, -1.0728, -1.1429,  ..., -1.3529, -1.2829, -1.2479],
          [-1.0903, -1.0728, -1.1604,  ..., -1.1604, -1.3004, -1.2829],
          [-1.1779, -1.1429, -1.1429,  ..., -1.2654, -1.2654, -1.2129],
          ...,
          [-1.4580, -1.4755, -1.4580,  ...,  0.4678,  0.4503,  0.4328],
          [-1.4580, -1.3880, -1.3880,  ...,  0.7304,  0.7479,  0.6604],
          [-1.5280, -1.4580, -1.4230,  ...,  1.1856,  0.9580,  0.8880]],

         [[ 0.2173,  0.1302, -0.0267,  ..., -0.4973, -0.4624, -0.4275],
          [ 0.0953, -0.0615, -0.1487,  ..., -0.4450, -0.6367, -0.5670],
          [ 0.1128, -0.0790, -0.1661,  ..., -0.5495, -0.6018, -0.4973],
          ...,
          [-0.7936, -0.8458, -0.7936,  ...,  1.3154,  1.0365,  0.8274],
          [-0.7587, -0.7238, -0.7238,  ...,  1.5245,  1.5420,  1.3502],
          [-0.7064, -0.6890, -0.6541,  ...,  1.6814,  1.8034,  1.8383]]],


        [[[ 0.6734,  0.6392,  0.6563,  ...,  1.1187,  1.1187,  1.1187],
          [ 0.6563,  0.6563,  0.6563,  ...,  1.1529,  1.0844,  1.0673],
          [ 0.6734,  0.6906,  0.6906,  ...,  1.1015,  1.0673,  1.0502],
          ...,
          [ 0.8276,  0.8104,  0.7933,  ...,  0.4679,  0.5364,  0.5022],
          [ 1.2728,  1.2385,  1.2728,  ...,  0.5193,  0.5707,  0.5364],
          [ 1.3070,  1.2385,  1.2385,  ...,  0.6049,  0.5193,  0.5193]],

         [[ 0.6604,  0.6604,  0.6604,  ...,  1.0630,  1.0280,  1.0105],
          [ 0.6604,  0.6954,  0.6604,  ...,  1.0630,  1.0455,  1.0455],
          [ 0.6604,  0.6779,  0.6429,  ...,  1.0455,  1.0455,  1.0105],
          ...,
          [ 0.9055,  0.8704,  0.9055,  ...,  0.4328,  0.4328,  0.4678],
          [ 1.3957,  1.3957,  1.3957,  ...,  0.4328,  0.4153,  0.4153],
          [ 1.3782,  1.3782,  1.3431,  ...,  0.4853,  0.4678,  0.4153]],

         [[ 0.7576,  0.6008,  0.6879,  ...,  1.1411,  1.1062,  1.1411],
          [ 0.6879,  0.4962,  0.6356,  ...,  1.2631,  1.0714,  1.0888],
          [ 0.6008,  0.6879,  0.7054,  ...,  1.0714,  0.9494,  1.0714],
          ...,
          [ 0.9145,  0.8099,  0.9145,  ...,  0.2871,  0.2522,  0.4962],
          [ 1.3502,  1.5071,  1.4374,  ...,  0.3045,  0.3568,  0.4265],
          [ 1.4722,  1.5768,  1.4548,  ...,  0.4265,  0.4265,  0.1128]]],


        [[[-2.1008, -2.1179, -2.1179,  ...,  1.4440,  1.3413,  1.3070],
          [-2.1179, -2.1179, -2.0837,  ...,  1.3242,  1.2557,  1.1872],
          [-2.1179, -2.0837, -2.0665,  ...,  1.2728,  1.1358,  0.9303],
          ...,
          [-1.8782, -2.0323, -2.1008,  ..., -2.1179, -2.1179, -2.1179],
          [-2.1179, -2.1008, -2.1179,  ..., -2.1179, -2.1179, -2.1179],
          [-2.1008, -2.1008, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],

         [[-2.0182, -2.0007, -2.0007,  ...,  1.4132,  1.3431,  1.3256],
          [-2.0357, -2.0182, -1.9832,  ...,  1.4482,  1.3782,  1.2206],
          [-2.0182, -1.9657, -1.9307,  ...,  1.4482,  1.3081,  1.1506],
          ...,
          [-1.7206, -1.9307, -2.0007,  ..., -2.0357, -2.0357, -2.0357],
          [-2.0007, -2.0007, -2.0182,  ..., -2.0357, -2.0357, -2.0357],
          [-2.0182, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],

         [[-1.7870, -1.7870, -1.7870,  ...,  1.2805,  1.1934,  1.1759],
          [-1.7696, -1.7173, -1.6824,  ...,  1.2805,  1.2457,  1.1585],
          [-1.6999, -1.6476, -1.6127,  ...,  1.3677,  1.2805,  1.0888],
          ...,
          [-1.4559, -1.6302, -1.6999,  ..., -1.8044, -1.8044, -1.8044],
          [-1.7522, -1.7347, -1.7522,  ..., -1.8044, -1.8044, -1.8044],
          [-1.7870, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])
[19:57:02.695503] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:02.712411] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:04.014422] INFO: mask:  tensor([[False,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True, False,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:04.225156] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.225419] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.225838] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.226305] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.226792] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.227253] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.227721] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.228185] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.228649] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.229116] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.229583] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.230054] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.230515] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.230976] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.231445] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.231910] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.232368] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.232832] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.233298] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:04.296147] INFO: samples:  tensor([[[[ 1.0331,  1.0159,  0.9988,  ...,  1.2214,  1.2214,  1.2557],
          [ 0.8961,  0.8961,  0.8789,  ...,  1.2214,  1.2385,  1.2557],
          [ 0.9132,  0.9303,  0.9474,  ...,  1.2385,  1.2385,  1.2385],
          ...,
          [ 0.0398, -0.0116, -0.0458,  ...,  0.2967,  0.3138,  0.3481],
          [-0.1314, -0.1486, -0.1143,  ...,  0.3823,  0.3481,  0.3823],
          [ 0.0569,  0.0912,  0.0912,  ...,  0.3481,  0.3481,  0.3994]],

         [[ 0.8179,  0.7479,  0.6078,  ...,  1.0455,  1.0630,  1.0630],
          [ 0.4503,  0.4328,  0.3978,  ...,  1.0455,  1.0630,  1.0455],
          [ 0.5028,  0.5903,  0.6429,  ...,  1.0280,  1.0455,  1.0630],
          ...,
          [-0.3375, -0.4426, -0.6877,  ..., -0.0574, -0.0399, -0.0749],
          [-0.6527, -0.7052, -0.7052,  ..., -0.0399, -0.0399, -0.0399],
          [-0.3725, -0.3375, -0.2150,  ..., -0.0399, -0.0399, -0.0224]],

         [[ 0.5311,  0.4091,  0.1476,  ...,  0.9145,  0.8797,  0.8622],
          [-0.1138, -0.1661, -0.1661,  ...,  0.8622,  0.8622,  0.8622],
          [ 0.0605,  0.2173,  0.2522,  ...,  0.8622,  0.8622,  0.8448],
          ...,
          [-0.6890, -0.8633, -1.2119,  ..., -0.4450, -0.4275, -0.3578],
          [-1.1421, -1.1421, -1.2119,  ..., -0.3927, -0.3578, -0.3230],
          [-0.8284, -0.7413, -0.6193,  ..., -0.3753, -0.3578, -0.3055]]],


        [[[-0.0972, -0.0287, -0.0629,  ..., -0.3883, -0.5596, -0.5424],
          [ 0.0569,  0.1254,  0.0741,  ..., -0.4397, -0.5253, -0.5253],
          [ 0.1597,  0.1083,  0.2111,  ..., -0.3883, -0.4054, -0.3027],
          ...,
          [-0.7993, -1.4672, -0.9534,  ...,  0.0056, -0.4397, -0.5253],
          [-0.2171, -0.9192, -0.6794,  ...,  0.4337,  0.2282,  0.0912],
          [-0.5596, -0.0458, -0.6623,  ...,  0.0569,  0.5707,  0.1597]],

         [[-0.0049, -0.0049,  0.0651,  ..., -0.4076, -0.5126, -0.5826],
          [ 0.1527,  0.1877,  0.2402,  ..., -0.4251, -0.3375, -0.3901],
          [ 0.2402,  0.3452,  0.3627,  ..., -0.4776, -0.4076, -0.3025],
          ...,
          [-0.7752, -1.3004, -0.8277,  ...,  0.2927, -0.2325, -0.6877],
          [-0.1099, -0.8452, -0.6001,  ...,  1.1155,  0.7654,  0.1352],
          [-0.7927, -0.2325, -0.7227,  ...,  0.3627,  1.0280,  0.4503]],

         [[-0.0092,  0.0605,  0.0953,  ..., -0.2707, -0.3927, -0.2707],
          [ 0.1476,  0.1825,  0.1999,  ..., -0.2881, -0.2707, -0.3927],
          [ 0.1302,  0.2173,  0.2348,  ..., -0.4101, -0.3230, -0.2532],
          ...,
          [-0.5147, -1.0201, -0.7064,  ...,  0.4962,  0.0256, -0.3230],
          [ 0.0779, -0.6367, -0.3578,  ...,  1.0888,  0.8971,  0.2696],
          [-0.7238, -0.3753, -0.8981,  ...,  0.3742,  1.2805,  0.7751]]],


        [[[ 0.5536,  0.5707,  0.7591,  ...,  1.2043,  1.2385,  1.2385],
          [ 1.0331,  0.7591,  0.7933,  ...,  1.2214,  1.2557,  1.2385],
          [ 0.5707,  0.5878,  0.7419,  ...,  1.2043,  1.1872,  1.1700],
          ...,
          [ 1.0331,  1.0159,  0.9303,  ...,  0.9817,  0.9303,  0.9988],
          [ 1.1358,  1.0331,  0.9474,  ...,  0.8789,  0.8618,  0.9132],
          [ 1.0673,  1.0159,  0.9646,  ...,  0.7933,  0.9303,  0.8961]],

         [[ 0.5553,  0.4503,  0.5903,  ...,  1.0980,  1.1155,  1.1331],
          [ 0.7829,  0.6779,  0.6254,  ...,  1.0805,  1.1155,  1.0980],
          [ 0.5203,  0.4503,  0.5903,  ...,  1.0630,  1.0455,  1.0280],
          ...,
          [ 0.8179,  0.8354,  0.7304,  ...,  0.7829,  0.7304,  0.6779],
          [ 0.8880,  0.7479,  0.6604,  ...,  0.6604,  0.6779,  0.7479],
          [ 0.8704,  0.8354,  0.7479,  ...,  0.7129,  0.7129,  0.6604]],

         [[ 0.4439,  0.4439,  0.5485,  ...,  0.9145,  0.9319,  0.9145],
          [ 0.5485,  0.5659,  0.4614,  ...,  0.8622,  0.8971,  0.8797],
          [ 0.5834,  0.4439,  0.4962,  ...,  0.8274,  0.8274,  0.7925],
          ...,
          [ 0.6182,  0.6008,  0.4614,  ...,  0.5311,  0.3916,  0.3742],
          [ 0.6531,  0.4788,  0.4614,  ...,  0.3916,  0.3219,  0.3742],
          [ 0.6008,  0.5485,  0.4439,  ...,  0.3742,  0.3742,  0.3219]]],


        ...,


        [[[-1.7583, -1.9124, -1.8439,  ..., -0.7137, -0.8678, -0.6623],
          [-1.8268, -1.9295, -1.8782,  ..., -0.6281, -0.9534, -0.7650],
          [-1.7240, -1.8268, -1.7412,  ..., -0.8678, -0.7137, -0.8678],
          ...,
          [-0.1999, -0.0801, -0.1999,  ...,  0.1083,  0.1426,  0.0569],
          [-0.1828, -0.3027, -0.2684,  ...,  0.1426,  0.1426,  0.0056],
          [-0.1486, -0.1486, -0.0972,  ..., -0.0629, -0.0458, -0.0116]],

         [[-1.5455, -1.7031, -1.6331,  ..., -0.2150, -0.4251, -0.2325],
          [-1.6856, -1.8081, -1.7556,  ..., -0.1275, -0.4951, -0.3025],
          [-1.6506, -1.7381, -1.6681,  ..., -0.3725, -0.2500, -0.3901],
          ...,
          [-0.0399,  0.0826, -0.0574,  ...,  0.3102,  0.3452,  0.2577],
          [-0.0224, -0.1450, -0.1275,  ...,  0.3452,  0.3452,  0.2052],
          [ 0.0126, -0.0049,  0.0476,  ...,  0.1352,  0.1527,  0.1877]],

         [[-1.5256, -1.6824, -1.5953,  ..., -0.8807, -1.0201, -0.7761],
          [-1.5953, -1.6824, -1.6302,  ..., -0.8110, -1.1596, -0.9678],
          [-1.4907, -1.5779, -1.4733,  ..., -1.0550, -0.9504, -1.1247],
          ...,
          [ 0.1651,  0.3045,  0.1999,  ...,  0.5485,  0.5834,  0.4962],
          [ 0.1825,  0.0953,  0.1302,  ...,  0.5834,  0.5834,  0.4439],
          [ 0.2173,  0.2522,  0.3045,  ...,  0.3742,  0.3916,  0.4265]]],


        [[[-1.1418, -1.0904, -1.0562,  ..., -1.3473, -1.3815, -1.3473],
          [-1.1418, -1.1247, -1.0904,  ..., -1.3644, -1.3473, -1.2959],
          [-1.1418, -1.0904, -1.0562,  ..., -1.3302, -1.3130, -1.3130],
          ...,
          [ 1.1529,  1.1358,  0.9988,  ...,  0.9646,  1.0331,  0.9646],
          [ 0.9817,  0.9474,  0.8961,  ...,  1.0331,  0.8961,  0.8961],
          [ 1.1529,  1.1015,  1.0844,  ...,  1.0502,  0.9817,  1.0159]],

         [[-1.1604, -1.1078, -1.0903,  ..., -1.2654, -1.3179, -1.2654],
          [-1.1253, -1.1429, -1.1078,  ..., -1.3004, -1.2654, -1.2304],
          [-1.1253, -1.1078, -1.0728,  ..., -1.2654, -1.2479, -1.2479],
          ...,
          [ 1.3431,  1.3431,  1.2556,  ...,  1.2206,  1.2906,  1.2206],
          [ 1.1681,  1.1681,  1.1506,  ...,  1.2906,  1.1506,  1.1506],
          [ 1.3606,  1.3256,  1.3431,  ...,  1.3081,  1.2381,  1.2731]],

         [[-1.0898, -1.0027, -1.0027,  ..., -1.0550, -1.0898, -1.0550],
          [-1.0550, -1.0550, -1.0201,  ..., -1.1073, -1.0898, -1.0376],
          [-1.0550, -1.0550, -1.0201,  ..., -1.0201, -1.0027, -0.9853],
          ...,
          [ 1.3677,  1.3851,  1.2980,  ...,  1.4374,  1.5071,  1.4374],
          [ 1.1934,  1.1934,  1.1934,  ...,  1.5071,  1.3677,  1.3677],
          [ 1.3851,  1.3502,  1.3851,  ...,  1.5245,  1.4548,  1.4897]]],


        [[[-2.0665, -2.1179, -2.0323,  ..., -1.9809, -2.0323, -1.9638],
          [-1.9980, -1.9809, -2.0665,  ..., -1.9467, -1.9467, -1.9638],
          [-2.0152, -2.0152, -2.0837,  ..., -1.8953, -1.9295, -1.9809],
          ...,
          [-2.0323, -1.9980, -1.9467,  ..., -1.8439, -1.8097, -1.7925],
          [-1.9638, -1.9295, -1.9295,  ..., -1.9295, -1.8953, -1.8953],
          [-1.9809, -1.9467, -1.9638,  ..., -1.8610, -1.8610, -1.8610]],

         [[ 0.3803,  0.3452,  0.3627,  ...,  0.5028,  0.4853,  0.5203],
          [ 0.3803,  0.4328,  0.3803,  ...,  0.3978,  0.4853,  0.5203],
          [ 0.3452,  0.3452,  0.3627,  ...,  0.5378,  0.5203,  0.4853],
          ...,
          [-0.0224, -0.0049,  0.0301,  ...,  0.6954,  0.7479,  0.7654],
          [-0.0049,  0.0301,  0.0476,  ...,  0.6429,  0.6429,  0.6429],
          [-0.0224,  0.0126,  0.0301,  ...,  0.6954,  0.6604,  0.6254]],

         [[ 1.3677,  1.2980,  1.3502,  ...,  1.3502,  1.3502,  1.4374],
          [ 1.3677,  1.3502,  1.3154,  ...,  1.3677,  1.4374,  1.4722],
          [ 1.3154,  1.2631,  1.2805,  ...,  1.4548,  1.4548,  1.4548],
          ...,
          [ 0.6531,  0.6531,  0.7228,  ...,  1.4374,  1.4722,  1.4897],
          [ 0.6705,  0.6705,  0.6879,  ...,  1.3851,  1.4200,  1.4374],
          [ 0.6705,  0.6705,  0.7054,  ...,  1.4200,  1.4025,  1.3851]]]])
[19:57:04.302224] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:04.318901] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:05.621485] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:05.831905] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.832155] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.832610] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.833073] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.833541] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.834005] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.834473] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.834947] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.835411] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.835874] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.836344] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.836813] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.837280] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.837745] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.838213] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.838685] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.839150] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.839614] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.840083] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:05.906855] INFO: samples:  tensor([[[[-1.7754, -1.8097, -1.9124,  ...,  1.2728,  1.3755,  1.4098],
          [-1.8782, -1.7583, -1.7925,  ..., -0.1143, -0.0801, -0.0972],
          [-1.9295, -1.8953, -1.7925,  ..., -0.5596, -0.5767, -0.6109],
          ...,
          [ 1.5125,  1.3755,  1.2899,  ..., -1.6213, -1.1418, -0.2171],
          [ 1.3242,  1.2557,  1.3070,  ..., -1.0904,  0.5707,  2.2489],
          [ 1.3413,  1.3584,  1.4440,  ...,  0.3138,  2.0605,  1.7694]],

         [[-1.8256, -1.8256, -1.7731,  ...,  1.2206,  1.3081,  1.3431],
          [-1.8606, -1.7381, -1.7556,  ..., -0.6527, -0.6001, -0.6176],
          [-1.8431, -1.8081, -1.7906,  ..., -0.8803, -0.8978, -0.8978],
          ...,
          [ 1.2906,  1.2031,  1.1331,  ..., -1.7206, -1.3529, -0.3375],
          [ 1.1155,  1.0980,  1.1856,  ..., -1.3004,  0.4153,  2.3235],
          [ 1.1681,  1.2031,  1.3256,  ...,  0.1527,  2.1134,  1.7633]],

         [[-1.6302, -1.6476, -1.6302,  ...,  0.5136,  0.6356,  0.7228],
          [-1.7173, -1.5779, -1.6127,  ..., -0.9853, -0.9330, -0.8981],
          [-1.6302, -1.6476, -1.6302,  ..., -1.1421, -1.1421, -1.1073],
          ...,
          [ 0.9668,  0.8971,  0.8448,  ..., -1.6127, -1.4210, -0.6541],
          [ 0.8099,  0.8099,  0.8971,  ..., -1.3513, -0.0092,  1.9080],
          [ 0.8622,  0.9319,  1.0714,  ..., -0.2184,  1.6988,  1.8034]]],


        [[[ 2.1119,  2.0092,  2.0948,  ..., -1.8097, -1.5870, -1.6213],
          [ 2.0263,  2.1290,  2.1462,  ..., -1.7069, -1.6555, -1.5185],
          [ 2.0605,  2.1462,  2.0948,  ..., -1.6384, -1.4843, -1.4158],
          ...,
          [ 2.2318,  2.2147,  2.2147,  ...,  0.5193,  0.8276,  0.5193],
          [ 2.2489,  2.2489,  2.2489,  ...,  0.7248,  0.4851,  1.0844],
          [ 2.2489,  2.2318,  2.2318,  ...,  0.6049,  0.9817,  0.0569]],

         [[ 2.1310,  1.9909,  2.1134,  ..., -1.7731, -1.7381, -1.8957],
          [ 2.0434,  2.1485,  2.1660,  ..., -1.6681, -1.8081, -1.8081],
          [ 2.0784,  2.1660,  2.1310,  ..., -1.5805, -1.6331, -1.7206],
          ...,
          [ 2.2535,  2.2360,  2.2710,  ...,  0.5203,  0.8179,  0.4328],
          [ 2.2710,  2.2710,  2.2360,  ...,  0.7304,  0.3627,  1.0455],
          [ 2.2885,  2.2885,  2.2360,  ...,  0.5903,  0.9405,  0.0476]],

         [[ 1.2108,  1.2282,  1.3154,  ..., -1.6476, -1.6999, -1.7522],
          [ 1.1237,  1.3677,  1.3502,  ..., -1.6476, -1.7173, -1.5430],
          [ 1.1585,  1.3502,  1.2980,  ..., -1.6824, -1.6476, -1.6476],
          ...,
          [ 1.5245,  1.4200,  1.4897,  ..., -0.3753, -0.1138, -0.0267],
          [ 1.5594,  1.4722,  1.6291,  ...,  0.0082, -0.4101,  0.5136],
          [ 1.5071,  1.5768,  1.6291,  ...,  0.0082, -0.2532, -0.8284]]],


        [[[-1.6727, -1.8097, -1.6727,  ...,  1.1872, -0.7137, -0.5767],
          [-1.6555, -1.7583, -1.7925,  ...,  2.0605,  0.3652, -0.8849],
          [-1.7069, -1.6727, -1.7412,  ...,  2.2489,  1.7865, -0.2513],
          ...,
          [ 0.2282,  0.1083, -0.0801,  ..., -1.7754, -1.7583, -1.2445],
          [ 0.1768,  0.0569, -0.0801,  ..., -2.0494, -1.7240, -2.0323],
          [ 0.1083,  0.1426, -0.0629,  ..., -1.7069, -1.8097, -1.2103]],

         [[-1.6155, -1.7206, -1.6856,  ...,  1.3782, -0.5651, -0.3725],
          [-1.5630, -1.6155, -1.4405,  ...,  2.2360,  0.5203, -0.7052],
          [-1.5455, -1.4755, -1.5980,  ...,  2.4111,  1.9559, -0.1099],
          ...,
          [ 0.4328,  0.2577, -0.0049,  ..., -1.7556, -1.8431, -1.0378],
          [ 0.3627,  0.2402, -0.0224,  ..., -1.9657, -1.7381, -1.9657],
          [ 0.3803,  0.3627,  0.0476,  ..., -1.6155, -1.7381, -1.3179]],

         [[-1.2119, -1.6476, -1.3513,  ...,  1.4374, -0.4973, -0.2532],
          [-1.3687, -1.5779, -1.3687,  ...,  2.4483,  0.5485, -0.7587],
          [-1.5953, -1.3513, -1.3513,  ...,  2.2740,  2.1346,  0.0256],
          ...,
          [ 0.5834,  0.5136,  0.2696,  ..., -1.4559, -1.4907, -0.9330],
          [ 0.4614,  0.4614,  0.1302,  ..., -1.7347, -1.4384, -1.6999],
          [ 0.3742,  0.4614,  0.0256,  ..., -1.2990, -1.5779, -1.0201]]],


        ...,


        [[[-0.3712, -0.3883, -0.3712,  ...,  0.0912,  0.1083,  0.1083],
          [-0.4226, -0.4226, -0.3883,  ...,  0.1597,  0.1426,  0.1426],
          [-0.4397, -0.4054, -0.3883,  ...,  0.1768,  0.1768,  0.1254],
          ...,
          [-0.0972,  0.0227,  0.1254,  ...,  0.2796,  0.2282,  0.4679],
          [ 0.0056,  0.0398, -0.0287,  ..., -0.1486,  0.0741,  0.2453],
          [-0.2684, -0.2342, -0.2856,  ..., -0.2684, -0.1486, -0.0116]],

         [[ 0.0301,  0.0126, -0.0049,  ...,  0.2577,  0.2402,  0.2402],
          [-0.0224, -0.0224, -0.0049,  ...,  0.2752,  0.2752,  0.2577],
          [-0.0399, -0.0224, -0.0224,  ...,  0.2927,  0.2927,  0.2927],
          ...,
          [ 0.1176,  0.2052,  0.2752,  ...,  0.3803,  0.3452,  0.4678],
          [ 0.1352,  0.1877,  0.2577,  ...,  0.1702,  0.1702,  0.4328],
          [ 0.0826,  0.1527,  0.0476,  ..., -0.0224,  0.1001,  0.1001]],

         [[ 0.5311,  0.5136,  0.5136,  ...,  0.5659,  0.5485,  0.5659],
          [ 0.4788,  0.4788,  0.5136,  ...,  0.5659,  0.5485,  0.5659],
          [ 0.4788,  0.5136,  0.4788,  ...,  0.5834,  0.5659,  0.5485],
          ...,
          [ 0.5311,  0.6182,  0.6008,  ...,  0.5659,  0.5311,  0.7576],
          [ 0.5659,  0.5659,  0.6356,  ...,  0.5136,  0.5136,  0.5659],
          [ 0.4788,  0.4265,  0.4614,  ...,  0.5136,  0.5311,  0.6182]]],


        [[[ 1.5125,  1.6324,  1.8208,  ..., -0.9534, -0.9705, -0.9705],
          [ 1.6838,  1.7180,  1.7009,  ..., -0.9705, -0.9705, -0.9705],
          [ 1.6838,  1.7009,  1.7009,  ..., -1.0219, -0.9534, -0.9363],
          ...,
          [ 1.8208,  1.8208,  1.8722,  ...,  0.0398,  0.0056, -0.0116],
          [ 1.5125,  1.6324,  1.7009,  ...,  0.2282,  0.2453,  0.2111],
          [ 0.9303,  1.3927,  1.5297,  ...,  0.3994,  0.3994,  0.3309]],

         [[ 1.9734,  2.0959,  2.2885,  ..., -0.7752, -0.7752, -0.7577],
          [ 2.1660,  2.2010,  2.1835,  ..., -0.8277, -0.8277, -0.7927],
          [ 2.1660,  2.1835,  2.1835,  ..., -0.8803, -0.8277, -0.7752],
          ...,
          [ 2.0959,  2.0959,  2.1485,  ...,  0.1176,  0.0826,  0.0826],
          [ 1.7983,  1.9384,  1.9909,  ...,  0.3452,  0.3627,  0.3277],
          [ 1.2031,  1.6758,  1.8158,  ...,  0.5203,  0.5203,  0.4503]],

         [[ 2.3611,  2.4831,  2.6400,  ..., -0.6193, -0.6193, -0.5844],
          [ 2.5529,  2.5877,  2.5703,  ..., -0.6715, -0.6193, -0.5844],
          [ 2.5529,  2.5703,  2.5703,  ..., -0.6890, -0.6018, -0.5321],
          ...,
          [ 2.4134,  2.3960,  2.4657,  ...,  0.3045,  0.2696,  0.2522],
          [ 2.1171,  2.2566,  2.3088,  ...,  0.5311,  0.5485,  0.5136],
          [ 1.5245,  1.9951,  2.1346,  ...,  0.7054,  0.7054,  0.6356]]],


        [[[ 0.3652,  0.3481,  0.3481,  ...,  2.0777,  2.0263,  1.9578],
          [ 0.4166,  0.4166,  0.4166,  ...,  2.0777,  2.0605,  1.9235],
          [ 0.4337,  0.4337,  0.4166,  ...,  2.1119,  2.0777,  1.9578],
          ...,
          [-1.4500, -1.4672, -1.4329,  ..., -0.5596, -0.6281, -0.7822],
          [-1.4329, -1.4672, -1.3987,  ..., -0.4226, -0.8164, -0.7993],
          [-1.5014, -1.5185, -1.4672,  ..., -0.4397, -0.5082, -0.5082]],

         [[ 0.2227,  0.1877,  0.1877,  ...,  1.5182,  1.4657,  1.4307],
          [ 0.2752,  0.2577,  0.2577,  ...,  1.5182,  1.5007,  1.3957],
          [ 0.2752,  0.2927,  0.2577,  ...,  1.5532,  1.5182,  1.3957],
          ...,
          [-1.3704, -1.3704, -1.3354,  ..., -0.7402, -0.8102, -0.9153],
          [-1.3529, -1.3704, -1.3004,  ..., -0.7752, -0.9853, -0.9503],
          [-1.4580, -1.4755, -1.4055,  ..., -0.8277, -0.8627, -0.8452]],

         [[ 0.1651,  0.1302,  0.1302,  ...,  1.0365,  0.9842,  0.9668],
          [ 0.2173,  0.1999,  0.1999,  ...,  1.0365,  1.0191,  0.9319],
          [ 0.2173,  0.2173,  0.1825,  ...,  1.0714,  1.0365,  0.9145],
          ...,
          [-1.6999, -1.7347, -1.6999,  ..., -1.4210, -1.4733, -1.4559],
          [-1.6650, -1.6999, -1.6824,  ..., -1.2990, -1.5430, -1.4907],
          [-1.7696, -1.7522, -1.6999,  ..., -1.4210, -1.4559, -1.4210]]]])
[19:57:05.912746] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:05.929689] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:07.233293] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True, False,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True, False,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:07.444259] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.444523] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.444950] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.445414] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.445876] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.446346] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.446824] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.447285] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.447749] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.448220] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.448686] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.449161] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.449619] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.450088] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.450558] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.451028] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.451491] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.451963] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.452429] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:07.515047] INFO: samples:  tensor([[[[-0.1828, -0.1657, -0.1657,  ..., -0.1828, -0.3027, -0.3883],
          [-0.0801,  0.0569,  0.1426,  ..., -0.1999, -0.3198, -0.4054],
          [ 0.3652,  0.3994,  0.3823,  ..., -0.2342, -0.2684, -0.3712],
          ...,
          [ 1.3755,  1.3242,  1.3584,  ...,  0.0398,  0.0569, -0.0458],
          [ 1.2899,  1.4098,  1.3927,  ...,  0.1083, -0.1143,  0.2282],
          [ 1.3413,  1.4440,  1.4269,  ..., -0.0801, -0.1143,  0.1597]],

         [[-0.3200, -0.2850, -0.2850,  ..., -0.4251, -0.5301, -0.5826],
          [-0.1450,  0.0126,  0.1527,  ..., -0.4426, -0.5476, -0.6001],
          [ 0.3277,  0.5028,  0.5553,  ..., -0.4776, -0.4776, -0.6001],
          ...,
          [ 1.5182,  1.4482,  1.4307,  ..., -2.0007, -1.9657, -1.9832],
          [ 1.4657,  1.5532,  1.5007,  ..., -1.9657, -1.9482, -1.9482],
          [ 1.4832,  1.5532,  1.5357,  ..., -1.9657, -1.9482, -1.8957]],

         [[-0.1835, -0.1835, -0.1312,  ..., -0.3927, -0.4973, -0.5670],
          [-0.1487, -0.0092,  0.1128,  ..., -0.3927, -0.4973, -0.6193],
          [ 0.5136,  0.5834,  0.8274,  ..., -0.4275, -0.4450, -0.5844],
          ...,
          [ 1.4897,  1.4548,  1.4025,  ..., -1.5953, -1.5604, -1.6302],
          [ 1.4374,  1.5245,  1.4374,  ..., -1.5430, -1.6476, -1.5430],
          [ 1.4374,  1.5420,  1.4897,  ..., -1.5953, -1.6650, -1.5256]]],


        [[[-0.5253, -0.6109, -0.6281,  ...,  1.0159,  0.9817,  0.9817],
          [-0.1486, -0.0458,  0.1254,  ...,  1.0159,  0.9646,  0.9817],
          [ 0.5707,  0.6049,  0.6563,  ...,  1.0502,  1.0502,  1.0331],
          ...,
          [ 0.3994,  0.4166,  0.4337,  ...,  0.4166,  0.4508,  0.5022],
          [ 0.3994,  0.3994,  0.3994,  ...,  0.3481,  0.3823,  0.4337],
          [ 0.4166,  0.3823,  0.3994,  ...,  0.4337,  0.4679,  0.4337]],

         [[-0.4601, -0.5476, -0.6001,  ...,  1.2031,  1.1856,  1.2031],
          [-0.1099,  0.0126,  0.1527,  ...,  1.2031,  1.1681,  1.1681],
          [ 0.7654,  0.7829,  0.8004,  ...,  1.2206,  1.2206,  1.2206],
          ...,
          [ 0.6078,  0.6078,  0.6779,  ...,  0.6078,  0.6429,  0.6779],
          [ 0.6254,  0.6254,  0.6779,  ...,  0.5378,  0.5728,  0.6254],
          [ 0.6779,  0.6254,  0.6779,  ...,  0.6078,  0.6254,  0.6254]],

         [[-0.8284, -0.9504, -1.0027,  ...,  1.4374,  1.4025,  1.4025],
          [-0.1138,  0.0779,  0.2348,  ...,  1.4025,  1.3677,  1.3677],
          [ 0.8274,  0.8448,  0.8797,  ...,  1.4374,  1.4025,  1.4200],
          ...,
          [ 0.9145,  0.9145,  0.9668,  ...,  0.8099,  0.8448,  0.8797],
          [ 0.9145,  0.9145,  0.9494,  ...,  0.7576,  0.7751,  0.8274],
          [ 0.9494,  0.9145,  0.9494,  ...,  0.8448,  0.8622,  0.8274]]],


        [[[ 0.5364, -0.4911, -0.5253,  ...,  1.6495,  1.6667,  1.6667],
          [ 0.3994, -0.4054, -0.4739,  ...,  1.6153,  1.5982,  1.6153],
          [ 0.3138, -0.3541, -0.4739,  ...,  1.5810,  1.5639,  1.5982],
          ...,
          [-1.5699, -1.5699, -1.5699,  ...,  0.2967,  0.2282,  0.2453],
          [-1.5699, -1.5699, -1.5528,  ..., -0.0972, -0.1143, -0.1143],
          [-1.5699, -1.5699, -1.5528,  ..., -0.1828, -0.2513, -0.2171]],

         [[ 0.6604, -0.3025, -0.2150,  ...,  1.7808,  1.7983,  1.7983],
          [ 0.5378, -0.1975, -0.1625,  ...,  1.7458,  1.7458,  1.7633],
          [ 0.4853, -0.1275, -0.1450,  ...,  1.7108,  1.6933,  1.7283],
          ...,
          [-1.3004, -1.3004, -1.3004,  ...,  0.1702,  0.1001,  0.1176],
          [-1.3004, -1.3004, -1.2829,  ..., -0.2325, -0.2500, -0.2500],
          [-1.3004, -1.3004, -1.2829,  ..., -0.3200, -0.3901, -0.3550]],

         [[ 0.8274, -0.2532, -0.3055,  ...,  0.4962,  0.4788,  0.4788],
          [ 0.6182, -0.1835, -0.2707,  ...,  0.4265,  0.4091,  0.4091],
          [ 0.4091, -0.2184, -0.2707,  ...,  0.4091,  0.3916,  0.4265],
          ...,
          [-1.0376, -1.0376, -1.0376,  ...,  0.3219,  0.2348,  0.2522],
          [-1.0376, -1.0376, -1.0201,  ..., -0.0964, -0.1138, -0.1138],
          [-1.0376, -1.0376, -1.0201,  ..., -0.1835, -0.2532, -0.2184]]],


        ...,


        [[[-0.2513, -0.2684, -0.2856,  ..., -0.0287, -0.0287, -0.0458],
          [-0.2171, -0.2513, -0.2684,  ..., -0.0116, -0.0116, -0.0458],
          [-0.2342, -0.2513, -0.2513,  ...,  0.0056, -0.0116, -0.0458],
          ...,
          [-1.3130, -1.2788, -1.2788,  ..., -1.4158, -1.3987, -1.3644],
          [-1.2617, -1.2445, -1.2788,  ..., -1.5185, -1.5014, -1.4672],
          [-1.2959, -1.3302, -1.3302,  ..., -1.6042, -1.5699, -1.5528]],

         [[-0.0924, -0.1099, -0.1450,  ...,  0.1352,  0.1176,  0.1001],
          [-0.0574, -0.0924, -0.1450,  ...,  0.1527,  0.1352,  0.1001],
          [-0.0749, -0.0924, -0.1275,  ...,  0.1527,  0.1352,  0.1001],
          ...,
          [-1.5455, -1.4930, -1.4930,  ..., -1.6155, -1.6155, -1.5980],
          [-1.4930, -1.4755, -1.4930,  ..., -1.6856, -1.6856, -1.6681],
          [-1.5280, -1.5630, -1.5455,  ..., -1.7556, -1.7206, -1.7206]],

         [[ 0.1302,  0.0953,  0.0779,  ...,  0.3568,  0.4091,  0.3742],
          [ 0.1476,  0.1128,  0.0779,  ...,  0.3742,  0.4265,  0.3916],
          [ 0.1302,  0.1128,  0.0605,  ...,  0.3916,  0.4265,  0.4091],
          ...,
          [-1.5430, -1.4907, -1.5081,  ..., -1.5779, -1.5953, -1.5953],
          [-1.5081, -1.4907, -1.5081,  ..., -1.6127, -1.6127, -1.6302],
          [-1.5430, -1.5779, -1.5430,  ..., -1.6650, -1.6650, -1.6476]]],


        [[[-0.4226, -0.7822, -0.7822,  ...,  0.6392,  0.6906,  0.7419],
          [ 0.0912, -0.2513, -0.4568,  ...,  0.7591,  0.7591,  0.6563],
          [ 0.7248,  0.3994, -0.0801,  ...,  0.6563,  0.5536,  0.4679],
          ...,
          [ 1.6153,  1.7352,  1.6667,  ...,  0.4851,  0.3652,  0.7591],
          [ 1.5639,  1.7694,  1.7694,  ...,  0.0056,  0.5878,  1.3755],
          [ 1.6667,  1.6495,  1.6838,  ...,  0.4851,  1.3927,  1.8037]],

         [[-0.2325, -0.6176, -0.5826,  ...,  0.8354,  0.8880,  0.9755],
          [ 0.3978,  0.0301, -0.1450,  ...,  0.9580,  0.9580,  0.9405],
          [ 1.0630,  0.7654,  0.3277,  ...,  0.8004,  0.7304,  0.6429],
          ...,
          [ 1.5182,  1.6057,  1.5882,  ...,  0.4153,  0.2577,  0.6779],
          [ 1.4657,  1.5707,  1.6408,  ..., -0.0224,  0.5028,  1.3957],
          [ 1.5707,  1.5182,  1.4482,  ...,  0.4853,  1.2206,  1.8683]],

         [[-0.7238, -0.8110, -0.8633,  ..., -0.0790,  0.0779,  0.0431],
          [-0.6018, -0.6193, -0.6367,  ...,  0.0605,  0.1651,  0.0431],
          [ 0.0953, -0.0267, -0.2707,  ...,  0.0779, -0.0267, -0.0964],
          ...,
          [ 0.8274,  1.0714,  1.2980,  ...,  0.2871,  0.0431,  0.0431],
          [ 0.6356,  0.8622,  1.2631,  ...,  0.1476,  0.3219,  0.8971],
          [ 0.7054,  0.7402,  0.9842,  ...,  0.6531,  1.0017,  1.4722]]],


        [[[ 1.7694,  1.4269,  0.5878,  ...,  1.4269, -0.3369, -1.0562],
          [ 1.7352,  1.3927,  0.6049,  ...,  1.4269, -0.3369, -1.0390],
          [ 1.6324,  1.3413,  0.6392,  ...,  1.4440, -0.3198, -1.0219],
          ...,
          [-1.3644, -1.3473, -1.2959,  ..., -1.9980, -1.9980, -1.9980],
          [-1.4500, -1.4672, -1.4843,  ..., -1.9809, -1.9980, -1.9980],
          [-1.4843, -1.5185, -1.5699,  ..., -1.9809, -1.9980, -1.9980]],

         [[ 1.6758,  1.3081,  0.4678,  ...,  1.3782, -0.4251, -1.1429],
          [ 1.6408,  1.3081,  0.5028,  ...,  1.4132, -0.3901, -1.1078],
          [ 1.5532,  1.2731,  0.5728,  ...,  1.4482, -0.3375, -1.0553],
          ...,
          [-1.0728, -1.0553, -1.0028,  ..., -1.9307, -1.9307, -1.9307],
          [-1.1604, -1.1779, -1.1954,  ..., -1.9132, -1.9307, -1.9307],
          [-1.1954, -1.2304, -1.2829,  ..., -1.9132, -1.9307, -1.9307]],

         [[ 1.8034,  1.4897,  0.6531,  ...,  1.8383,  0.0605, -0.6541],
          [ 1.7860,  1.4897,  0.6879,  ...,  1.8383,  0.0605, -0.6541],
          [ 1.7337,  1.4722,  0.7925,  ...,  1.8557,  0.0779, -0.6541],
          ...,
          [-0.9504, -0.9330, -0.8981,  ..., -1.7347, -1.7347, -1.7347],
          [-1.0376, -1.0550, -1.0724,  ..., -1.7173, -1.7347, -1.7347],
          [-1.0724, -1.1073, -1.1596,  ..., -1.7173, -1.7347, -1.7347]]]])
[19:57:07.521159] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:07.538001] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:08.841801] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False,  True,  True],
        [ True, False,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:09.052538] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.052803] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.053218] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.053685] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.054146] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.054615] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.055078] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.055551] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.056020] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.056489] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.056954] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.057427] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.057886] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.058353] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.058829] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.059293] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.059752] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.060218] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.060688] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:09.126688] INFO: samples:  tensor([[[[-1.4672, -1.4843, -1.4843,  ..., -1.2103, -1.0562, -0.8335],
          [-1.4672, -1.5014, -1.4843,  ..., -1.1418, -1.0562, -1.5357],
          [-1.4672, -1.5014, -1.5185,  ..., -1.0562, -1.2274, -1.3473],
          ...,
          [-1.7583, -1.4158, -1.4672,  ..., -1.2445, -1.4158, -1.4843],
          [-1.7069, -1.4843, -1.5014,  ..., -1.2788, -1.4329, -1.5699],
          [-1.6727, -1.4843, -1.6042,  ..., -1.2445, -1.4329, -1.6384]],

         [[-1.5980, -1.5630, -1.5630,  ..., -0.4776, -0.3200, -0.1975],
          [-1.5980, -1.5630, -1.5630,  ..., -0.5301, -0.4601, -1.0553],
          [-1.5980, -1.5980, -1.5980,  ..., -0.6176, -0.7927, -1.0203],
          ...,
          [-1.6681, -1.4580, -1.5280,  ..., -1.3354, -1.4755, -1.4580],
          [-1.6155, -1.4930, -1.5455,  ..., -1.3529, -1.4930, -1.5455],
          [-1.5805, -1.4930, -1.6155,  ..., -1.3354, -1.4930, -1.6155]],

         [[-1.4733, -1.4907, -1.4907,  ..., -1.4733, -1.3861, -1.0201],
          [-1.4733, -1.5081, -1.4907,  ..., -1.2990, -1.2467, -1.5604],
          [-1.4733, -1.5081, -1.5256,  ..., -1.0376, -1.2990, -1.3164],
          ...,
          [-1.4384, -1.2119, -1.2816,  ..., -1.1073, -1.2816, -1.3687],
          [-1.3861, -1.2467, -1.2990,  ..., -1.1421, -1.2990, -1.4559],
          [-1.3513, -1.2641, -1.3861,  ..., -1.1073, -1.2990, -1.5256]]],


        [[[-0.9534, -0.8678, -0.8507,  ..., -0.9363, -1.0390, -1.2103],
          [-0.9705, -0.8849, -0.7993,  ..., -0.9705, -1.0733, -1.1760],
          [-0.8678, -0.8335, -0.7993,  ..., -0.7993, -1.0048, -1.1589],
          ...,
          [ 1.1015,  1.0844,  1.1015,  ..., -0.3027, -0.3712, -0.4226],
          [ 1.0331,  1.0502,  1.0673,  ..., -0.3712, -0.3712, -0.3883],
          [ 0.9988,  0.9817,  1.0159,  ..., -0.3712, -0.3712, -0.3541]],

         [[-1.1954, -1.1429, -1.1604,  ..., -1.2129, -1.3179, -1.4930],
          [-1.2654, -1.1954, -1.1253,  ..., -1.2829, -1.3880, -1.4755],
          [-1.2129, -1.1954, -1.1604,  ..., -1.1604, -1.3354, -1.4580],
          ...,
          [ 1.0805,  1.0630,  1.0805,  ..., -0.7052, -0.7752, -0.8277],
          [ 1.0105,  1.0280,  1.0455,  ..., -0.7752, -0.7752, -0.7752],
          [ 0.9755,  0.9580,  1.0105,  ..., -0.7402, -0.7402, -0.7402]],

         [[-1.2641, -1.2119, -1.2467,  ..., -1.2293, -1.3339, -1.4907],
          [-1.3164, -1.2641, -1.1944,  ..., -1.2990, -1.3861, -1.4559],
          [-1.2467, -1.2467, -1.2293,  ..., -1.1596, -1.3164, -1.4384],
          ...,
          [ 1.2805,  1.2631,  1.2805,  ..., -0.6715, -0.7413, -0.7936],
          [ 1.2108,  1.2282,  1.2457,  ..., -0.7413, -0.7413, -0.7413],
          [ 1.1759,  1.1585,  1.1934,  ..., -0.7238, -0.7238, -0.7238]]],


        [[[ 1.7180,  1.7352,  1.7180,  ...,  1.8208,  1.8037,  1.8893],
          [ 1.7009,  1.6838,  1.7009,  ...,  1.8208,  1.8208,  1.8893],
          [ 1.7009,  1.6838,  1.7009,  ...,  1.8037,  1.8208,  1.8550],
          ...,
          [ 0.9817,  0.8618,  0.7419,  ..., -1.4329, -1.4158, -1.3987],
          [ 1.1700,  1.1358,  1.1358,  ..., -1.1932, -1.1932, -1.1418],
          [ 0.4508,  0.4508,  0.5022,  ..., -0.3541, -0.3541, -0.3369]],

         [[ 1.6758,  1.6933,  1.6583,  ...,  1.8859,  1.8683,  1.9034],
          [ 1.6933,  1.6758,  1.6408,  ...,  1.8683,  1.8683,  1.8859],
          [ 1.6933,  1.6758,  1.6408,  ...,  1.8683,  1.8859,  1.8859],
          ...,
          [ 0.6779,  0.5378,  0.3978,  ..., -1.3880, -1.3704, -1.3529],
          [ 0.4678,  0.4153,  0.3803,  ..., -1.4580, -1.4755, -1.4230],
          [-1.1429, -1.1429, -1.1604,  ..., -1.7206, -1.7206, -1.7381]],

         [[ 0.7054,  0.7054,  0.6705,  ...,  0.7054,  0.7054,  0.8797],
          [ 0.7054,  0.6705,  0.6182,  ...,  0.6705,  0.6879,  0.8274],
          [ 0.7054,  0.6705,  0.6182,  ...,  0.7576,  0.7751,  0.8274],
          ...,
          [-0.4973, -0.6367, -0.7587,  ..., -1.4907, -1.4907, -1.4733],
          [-0.4275, -0.4798, -0.4973,  ..., -1.6302, -1.6476, -1.5604],
          [-1.3861, -1.3861, -1.3861,  ..., -1.6650, -1.6824, -1.6302]]],


        ...,


        [[[-0.6794, -0.7650, -0.7993,  ..., -0.5767, -0.6281, -0.6623],
          [-0.6965, -0.7650, -0.7993,  ..., -0.5938, -0.6623, -0.6623],
          [-0.6965, -0.7479, -0.7822,  ..., -0.6109, -0.6794, -0.6794],
          ...,
          [-1.1418, -1.1247, -1.1075,  ...,  0.7591,  0.8104,  0.8618],
          [-1.1418, -1.1418, -1.1247,  ...,  0.9132,  0.8104,  0.7591],
          [-1.1589, -1.1418, -1.1247,  ...,  0.8104,  0.7419,  0.7591]],

         [[-0.5826, -0.6702, -0.7052,  ..., -0.4776, -0.5301, -0.5651],
          [-0.6001, -0.6702, -0.7052,  ..., -0.4951, -0.5651, -0.5651],
          [-0.6001, -0.6527, -0.6877,  ..., -0.5126, -0.5826, -0.5826],
          ...,
          [-1.0028, -0.9853, -0.9678,  ...,  0.4153,  0.4853,  0.5378],
          [-1.0028, -1.0028, -0.9853,  ...,  0.5903,  0.5028,  0.4503],
          [-1.0203, -1.0028, -0.9853,  ...,  0.4853,  0.4328,  0.4503]],

         [[-0.4450, -0.5321, -0.5670,  ..., -0.3404, -0.3927, -0.4275],
          [-0.4624, -0.5321, -0.5670,  ..., -0.3578, -0.4275, -0.4101],
          [-0.4624, -0.5147, -0.5495,  ..., -0.3578, -0.4450, -0.4275],
          ...,
          [-0.8284, -0.8110, -0.8284,  ...,  0.3568,  0.4091,  0.4788],
          [-0.8284, -0.8284, -0.8110,  ...,  0.6008,  0.5136,  0.4265],
          [-0.8458, -0.8284, -0.8110,  ...,  0.4962,  0.4439,  0.4614]]],


        [[[ 1.2214,  1.3070,  0.5707,  ..., -0.0287, -0.5253, -0.1314],
          [ 1.2385,  1.2899,  0.6049,  ..., -0.6109, -0.6281,  0.1597],
          [ 1.1529,  1.2557,  0.6221,  ..., -0.1999, -0.2342,  0.5364],
          ...,
          [ 1.5810,  1.6153,  1.6324,  ..., -1.4329, -1.2445, -1.1932],
          [ 1.5468,  1.5982,  1.6153,  ..., -1.2959, -1.0904, -1.0733],
          [ 1.5810,  1.6153,  1.6324,  ..., -1.3987, -1.2959, -1.4672]],

         [[ 0.9930,  1.0805,  0.3627,  ..., -0.0224, -0.4426, -0.0049],
          [ 0.9930,  1.0805,  0.3978,  ..., -0.7752, -0.7227,  0.0826],
          [ 0.9580,  1.0455,  0.4153,  ..., -0.2850, -0.1099,  0.6254],
          ...,
          [ 1.2206,  1.2381,  1.3081,  ..., -1.0728, -0.8627, -0.9153],
          [ 1.2031,  1.2906,  1.3431,  ..., -0.8803, -0.7752, -0.8803],
          [ 1.2556,  1.3081,  1.3431,  ..., -1.0028, -0.9153, -1.1779]],

         [[ 0.6705,  0.7402, -0.0441,  ...,  0.0082, -0.4624, -0.1835],
          [ 0.6705,  0.7576, -0.0267,  ..., -0.7064, -0.7064, -0.0790],
          [ 0.6356,  0.7751,  0.0082,  ..., -0.2358, -0.1487,  0.4265],
          ...,
          [ 1.1759,  1.1759,  1.2457,  ..., -1.2641, -1.1421, -1.2119],
          [ 1.1934,  1.2282,  1.2631,  ..., -1.1596, -1.0201, -1.0550],
          [ 1.2282,  1.2980,  1.2980,  ..., -1.1944, -1.1944, -1.3861]]],


        [[[ 0.2282,  0.2453,  0.3652,  ..., -0.1999, -0.7137, -0.4054],
          [ 0.1083,  0.1254,  0.2282,  ...,  0.1254, -0.4397, -0.6109],
          [ 0.0398,  0.0398,  0.0569,  ...,  0.3994, -0.9192, -0.8507],
          ...,
          [-1.9809, -1.4158, -0.9363,  ..., -0.8335, -1.2617, -0.9534],
          [-1.8097, -1.2788, -1.4672,  ..., -1.4158, -1.1247, -1.0733],
          [-1.4672, -0.9192, -1.5185,  ..., -1.3987, -1.2617, -1.0904]],

         [[ 1.0105,  1.0280,  1.0805,  ...,  0.1352, -0.4076,  0.0476],
          [ 0.8880,  0.9230,  0.9930,  ...,  0.4503, -0.3025, -0.2150],
          [ 0.8704,  0.8704,  0.8704,  ...,  0.7829, -0.7227, -0.4776],
          ...,
          [-1.5980, -1.0553, -0.4251,  ..., -0.1800, -0.6001, -0.2850],
          [-1.4755, -0.8452, -0.8978,  ..., -0.8452, -0.4776, -0.3725],
          [-1.1253, -0.3901, -1.0028,  ..., -0.7752, -0.6352, -0.4601]],

         [[ 2.6226,  2.6051,  2.6226,  ...,  1.4025,  0.7054,  0.6879],
          [ 2.5529,  2.5703,  2.5529,  ...,  2.0300,  0.9145,  0.3393],
          [ 2.6226,  2.6051,  2.5529,  ...,  2.2914,  0.3393,  0.1825],
          ...,
          [-1.6999, -1.3513, -0.9678,  ..., -1.0027, -1.4559, -1.1596],
          [-1.5779, -1.2119, -1.4384,  ..., -1.5604, -1.2816, -1.2641],
          [-1.2816, -1.1073, -1.5953,  ..., -1.5256, -1.4210, -1.2816]]]])
[19:57:09.133087] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:09.149946] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:10.453145] INFO: mask:  tensor([[ True, False,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True, False,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True, False,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:10.663833] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.664101] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.664524] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.664988] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.665453] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.665922] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.666383] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.666855] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.667319] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.667788] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.668253] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.668727] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.669190] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.669660] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.670130] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.670600] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.671061] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.671524] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.671989] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:10.730663] INFO: samples:  tensor([[[[ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2147,  2.1804],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ..., -0.6965, -0.6109, -0.6281],
          [ 2.2489,  2.2489,  2.2489,  ..., -0.6794, -0.7137, -0.8335],
          [ 2.2489,  2.2489,  2.2489,  ..., -0.6281, -0.7479, -0.9534]],

         [[ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.3936,  2.3761,  2.3410],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ..., -0.2150, -0.1800, -0.1275],
          [ 2.4286,  2.4286,  2.4286,  ..., -0.1975, -0.2675, -0.3200],
          [ 2.4286,  2.4286,  2.4286,  ..., -0.1275, -0.2675, -0.3901]],

         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6226],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ..., -0.2184, -0.1487, -0.0790],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.2010, -0.2358, -0.2707],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.1312, -0.2358, -0.3578]]],


        [[[ 0.3138,  0.3138,  0.3309,  ..., -0.8335, -0.6623, -0.5767],
          [ 0.3138,  0.3138,  0.3138,  ..., -0.8507, -0.6623, -0.5767],
          [ 0.3309,  0.2967,  0.2967,  ..., -0.7479, -0.7137, -0.6623],
          ...,
          [ 0.7762,  0.0741, -0.1143,  ...,  0.6563,  0.5707,  0.5022],
          [ 0.0398, -0.1999,  0.0741,  ...,  0.5707,  0.1939,  0.1939],
          [ 0.1597, -0.3883, -0.1143,  ...,  0.2624, -0.2171,  0.1597]],

         [[-0.5826, -0.6176, -0.6176,  ..., -1.2129, -1.1253, -1.0728],
          [-0.5826, -0.6001, -0.6001,  ..., -1.1429, -1.0903, -1.0378],
          [-0.5651, -0.6176, -0.6176,  ..., -1.0728, -1.1078, -1.0728],
          ...,
          [-0.2675, -0.5826, -0.6527,  ...,  0.3277,  0.4678,  0.3978],
          [-0.6001, -0.5651, -0.4251,  ...,  0.2752,  0.1001,  0.1001],
          [-0.6702, -0.8978, -0.5826,  ...,  0.1877, -0.4776,  0.0301]],

         [[-1.3339, -1.3513, -1.3339,  ..., -1.3339, -1.3339, -1.3687],
          [-1.3339, -1.3164, -1.3164,  ..., -1.4036, -1.4733, -1.4210],
          [-1.3164, -1.3513, -1.3339,  ..., -1.2641, -1.3164, -1.3164],
          ...,
          [-0.8110, -1.1596, -1.2293,  ..., -0.0790,  0.2696,  0.3219],
          [-0.9853, -0.9504, -0.9330,  ...,  0.1302, -0.0615, -0.2707],
          [-1.2293, -1.2119, -1.0201,  ..., -0.0790, -0.8110, -0.2532]]],


        [[[ 0.7419,  0.8104,  0.8789,  ..., -1.3815, -1.4158, -1.4329],
          [ 0.7762,  0.8618,  0.8789,  ..., -1.3644, -1.4329, -1.4158],
          [ 0.8789,  0.8789,  0.9132,  ..., -1.3987, -1.3987, -1.3815],
          ...,
          [ 1.7009,  1.6495,  1.7694,  ...,  1.2214,  1.1529,  1.1700],
          [ 1.6838,  1.6495,  1.7009,  ...,  1.1529,  1.1872,  1.2043],
          [ 1.6495,  1.6495,  1.7180,  ...,  1.1700,  1.2043,  1.2043]],

         [[ 0.8880,  0.9755,  1.0455,  ..., -1.2829, -1.3004, -1.3354],
          [ 0.9405,  1.0280,  1.0455,  ..., -1.3004, -1.3004, -1.3004],
          [ 1.0455,  1.0630,  1.0805,  ..., -1.3179, -1.2654, -1.2479],
          ...,
          [ 1.8859,  1.8158,  1.9384,  ...,  1.3256,  1.2556,  1.2031],
          [ 1.8859,  1.8333,  1.8508,  ...,  1.2206,  1.2906,  1.2731],
          [ 1.8508,  1.8333,  1.8859,  ...,  1.1856,  1.2206,  1.2381]],

         [[ 1.0365,  1.0714,  1.1411,  ..., -1.3339, -1.1770, -1.2293],
          [ 1.0365,  1.1585,  1.1411,  ..., -1.2467, -1.2293, -1.2467],
          [ 1.1411,  1.1237,  1.2108,  ..., -1.1770, -1.2816, -1.2641],
          ...,
          [ 2.1520,  2.0823,  2.1694,  ...,  1.4722,  1.4200,  1.3851],
          [ 2.1520,  2.1520,  2.0997,  ...,  1.3851,  1.4374,  1.4374],
          [ 2.0823,  2.0648,  2.1171,  ...,  1.3677,  1.4025,  1.4200]]],


        ...,


        [[[ 1.4612,  1.2043, -0.5938,  ..., -0.1828, -0.1314, -0.1143],
          [ 1.3927,  1.4954,  0.3652,  ..., -0.2684, -0.3027, -0.3027],
          [ 1.3584,  2.0605,  1.9235,  ..., -0.2342, -0.3198, -0.3883],
          ...,
          [-2.0494, -2.0665, -2.0837,  ..., -1.1760, -1.0904, -1.0219],
          [-2.1179, -2.1179, -2.1008,  ..., -0.7650, -0.7993, -0.8507],
          [-2.1179, -2.1179, -2.1008,  ..., -0.6109, -0.6965, -0.7308]],

         [[ 1.5182,  1.2381, -0.5651,  ...,  0.2927,  0.3277,  0.3452],
          [ 1.4307,  1.5532,  0.4153,  ...,  0.1877,  0.1527,  0.1527],
          [ 1.3957,  2.0784,  1.9209,  ...,  0.2227,  0.2052,  0.1001],
          ...,
          [-1.8606, -1.8782, -1.8957,  ..., -0.8277, -0.7227, -0.6527],
          [-1.9657, -1.9482, -1.9132,  ..., -0.4076, -0.4251, -0.4951],
          [-1.9657, -1.9482, -1.9132,  ..., -0.2500, -0.3375, -0.3725]],

         [[ 1.5245,  1.3502, -0.4275,  ...,  0.7228,  0.8099,  0.8274],
          [ 1.4722,  1.6640,  0.5659,  ...,  0.5834,  0.6356,  0.6182],
          [ 1.4025,  2.1520,  2.0474,  ...,  0.6705,  0.6182,  0.5311],
          ...,
          [-1.6650, -1.6824, -1.6999,  ..., -0.5495, -0.4450, -0.3753],
          [-1.7696, -1.7522, -1.7173,  ..., -0.1312, -0.1487, -0.2184],
          [-1.7696, -1.7522, -1.7173,  ...,  0.0256, -0.0615, -0.0964]]],


        [[[ 0.9132,  0.9132,  0.9132,  ...,  1.0673,  1.1187,  0.9646],
          [ 0.9132,  0.9132,  0.8447,  ...,  1.2728,  1.1700,  1.1187],
          [ 0.9646,  0.8961,  0.8789,  ...,  1.3242,  1.2214,  1.0844],
          ...,
          [-1.2445, -1.4329, -1.4500,  ..., -1.7754, -1.5528, -1.1589],
          [-1.0562, -1.6898, -1.6727,  ..., -1.0904, -0.8849, -1.2274],
          [-1.6384, -1.5357, -1.6727,  ..., -0.9020, -0.3712,  0.3309]],

         [[ 0.9405,  0.9755,  0.9055,  ...,  0.7304,  0.8179,  0.6429],
          [ 0.8880,  0.9230,  0.8354,  ...,  0.9405,  0.8704,  0.8529],
          [ 0.9230,  0.8880,  0.8529,  ...,  0.9580,  0.9055,  0.8880],
          ...,
          [-1.2829, -1.4055, -1.3529,  ..., -1.6155, -1.4055, -1.0203],
          [-0.8627, -1.4580, -1.3704,  ..., -1.1779, -0.8978, -1.4055],
          [-1.5805, -1.3529, -1.4405,  ..., -0.8803, -0.4601,  0.2927]],

         [[ 0.7925,  0.7402,  0.8099,  ...,  0.5659,  0.5136,  0.5311],
          [ 0.7576,  0.7576,  0.7576,  ...,  0.8274,  0.7402,  0.7054],
          [ 0.9319,  0.8971,  0.9842,  ...,  0.8274,  0.7054,  0.5311],
          ...,
          [-0.8633, -1.0376, -1.0550,  ..., -1.1770, -1.0027, -0.6715],
          [-0.4450, -1.0027, -1.0550,  ..., -0.7936, -0.6018, -0.8110],
          [-1.1073, -0.9504, -1.1073,  ..., -0.5321, -0.2010,  0.1128]]],


        [[[ 0.3652, -0.5082, -1.1760,  ..., -1.0219, -1.0048, -1.0048],
          [ 0.3481, -0.6452, -1.2274,  ..., -1.0048, -0.9020, -0.9363],
          [ 0.3138, -0.6452, -1.2274,  ..., -0.9534, -0.8678, -1.0048],
          ...,
          [-0.4739, -0.2684, -0.7650,  ..., -0.4568, -0.3198, -0.4054],
          [-0.5082, -0.7822, -0.6623,  ..., -0.3027, -0.4739, -0.3712],
          [-0.7993, -0.7137, -0.5082,  ..., -0.4739, -0.2513,  0.0741]],

         [[ 0.3102, -0.3725, -0.8277,  ..., -1.0378, -0.9853, -0.9853],
          [ 0.3452, -0.4776, -0.7927,  ..., -0.9678, -0.8277, -0.8102],
          [ 0.3277, -0.4776, -0.8102,  ..., -0.8978, -0.7402, -0.8627],
          ...,
          [-0.1450,  0.0826, -0.4251,  ..., -0.0399,  0.0476, -0.0224],
          [-0.2150, -0.4951, -0.3725,  ...,  0.1176, -0.0924,  0.0126],
          [-0.4776, -0.3901, -0.1625,  ..., -0.0224,  0.2052,  0.4853]],

         [[ 0.1825, -0.2532, -0.4101,  ..., -1.1247, -1.0898, -1.0898],
          [ 0.1999, -0.3753, -0.4450,  ..., -1.0376, -0.9504, -0.9504],
          [ 0.1825, -0.3753, -0.4450,  ..., -0.9504, -0.8458, -0.9678],
          ...,
          [-0.4624, -0.2184, -0.7238,  ..., -0.6367, -0.4101, -0.5147],
          [-0.4101, -0.6890, -0.5670,  ..., -0.4450, -0.5321, -0.4275],
          [-0.6541, -0.5844, -0.3927,  ..., -0.6018, -0.4275, -0.1138]]]])
[19:57:10.738241] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:10.757660] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:12.059974] INFO: mask:  tensor([[ True,  True, False,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True, False],
        [ True,  True, False,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:12.270777] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.271042] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.271460] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.271932] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.272398] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.272862] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.273324] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.273794] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.274259] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.274749] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.275196] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.275666] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.276127] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.276594] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.277066] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.277539] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.277997] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.278467] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.278943] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:12.336981] INFO: samples:  tensor([[[[ 1.7352,  1.7180,  1.7009,  ...,  1.2728,  2.2489,  0.8789],
          [ 1.7180,  1.7180,  1.7009,  ..., -0.2856,  1.8550,  1.3927],
          [ 1.7352,  1.7352,  1.7352,  ..., -0.3369,  0.9646,  0.8104],
          ...,
          [ 0.2624,  0.1768,  0.1254,  ...,  0.3823,  0.2111,  0.2453],
          [-0.0629, -0.0116,  0.2111,  ...,  0.2796, -0.0801,  0.2111],
          [ 0.0569,  0.1426,  0.1768,  ..., -0.1314, -0.0287,  0.2111]],

         [[ 1.9559,  1.9734,  1.9559,  ...,  1.4657,  2.4286,  1.5007],
          [ 1.9734,  1.9734,  1.9559,  ..., -0.2500,  1.9209,  1.7458],
          [ 1.9734,  1.9734,  1.9734,  ...,  0.1877,  1.0105,  0.9755],
          ...,
          [ 1.0105,  0.9055,  0.7654,  ...,  1.1506,  1.0105,  0.9055],
          [ 0.7129,  0.7829,  0.9055,  ...,  1.0630,  0.7304,  0.9580],
          [ 0.6954,  0.8004,  0.8179,  ...,  0.6429,  0.7829,  1.0105]],

         [[ 2.2566,  2.2566,  2.2391,  ...,  1.4548,  2.6400,  1.7685],
          [ 2.2391,  2.2391,  2.2217,  ..., -0.3404,  2.0997,  1.8731],
          [ 2.2391,  2.2391,  2.2391,  ...,  0.1651,  0.8797,  0.5659],
          ...,
          [-0.2010, -0.3927, -0.4973,  ..., -0.5670, -0.6193, -0.5844],
          [-0.5147, -0.5844, -0.4624,  ..., -0.4450, -0.8110, -0.4798],
          [-0.6541, -0.6367, -0.6890,  ..., -0.8110, -0.8284, -0.4624]]],


        [[[ 1.3584,  1.0331,  1.5125,  ..., -1.2103, -0.7993, -1.7754],
          [ 0.9988,  1.2557,  1.1187,  ..., -1.4500, -0.9020, -1.5699],
          [ 1.0159,  0.8447,  0.8276,  ..., -1.3987, -1.1418, -1.7754],
          ...,
          [ 0.5022,  0.4851,  0.4679,  ...,  0.4166,  0.2796,  0.4166],
          [ 0.6221,  0.5878,  0.6221,  ...,  0.4679,  0.3309,  0.4337],
          [ 0.3481,  0.3309,  0.4337,  ...,  0.4166,  0.3823,  0.1768]],

         [[ 1.0980,  0.6779,  1.2381,  ..., -1.3354, -1.2479, -1.8782],
          [ 0.8004,  1.0105,  0.9230,  ..., -1.5280, -1.4405, -1.8431],
          [ 0.8880,  0.6078,  0.4503,  ..., -1.5630, -1.3179, -1.8431],
          ...,
          [ 0.3102,  0.2227,  0.1001,  ...,  0.2402,  0.3627,  0.2227],
          [ 0.1001,  0.2052,  0.1702,  ...,  0.1352,  0.3277,  0.2402],
          [ 0.1702,  0.1352,  0.2577,  ...,  0.4153,  0.2052, -0.2850]],

         [[ 0.2348, -0.0441,  1.1237,  ..., -1.4210, -1.2119, -1.7347],
          [ 0.1128,  0.5136,  0.6879,  ..., -1.3687, -1.1770, -1.5430],
          [ 0.6356,  0.6008,  0.0605,  ..., -1.3861, -1.3513, -1.7870],
          ...,
          [-0.6367, -0.5844, -0.8110,  ..., -0.7238, -0.8284, -0.6018],
          [-0.1138, -0.8458, -0.6890,  ..., -0.4450, -0.4101, -0.7238],
          [-0.8458, -0.7238, -0.4798,  ..., -0.3404, -0.2532, -0.6890]]],


        [[[-0.9534, -0.9363, -0.9363,  ...,  0.7933,  0.7762,  0.8104],
          [-0.9534, -0.9020, -0.9020,  ...,  0.8104,  0.7933,  0.7933],
          [-0.9534, -0.9020, -0.9020,  ...,  0.8104,  0.7933,  0.7933],
          ...,
          [ 1.2728,  1.2214,  1.2214,  ..., -0.7993, -0.7822, -0.7822],
          [ 1.2728,  1.2043,  1.2214,  ..., -0.7993, -0.7822, -0.7822],
          [ 1.2728,  1.1872,  1.2043,  ..., -0.8164, -0.7993, -0.7822]],

         [[-0.8452, -0.8277, -0.8277,  ...,  0.3627,  0.3452,  0.3452],
          [-0.8452, -0.7927, -0.7927,  ...,  0.4153,  0.3978,  0.3978],
          [-0.8452, -0.7752, -0.7752,  ...,  0.4503,  0.4503,  0.4328],
          ...,
          [ 1.0280,  0.9580,  0.9580,  ..., -0.7577, -0.7402, -0.7402],
          [ 1.0455,  0.9580,  0.9580,  ..., -0.7577, -0.7402, -0.7402],
          [ 1.0630,  0.9405,  0.9580,  ..., -0.7752, -0.7577, -0.7402]],

         [[-0.5844, -0.5670, -0.5670,  ..., -1.4036, -1.4384, -1.4733],
          [-0.5844, -0.5321, -0.5321,  ..., -1.4907, -1.5779, -1.6302],
          [-0.6018, -0.5147, -0.5321,  ..., -1.5604, -1.6302, -1.6999],
          ...,
          [-0.5670, -0.6715, -0.7238,  ..., -0.1312, -0.1138, -0.1138],
          [-0.5670, -0.6715, -0.6890,  ..., -0.1312, -0.1138, -0.1138],
          [-0.5495, -0.6541, -0.6715,  ..., -0.1487, -0.1312, -0.1138]]],


        ...,


        [[[-0.6623, -0.6452, -0.6281,  ...,  0.9646,  0.8961,  0.8789],
          [-0.5424, -0.4911, -0.4911,  ...,  0.8961,  0.8961,  0.9132],
          [-0.4397, -0.4054, -0.4226,  ...,  0.8961,  0.9303,  0.9303],
          ...,
          [ 2.1462,  2.1633,  2.0777,  ...,  2.1804,  2.1633,  2.1804],
          [ 2.1804,  2.1462,  2.0948,  ...,  2.1633,  2.1804,  2.1804],
          [ 2.1633,  2.0948,  2.1462,  ...,  2.1119,  2.1290,  2.1119]],

         [[-0.8102, -0.8102, -0.8102,  ...,  1.1681,  1.1506,  1.1331],
          [-0.7227, -0.6702, -0.6527,  ...,  1.0980,  1.1155,  1.1681],
          [-0.6527, -0.6001, -0.5826,  ...,  1.0980,  1.1331,  1.1506],
          ...,
          [ 2.3235,  2.3936,  2.3235,  ...,  2.3585,  2.3410,  2.3585],
          [ 2.3585,  2.3585,  2.3235,  ...,  2.3410,  2.3585,  2.3585],
          [ 2.3585,  2.2885,  2.3410,  ...,  2.3060,  2.3235,  2.3060]],

         [[-0.9156, -0.8633, -0.8458,  ...,  1.3677,  1.3328,  1.3154],
          [-0.8110, -0.7761, -0.7413,  ...,  1.2980,  1.3154,  1.3502],
          [-0.7587, -0.6890, -0.6890,  ...,  1.2980,  1.3328,  1.3502],
          ...,
          [ 2.4483,  2.5006,  2.3960,  ...,  2.5877,  2.5354,  2.5529],
          [ 2.4831,  2.4657,  2.4134,  ...,  2.5180,  2.5354,  2.5354],
          [ 2.5180,  2.3960,  2.4483,  ...,  2.4134,  2.4483,  2.4308]]],


        [[[ 2.0605,  1.9749,  2.0605,  ...,  0.2796,  0.5536,  0.6049],
          [ 1.9407,  2.0263,  1.9920,  ...,  1.1700,  0.9132,  0.6049],
          [ 1.9920,  1.9749,  2.1119,  ...,  0.9817,  1.2385,  0.9817],
          ...,
          [ 1.7523,  1.6667,  1.5468,  ...,  1.5125,  1.4098,  0.9303],
          [ 0.7591,  1.1700,  2.0263,  ...,  0.5364,  0.7933,  0.5022],
          [ 1.3584,  0.6734,  1.7865,  ...,  0.2453,  0.7248,  0.1597]],

         [[ 1.8333,  1.7633,  1.8508,  ..., -0.1275,  0.2052,  0.2052],
          [ 1.7108,  1.8508,  1.8683,  ...,  0.7479,  0.5378,  0.1877],
          [ 1.7633,  1.8158,  2.0434,  ...,  0.6604,  0.9405,  0.5903],
          ...,
          [ 1.6583,  1.6232,  1.4132,  ...,  1.3782,  1.5707,  1.2031],
          [ 0.5903,  1.1856,  2.0259,  ...,  0.6429,  1.2731,  0.9230],
          [ 1.3957,  0.5553,  1.9034,  ...,  0.7479,  1.1856,  0.6779]],

         [[ 1.2631,  1.1585,  1.2631,  ..., -0.4275, -0.1312, -0.0615],
          [ 1.1062,  1.2805,  1.3328,  ...,  0.1825,  0.0779,  0.0256],
          [ 1.1934,  1.3154,  1.6640,  ...,  0.2522,  0.4788,  0.2348],
          ...,
          [ 0.6531,  0.6705,  0.7576,  ...,  0.5136,  0.6008,  0.0082],
          [-0.0964,  0.4439,  1.2980,  ..., -0.6018, -0.0267, -0.3578],
          [ 0.6531, -0.1138,  0.9668,  ..., -0.8110, -0.1312, -0.6541]]],


        [[[ 0.2282, -0.1314, -0.4054,  ..., -0.2171, -0.0801, -0.0116],
          [ 0.2111, -0.1143, -0.4054,  ..., -0.1999, -0.0801, -0.0458],
          [ 0.1939, -0.1143, -0.3712,  ..., -0.2342, -0.0801, -0.0287],
          ...,
          [-0.4054, -0.1143,  0.1939,  ..., -0.4054,  0.0912,  0.3823],
          [-0.4226, -0.0629,  0.3138,  ..., -0.3541,  0.0741,  0.2967],
          [-0.3712, -0.3541, -0.2513,  ..., -0.3369,  0.0912,  0.1426]],

         [[ 0.2927, -0.1275, -0.3725,  ..., -0.3375, -0.1975, -0.1450],
          [ 0.2577, -0.1275, -0.3200,  ..., -0.3550, -0.1975, -0.1625],
          [ 0.2402, -0.1275, -0.2850,  ..., -0.4076, -0.1800, -0.1450],
          ...,
          [-0.7402, -0.3901,  0.0301,  ..., -0.0399,  0.4328,  0.6429],
          [-0.7577, -0.3200,  0.1527,  ...,  0.0476,  0.4153,  0.5378],
          [-0.7052, -0.6176, -0.3901,  ...,  0.0301,  0.4153,  0.3627]],

         [[ 0.4091, -0.0092, -0.2881,  ..., -0.3055, -0.1487, -0.0615],
          [ 0.4265, -0.0092, -0.2881,  ..., -0.3055, -0.1312, -0.0964],
          [ 0.4091, -0.0092, -0.2532,  ..., -0.3404, -0.1138, -0.0790],
          ...,
          [-0.8807, -0.4973, -0.0790,  ...,  0.1476,  0.6182,  0.8448],
          [-0.8807, -0.4275,  0.0953,  ...,  0.2173,  0.5834,  0.7228],
          [-0.8458, -0.7064, -0.3927,  ...,  0.1825,  0.5834,  0.5485]]]])
[19:57:12.343123] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:12.359941] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:13.665113] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True, False,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True, False, False],
        [ True,  True,  True,  ...,  True, False,  True]], device='cuda:0')
[19:57:13.875973] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.876243] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.876666] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.877133] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.877599] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.878070] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.878532] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.879010] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.879476] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.879942] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.880406] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.880878] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.881340] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.881807] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.882273] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.882766] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.883211] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.883697] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.884187] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:13.946003] INFO: samples:  tensor([[[[-2.0323, -2.0323, -2.0837,  ..., -1.1760, -1.2103, -1.1932],
          [-2.1008, -2.0665, -2.0494,  ..., -1.1932, -1.1760, -1.2103],
          [-2.0665, -2.1008, -2.1179,  ..., -1.1075, -1.1075, -1.0733],
          ...,
          [ 0.2111,  0.0398,  0.6563,  ...,  0.3994,  0.3823,  0.3138],
          [ 0.1254,  0.2453,  0.7591,  ...,  0.5022,  0.4508,  0.3309],
          [ 0.1426,  0.4508,  0.6563,  ...,  0.2967,  0.3994,  0.3138]],

         [[-0.4076, -0.3025, -0.1800,  ..., -0.7227, -0.7752, -0.7227],
          [-0.3550, -0.3550, -0.2150,  ..., -0.7752, -0.7052, -0.7227],
          [-0.3725, -0.3200, -0.0924,  ..., -0.6702, -0.6877, -0.6001],
          ...,
          [ 0.4853,  0.3102,  0.9230,  ...,  0.8704,  0.8704,  0.8529],
          [ 0.3803,  0.5203,  1.0455,  ...,  0.9755,  0.9055,  0.8529],
          [ 0.3803,  0.7304,  0.9580,  ...,  0.7654,  0.8529,  0.8004]],

         [[ 0.0605,  0.1651,  0.2871,  ..., -0.4798, -0.4973, -0.4973],
          [ 0.0779,  0.1651,  0.3393,  ..., -0.5321, -0.4624, -0.4973],
          [ 0.0779,  0.0953,  0.3393,  ..., -0.4624, -0.4450, -0.4101],
          ...,
          [ 0.7228,  0.5834,  1.2108,  ...,  1.0714,  1.0888,  1.0191],
          [ 0.6182,  0.7925,  1.3502,  ...,  1.1759,  1.1062,  1.0539],
          [ 0.6531,  1.0017,  1.2108,  ...,  0.9494,  1.0539,  1.0365]]],


        [[[ 1.8550,  1.8893,  1.8893,  ...,  1.7523,  1.9064,  1.9920],
          [ 1.7694,  1.8722,  1.8722,  ...,  1.7694,  1.8893,  1.9749],
          [ 1.7180,  1.8379,  1.8550,  ...,  1.8893,  1.9235,  1.9407],
          ...,
          [ 1.9235,  1.9064,  1.5297,  ...,  1.8722,  1.5468,  1.5810],
          [ 1.8550,  1.6324,  1.1529,  ...,  1.7523,  1.4954,  1.6667],
          [ 1.8893,  1.5639,  0.8447,  ...,  1.5468,  1.2728,  1.7523]],

         [[ 1.6232,  1.6583,  1.6583,  ...,  1.4482,  1.6057,  1.6933],
          [ 1.5357,  1.6408,  1.6408,  ...,  1.4657,  1.5882,  1.6933],
          [ 1.4832,  1.6057,  1.6232,  ...,  1.6057,  1.6232,  1.6933],
          ...,
          [ 1.6057,  1.5882,  1.2206,  ...,  1.6057,  1.2731,  1.3081],
          [ 1.5357,  1.3081,  0.8529,  ...,  1.4832,  1.2031,  1.3782],
          [ 1.5707,  1.2381,  0.5378,  ...,  1.2381,  0.9580,  1.4482]],

         [[ 1.5245,  1.5594,  1.5594,  ...,  1.3328,  1.4897,  1.5768],
          [ 1.4374,  1.5420,  1.5420,  ...,  1.3502,  1.4722,  1.5594],
          [ 1.3851,  1.5071,  1.5245,  ...,  1.4722,  1.5071,  1.5420],
          ...,
          [ 1.4374,  1.4200,  1.0539,  ...,  1.4722,  1.1411,  1.1759],
          [ 1.3502,  1.1411,  0.6705,  ...,  1.3328,  1.0714,  1.2457],
          [ 1.3677,  1.0714,  0.3568,  ...,  1.0888,  0.8099,  1.2980]]],


        [[[ 1.8893,  1.8893,  1.9235,  ...,  0.6049,  0.8447,  0.8104],
          [ 1.5639,  1.8722,  1.9578,  ..., -0.0801,  0.6392,  0.7762],
          [ 1.5297,  1.5982,  1.7352,  ..., -0.2513,  0.6049,  0.8789],
          ...,
          [ 1.4954,  1.5810,  1.9578,  ...,  0.3994,  0.6049,  0.9646],
          [ 1.8722,  1.0331,  1.5982,  ..., -0.3541, -0.0801,  0.4679],
          [ 1.3413,  0.9817,  1.4612,  ..., -0.1143, -0.6109,  0.2111]],

         [[ 2.0084,  2.0084,  2.0434,  ...,  0.6254,  0.8704,  0.8354],
          [ 1.6758,  1.9909,  2.0784,  ..., -0.0749,  0.6604,  0.8004],
          [ 1.6758,  1.7283,  1.8683,  ..., -0.2500,  0.6078,  0.9055],
          ...,
          [ 1.6758,  1.7458,  2.1310,  ...,  0.5203,  0.7304,  1.0805],
          [ 2.0609,  1.2206,  1.8158,  ..., -0.2500,  0.0476,  0.5728],
          [ 1.5182,  1.1506,  1.6758,  ..., -0.0049, -0.5126,  0.3277]],

         [[ 1.9603,  1.9603,  1.9951,  ...,  0.5659,  0.8099,  0.7751],
          [ 1.6117,  1.9428,  2.0300,  ..., -0.1312,  0.6008,  0.7402],
          [ 1.5594,  1.6465,  1.8383,  ..., -0.2707,  0.5485,  0.8448],
          ...,
          [ 1.6465,  1.7685,  2.2391,  ...,  0.7054,  0.9145,  1.3328],
          [ 2.0474,  1.2108,  1.8905,  ..., -0.0790,  0.2348,  0.8099],
          [ 1.5245,  1.2108,  1.7511,  ...,  0.1651, -0.3055,  0.5136]]],


        ...,


        [[[-0.0287, -0.0458, -0.0287,  ..., -0.2171, -0.1143,  0.0056],
          [-0.0116, -0.0116,  0.0227,  ..., -0.2342, -0.0972,  0.0056],
          [ 0.0056,  0.0227,  0.0398,  ..., -0.1999, -0.0629,  0.0056],
          ...,
          [-0.1486, -0.1143, -0.1999,  ...,  0.0912,  0.0569,  0.2453],
          [-0.0972, -0.0458, -0.0972,  ...,  0.4337,  0.3138,  0.3652],
          [-0.1314, -0.0287, -0.0972,  ...,  0.7077,  0.5536,  0.5707]],

         [[-0.0749, -0.1099, -0.1099,  ..., -0.3200, -0.1800, -0.0574],
          [-0.0924, -0.1099, -0.1099,  ..., -0.3200, -0.1800, -0.0574],
          [-0.0924, -0.1099, -0.0924,  ..., -0.3025, -0.1625, -0.0749],
          ...,
          [-0.2325, -0.2325, -0.3025,  ..., -0.0399, -0.0574,  0.1176],
          [-0.1099, -0.1450, -0.2325,  ...,  0.2927,  0.2052,  0.2227],
          [-0.1800, -0.1275, -0.2150,  ...,  0.5728,  0.4503,  0.4503]],

         [[-0.3055, -0.3753, -0.3578,  ..., -0.4450, -0.3753, -0.2358],
          [-0.2707, -0.3404, -0.3055,  ..., -0.4450, -0.2881, -0.1835],
          [-0.2184, -0.2707, -0.2707,  ..., -0.4101, -0.2358, -0.1487],
          ...,
          [-0.1138, -0.0790, -0.2184,  ..., -0.2707, -0.2881, -0.0615],
          [-0.1661, -0.1138, -0.0790,  ...,  0.0605, -0.0092,  0.1128],
          [-0.1661, -0.1138, -0.1661,  ...,  0.3393,  0.2348,  0.3219]]],


        [[[-1.3302, -1.3473, -1.3815,  ..., -0.6109, -0.7308, -0.9020],
          [-1.3473, -1.3302, -1.3473,  ..., -1.0390, -0.9534, -1.0048],
          [-1.3130, -1.2959, -1.3302,  ..., -1.1247, -1.2445, -1.3644],
          ...,
          [-0.2342, -0.3369, -0.3027,  ..., -1.2103, -1.2274, -1.2445],
          [-0.5253, -0.6452, -0.5253,  ..., -0.9192, -0.8849, -0.9020],
          [-0.7993, -0.8507, -0.5596,  ..., -0.6452, -0.4739, -0.5253]],

         [[-1.7556, -1.7556, -1.7381,  ..., -0.7577, -0.7402, -0.8102],
          [-1.7731, -1.7206, -1.7206,  ..., -1.2654, -1.0553, -0.9853],
          [-1.7381, -1.7031, -1.7206,  ..., -1.4405, -1.4755, -1.4580],
          ...,
          [-1.0028, -1.1253, -0.8803,  ..., -1.6331, -1.6506, -1.6506],
          [-1.1253, -1.2479, -0.9678,  ..., -1.4405, -1.4055, -1.4055],
          [-1.3004, -1.3354, -0.9328,  ..., -1.2304, -1.0378, -1.0903]],

         [[-1.5953, -1.5953, -1.5953,  ..., -0.8110, -0.7936, -0.8458],
          [-1.5953, -1.5604, -1.5604,  ..., -1.3164, -1.0898, -1.0376],
          [-1.5604, -1.5430, -1.5604,  ..., -1.5081, -1.5081, -1.5081],
          ...,
          [-1.1770, -1.1421, -0.7936,  ..., -1.4210, -1.3861, -1.3687],
          [-1.2816, -1.2293, -0.8110,  ..., -1.2816, -1.1944, -1.1596],
          [-1.4036, -1.2990, -0.7413,  ..., -1.1247, -0.8981, -0.9330]]],


        [[[-0.7137, -0.7137, -0.7137,  ..., -1.0562, -1.0733, -1.0904],
          [-0.6965, -0.6965, -0.6965,  ..., -1.0562, -1.0733, -1.0904],
          [-0.6794, -0.6794, -0.6794,  ..., -1.0562, -1.0733, -1.0904],
          ...,
          [-1.5870, -1.5870, -1.5870,  ..., -0.4568, -0.4397, -0.4054],
          [-1.5870, -1.5870, -1.5870,  ..., -0.4911, -0.4739, -0.4568],
          [-1.5870, -1.5870, -1.5870,  ..., -0.4911, -0.4739, -0.4568]],

         [[-0.2850, -0.2850, -0.2850,  ..., -0.7227, -0.7402, -0.7577],
          [-0.2675, -0.2675, -0.2675,  ..., -0.7227, -0.7402, -0.7577],
          [-0.2500, -0.2500, -0.2500,  ..., -0.7227, -0.7402, -0.7577],
          ...,
          [-1.5805, -1.5805, -1.5805,  ..., -0.0224, -0.0049, -0.0049],
          [-1.5805, -1.5805, -1.5805,  ..., -0.0399, -0.0574, -0.0574],
          [-1.5805, -1.5805, -1.5805,  ..., -0.0574, -0.0574, -0.0574]],

         [[ 0.1825,  0.1825,  0.1825,  ..., -0.2184, -0.2358, -0.2532],
          [ 0.1999,  0.1999,  0.1999,  ..., -0.2184, -0.2358, -0.2532],
          [ 0.2173,  0.2173,  0.2173,  ..., -0.2184, -0.2358, -0.2532],
          ...,
          [-1.2816, -1.2816, -1.2816,  ...,  0.4788,  0.4788,  0.4788],
          [-1.2816, -1.2816, -1.2816,  ...,  0.4614,  0.4614,  0.4614],
          [-1.2816, -1.2816, -1.2816,  ...,  0.4439,  0.4439,  0.4439]]]])
[19:57:13.952594] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:13.970447] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:15.274972] INFO: mask:  tensor([[ True,  True, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [False,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:15.486272] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.486543] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.486966] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.487426] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.487895] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.488359] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.488824] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.489288] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.489761] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.490224] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.490688] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.491163] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.491630] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.492100] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.492570] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.493039] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.493499] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.493966] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.494436] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:15.553981] INFO: samples:  tensor([[[[ 2.2489,  2.2489,  2.2489,  ..., -0.1143, -0.1314, -0.1143],
          [ 2.2489,  2.2489,  2.2489,  ..., -0.1143, -0.1314, -0.1143],
          [ 2.2489,  2.2489,  2.2489,  ..., -0.0972, -0.1143, -0.0972],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ..., -1.0390, -1.0390, -1.0904],
          [ 2.2489,  2.2489,  2.2489,  ..., -1.1075, -1.1075, -1.1418],
          [ 2.2489,  2.2489,  2.2489,  ..., -1.1589, -1.1589, -1.1589]],

         [[ 2.4286,  2.4286,  2.4286,  ...,  0.0476,  0.0301,  0.0301],
          [ 2.4286,  2.4286,  2.4286,  ...,  0.0651,  0.0476,  0.0301],
          [ 2.4286,  2.4286,  2.4286,  ...,  0.0651,  0.0476,  0.0476],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ..., -0.5826, -0.6001, -0.6352],
          [ 2.4286,  2.4286,  2.4286,  ..., -0.6527, -0.6702, -0.6877],
          [ 2.4286,  2.4286,  2.4286,  ..., -0.7052, -0.7227, -0.7052]],

         [[ 2.6400,  2.6400,  2.6400,  ..., -0.3753, -0.3753, -0.3404],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.3753, -0.3753, -0.3404],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.3578, -0.3578, -0.3230],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ..., -0.0964, -0.1487, -0.2184],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.1661, -0.2184, -0.2707],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.2184, -0.2532, -0.2881]]],


        [[[-0.7993, -1.3987, -1.2445,  ..., -1.6042, -1.0048, -0.5938],
          [-1.0562, -1.2445, -1.1589,  ..., -1.4500, -1.3302, -1.2788],
          [-1.1247, -1.4500, -1.4158,  ..., -1.6384, -1.4843, -1.3473],
          ...,
          [-1.8953, -1.9638, -1.8953,  ..., -0.4397, -0.1657, -0.3198],
          [-1.9124, -1.9124, -1.9467,  ...,  0.1768,  0.0912, -0.1486],
          [-1.9638, -1.9295, -1.9295,  ...,  0.0741, -0.2513, -0.3541]],

         [[-0.2500, -0.7227, -0.6702,  ..., -1.4055, -0.8277, -0.3901],
          [-0.6702, -0.5301, -0.5476,  ..., -1.1429, -1.1604, -1.0203],
          [-0.9328, -0.6702, -0.5826,  ..., -1.2654, -1.1078, -1.1078],
          ...,
          [-1.9657, -2.0182, -1.9482,  ..., -0.9678, -0.7752, -0.8452],
          [-1.9657, -1.9832, -1.9657,  ..., -0.3375, -0.4426, -0.6877],
          [-1.9657, -1.9832, -1.9657,  ..., -0.4776, -0.7752, -0.9153]],

         [[-0.2010, -1.1247, -1.0724,  ..., -1.4907, -1.1247, -0.8284],
          [-1.0376, -1.0898, -1.1421,  ..., -1.3687, -1.4384, -1.5430],
          [-1.2293, -1.4036, -1.4036,  ..., -1.6302, -1.5604, -1.5081],
          ...,
          [-1.7347, -1.8044, -1.7522,  ..., -0.9504, -0.7064, -0.7936],
          [-1.7347, -1.7870, -1.7870,  ..., -0.3404, -0.4101, -0.6193],
          [-1.7870, -1.8044, -1.7870,  ..., -0.4450, -0.7413, -0.8284]]],


        [[[-0.6623, -0.6281, -0.5938,  ..., -2.0494, -2.0494, -2.0494],
          [-0.6452, -0.6109, -0.5767,  ..., -2.0665, -2.0665, -2.0665],
          [-0.5767, -0.5424, -0.5424,  ..., -2.0837, -2.0837, -2.0837],
          ...,
          [ 2.1119,  2.1119,  2.1119,  ..., -2.1179, -2.1179, -2.1179],
          [ 2.1290,  2.1290,  2.1290,  ..., -2.1179, -2.1179, -2.1179],
          [ 2.1290,  2.1290,  2.1290,  ..., -2.1179, -2.1179, -2.1179]],

         [[-2.0357, -1.9657, -1.8957,  ..., -1.9307, -1.9307, -1.9307],
          [-2.0357, -1.9657, -1.8957,  ..., -1.9482, -1.9482, -1.9482],
          [-2.0357, -1.9482, -1.8782,  ..., -1.9657, -1.9832, -2.0007],
          ...,
          [-1.5630, -1.5630, -1.5455,  ..., -2.0357, -2.0357, -2.0357],
          [-1.5455, -1.5455, -1.5280,  ..., -2.0357, -2.0357, -2.0357],
          [-1.5455, -1.5455, -1.5280,  ..., -2.0357, -2.0357, -2.0357]],

         [[-1.8044, -1.7696, -1.7347,  ..., -1.7173, -1.7173, -1.7173],
          [-1.8044, -1.7696, -1.7173,  ..., -1.7347, -1.7347, -1.7347],
          [-1.8044, -1.7522, -1.6999,  ..., -1.7522, -1.7522, -1.7696],
          ...,
          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],
          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],
          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],


        ...,


        [[[ 2.0434,  2.0434,  2.0263,  ...,  2.1119,  2.0948,  2.0777],
          [ 2.0263,  2.0605,  2.0777,  ...,  2.1290,  2.0948,  2.0777],
          [ 2.0434,  2.0777,  2.0777,  ...,  2.1290,  2.0777,  2.0605],
          ...,
          [ 1.9064,  1.8722,  1.8893,  ...,  1.4954,  1.4954,  1.4440],
          [ 1.8893,  1.8550,  1.8550,  ...,  1.4954,  1.5297,  1.4954],
          [ 1.8893,  1.9235,  1.9407,  ...,  1.4783,  1.5125,  1.5125]],

         [[ 2.0784,  2.0784,  2.0609,  ...,  2.0959,  2.0784,  2.0609],
          [ 2.0609,  2.0959,  2.1134,  ...,  2.1134,  2.0784,  2.0609],
          [ 2.0784,  2.1134,  2.1134,  ...,  2.1134,  2.0609,  2.0434],
          ...,
          [ 1.9209,  1.9209,  1.9384,  ...,  1.6057,  1.5882,  1.5357],
          [ 1.9384,  1.9034,  1.9034,  ...,  1.6057,  1.5882,  1.5532],
          [ 1.9384,  1.9734,  1.9909,  ...,  1.5707,  1.5707,  1.5707]],

         [[ 2.5180,  2.5180,  2.5006,  ...,  2.5529,  2.5354,  2.5180],
          [ 2.5006,  2.5354,  2.5529,  ...,  2.5703,  2.5354,  2.5180],
          [ 2.5180,  2.5529,  2.5529,  ...,  2.5703,  2.5180,  2.5006],
          ...,
          [ 2.4308,  2.4134,  2.4308,  ...,  2.0474,  2.0474,  1.9951],
          [ 2.4657,  2.4134,  2.3960,  ...,  2.0474,  2.0474,  2.0125],
          [ 2.4657,  2.4831,  2.4831,  ...,  2.0125,  2.0300,  2.0300]]],


        [[[ 0.9817,  1.0673,  1.1358,  ..., -0.3883, -0.4739, -0.4911],
          [ 1.0331,  1.1187,  1.2214,  ..., -0.4397, -0.4568, -0.4568],
          [ 1.1187,  1.1700,  1.2214,  ..., -0.4226, -0.3541, -0.3198],
          ...,
          [ 0.9303,  0.9303,  0.9988,  ..., -1.1589, -1.2617, -0.6109],
          [ 0.9988,  1.0502,  1.1015,  ..., -0.3027, -1.3130, -0.9877],
          [ 1.0331,  1.0844,  1.1358,  ...,  1.0502, -0.6623, -1.3473]],

         [[-0.4601, -0.4076, -0.2850,  ..., -1.4055, -1.3880, -1.4230],
          [-0.4251, -0.3550, -0.1975,  ..., -1.4405, -1.4405, -1.4405],
          [-0.3901, -0.3200, -0.1975,  ..., -1.4055, -1.4055, -1.4230],
          ...,
          [-0.8277, -0.8102, -0.7752,  ..., -2.0357, -1.9482, -1.7031],
          [-0.7402, -0.7052, -0.6702,  ..., -1.4230, -2.0357, -1.8957],
          [-0.7052, -0.6877, -0.6527,  ..., -0.3901, -1.6681, -2.0357]],

         [[-1.5081, -1.4036, -1.3339,  ..., -1.7696, -1.7522, -1.7870],
          [-1.4733, -1.3513, -1.2641,  ..., -1.7870, -1.7870, -1.7870],
          [-1.4210, -1.3339, -1.2293,  ..., -1.8044, -1.7870, -1.7696],
          ...,
          [-1.8044, -1.8044, -1.7696,  ..., -1.8044, -1.7870, -1.6824],
          [-1.7696, -1.7696, -1.7696,  ..., -1.5081, -1.7696, -1.8044],
          [-1.7522, -1.7870, -1.7870,  ..., -1.0724, -1.5604, -1.8044]]],


        [[[ 1.6667,  1.5125,  1.5468,  ...,  1.5982,  1.9235,  1.5468],
          [ 1.2728,  1.3927,  1.5125,  ...,  1.7865,  1.7865,  2.0605],
          [ 1.4098,  1.8379,  1.5125,  ...,  1.7180,  1.9920,  1.8379],
          ...,
          [-2.1179, -1.9638, -1.1932,  ...,  0.0398,  0.0569,  0.0569],
          [-1.6213, -1.8268, -1.3302,  ...,  0.0227,  0.0569,  0.0227],
          [-1.6555, -1.8439, -1.3815,  ...,  0.0227,  0.0569,  0.0056]],

         [[ 1.5707,  1.3782,  1.4832,  ...,  1.5532,  1.8859,  1.5007],
          [ 1.1506,  1.2556,  1.3957,  ...,  1.7283,  1.7458,  2.0259],
          [ 1.2556,  1.7108,  1.4132,  ...,  1.6583,  1.9559,  1.7983],
          ...,
          [-2.0357, -1.6856, -1.2829,  ..., -0.7052, -0.6877, -0.6877],
          [-1.5980, -1.7381, -1.4580,  ..., -0.7227, -0.6877, -0.7227],
          [-1.7206, -1.7556, -1.4755,  ..., -0.6877, -0.6877, -0.7402]],

         [[ 0.9842,  0.9319,  0.8274,  ...,  0.9668,  1.2631,  0.8797],
          [ 0.4962,  0.7228,  0.8274,  ...,  1.1411,  1.1237,  1.4025],
          [ 0.7228,  1.0888,  0.8622,  ...,  1.0714,  1.3328,  1.1759],
          ...,
          [-1.5779, -1.3339, -1.3164,  ..., -0.8981, -0.8981, -0.8981],
          [-1.4559, -1.3164, -1.4036,  ..., -0.8981, -0.8807, -0.9504],
          [-1.3861, -1.4036, -1.4210,  ..., -0.9156, -0.8981, -0.9504]]]])
[19:57:15.560917] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:15.578331] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:16.884890] INFO: mask:  tensor([[ True,  True,  True,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True, False,  True]], device='cuda:0')
[19:57:17.095621] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.095892] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.096324] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.096790] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.097257] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.097726] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.098192] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.098656] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.099129] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.099596] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.100060] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.100532] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.100996] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.101462] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.101929] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.102395] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.102864] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.103330] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.103802] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:17.161333] INFO: samples:  tensor([[[[-0.7308, -0.6965, -0.6623,  ..., -0.4739, -0.4397, -0.4397],
          [-0.6623, -0.6965, -0.6965,  ..., -0.4397, -0.4568, -0.4226],
          [-0.6623, -0.6794, -0.6794,  ..., -0.4397, -0.4568, -0.4397],
          ...,
          [-0.2342, -1.1932, -0.8678,  ...,  0.5878,  0.3652, -0.9534],
          [-0.6281, -0.8335, -0.7993,  ...,  0.2111,  0.6221,  1.2043],
          [-1.2959, -1.3302, -1.5014,  ..., -1.0219, -1.4672, -0.8849]],

         [[-0.5826, -0.5476, -0.5126,  ..., -0.2850, -0.2850, -0.2850],
          [-0.4951, -0.5301, -0.5301,  ..., -0.2850, -0.2850, -0.2500],
          [-0.4951, -0.5126, -0.5301,  ..., -0.2850, -0.2850, -0.2850],
          ...,
          [-0.6001, -1.3880, -1.3179,  ...,  0.1702, -0.2500, -1.1954],
          [-0.8277, -1.1954, -1.0553,  ..., -0.2500, -0.0049,  0.1702],
          [-1.5805, -1.5805, -1.7381,  ..., -1.2654, -1.5280, -1.3529]],

         [[-0.1487, -0.1138, -0.0790,  ...,  0.1302,  0.1476,  0.1476],
          [-0.1487, -0.1487, -0.1487,  ...,  0.1476,  0.0953,  0.1302],
          [-0.1138, -0.1312, -0.0964,  ...,  0.1476,  0.0953,  0.1476],
          ...,
          [-0.7413, -1.3687, -1.3164,  ..., -0.4450, -0.9156, -1.3861],
          [-0.8981, -1.2467, -1.3513,  ..., -0.6541, -0.4450, -0.7413],
          [-1.5953, -1.5430, -1.8044,  ..., -1.4036, -1.4733, -1.7696]]],


        [[[ 0.9132,  0.8789,  0.8104,  ...,  1.1872,  1.2728,  1.3413],
          [ 0.8789,  0.8447,  0.7762,  ...,  1.0159,  1.2043,  1.4440],
          [ 0.8276,  0.8104,  0.7419,  ...,  0.8447,  0.8961,  1.1529],
          ...,
          [-1.5357, -1.5185, -1.5357,  ...,  2.2318,  2.2318,  2.0777],
          [-1.5014, -1.5528, -1.5699,  ...,  2.1975,  2.2489,  2.1119],
          [-1.5014, -1.5357, -1.5870,  ...,  2.1633,  2.2147,  2.1975]],

         [[ 1.6933,  1.6758,  1.6057,  ..., -0.8452, -0.8102, -0.7577],
          [ 1.6758,  1.6408,  1.5882,  ..., -0.9328, -0.8978, -0.7227],
          [ 1.6408,  1.6408,  1.5882,  ..., -0.9678, -1.1253, -0.9853],
          ...,
          [-1.3529, -1.3529, -1.3704,  ...,  0.6779,  0.8704,  1.2206],
          [-1.3880, -1.4405, -1.4580,  ...,  0.7129,  0.8880,  1.0805],
          [-1.3529, -1.4055, -1.4580,  ...,  0.7479,  0.7829,  0.9055]],

         [[ 1.1411,  1.1237,  1.0539,  ...,  0.9668,  1.0714,  1.1411],
          [ 1.1062,  1.0714,  1.0191,  ...,  0.7054,  0.8971,  1.1585],
          [ 1.0191,  1.0017,  0.9494,  ...,  0.3916,  0.4614,  0.7576],
          ...,
          [-1.6999, -1.6999, -1.6999,  ...,  2.4831,  2.4657,  2.4831],
          [-1.6650, -1.7173, -1.7347,  ...,  2.5006,  2.4831,  2.3786],
          [-1.6302, -1.6476, -1.7173,  ...,  2.5006,  2.3786,  2.2566]]],


        [[[ 0.5193,  0.5193,  0.5193,  ...,  0.6221,  0.6221,  0.6221],
          [ 0.5193,  0.5193,  0.5193,  ...,  0.6221,  0.6221,  0.6221],
          [ 0.5193,  0.5193,  0.5193,  ...,  0.6221,  0.6221,  0.6221],
          ...,
          [ 0.2624,  0.2624,  0.2624,  ..., -0.7822, -0.7650, -0.7479],
          [ 0.2624,  0.2624,  0.2624,  ..., -0.7308, -0.6965, -0.6794],
          [ 0.2624,  0.2624,  0.2624,  ..., -0.6965, -0.6794, -0.6623]],

         [[ 0.3978,  0.3978,  0.3978,  ...,  0.5028,  0.5028,  0.5028],
          [ 0.3978,  0.3978,  0.3978,  ...,  0.5028,  0.5028,  0.5028],
          [ 0.3978,  0.3978,  0.3978,  ...,  0.5028,  0.5028,  0.5028],
          ...,
          [ 0.0826,  0.0826,  0.0826,  ..., -0.8803, -0.8627, -0.8627],
          [ 0.0826,  0.0826,  0.0826,  ..., -0.8277, -0.7927, -0.7927],
          [ 0.0826,  0.0826,  0.0826,  ..., -0.7927, -0.7752, -0.7752]],

         [[ 0.5659,  0.5659,  0.5659,  ...,  0.6705,  0.6705,  0.6705],
          [ 0.5659,  0.5659,  0.5659,  ...,  0.6705,  0.6705,  0.6705],
          [ 0.5659,  0.5659,  0.5659,  ...,  0.6705,  0.6705,  0.6705],
          ...,
          [ 0.2348,  0.2348,  0.2348,  ..., -1.0376, -1.0201, -1.0201],
          [ 0.2348,  0.2348,  0.2348,  ..., -0.9678, -0.9504, -0.9504],
          [ 0.2348,  0.2348,  0.2348,  ..., -0.9504, -0.9330, -0.9330]]],


        ...,


        [[[-0.0972,  0.0569,  0.1768,  ..., -1.4329, -1.4500, -1.4672],
          [-0.0629,  0.0741,  0.0741,  ..., -1.4672, -1.5014, -1.5185],
          [-0.0116, -0.0116, -0.1486,  ..., -1.4843, -1.5185, -1.5528],
          ...,
          [ 1.1015,  0.3481,  0.1939,  ..., -1.1247, -1.0562, -1.0048],
          [ 1.1529,  0.6906,  0.2453,  ..., -1.2445, -1.1247, -0.9192],
          [ 0.8789,  0.6734, -0.3883,  ..., -1.2617, -1.2274, -1.0219]],

         [[-0.2850, -0.1450, -0.0924,  ..., -1.7556, -1.7206, -1.7031],
          [-0.2500, -0.1275, -0.1800,  ..., -1.7906, -1.7556, -1.7381],
          [-0.2325, -0.2325, -0.4251,  ..., -1.8081, -1.7556, -1.7206],
          ...,
          [ 1.1155,  0.3102,  0.0651,  ..., -1.2479, -1.2304, -1.1779],
          [ 1.2206,  0.7304,  0.1702,  ..., -1.3004, -1.2479, -1.0378],
          [ 0.9755,  0.7304, -0.4601,  ..., -1.3004, -1.3179, -1.1429]],

         [[-0.0964, -0.0267, -0.0092,  ..., -1.5953, -1.5779, -1.5779],
          [-0.1138, -0.0615, -0.1312,  ..., -1.6127, -1.6127, -1.6650],
          [-0.1312, -0.1661, -0.3927,  ..., -1.5779, -1.6302, -1.6999],
          ...,
          [ 1.0714,  0.2348, -0.0615,  ..., -0.9678, -1.0550, -1.0898],
          [ 1.1411,  0.6356,  0.0605,  ..., -1.0724, -1.0724, -0.9678],
          [ 0.8971,  0.6182, -0.5495,  ..., -1.1073, -1.1596, -1.0724]]],


        [[[ 0.0912,  0.0398, -0.0287,  ..., -0.2513, -0.1314, -0.1143],
          [ 0.2111,  0.1939,  0.1597,  ..., -0.0972, -0.0972, -0.1143],
          [ 0.3652,  0.3309,  0.2453,  ...,  0.0227, -0.0629, -0.0972],
          ...,
          [ 1.4612,  1.4440,  1.4954,  ...,  0.6049,  0.4851,  0.3823],
          [ 1.4954,  1.4612,  1.4783,  ...,  0.6906,  0.6221,  0.5193],
          [ 1.4954,  1.4612,  1.4954,  ...,  0.7077,  0.7248,  0.6906]],

         [[ 0.2227,  0.1702,  0.1001,  ..., -0.1099,  0.0126,  0.0476],
          [ 0.3277,  0.3102,  0.2752,  ...,  0.0301,  0.0301,  0.0126],
          [ 0.4678,  0.4328,  0.3452,  ...,  0.1352,  0.0476,  0.0126],
          ...,
          [ 1.3256,  1.2906,  1.3431,  ...,  0.7129,  0.6078,  0.4853],
          [ 1.3256,  1.2906,  1.3081,  ...,  0.8179,  0.7479,  0.6429],
          [ 1.3256,  1.2906,  1.3256,  ...,  0.8354,  0.8529,  0.8179]],

         [[ 0.4439,  0.3916,  0.3393,  ...,  0.0082,  0.1302,  0.1476],
          [ 0.5659,  0.5485,  0.5311,  ...,  0.1476,  0.1651,  0.1476],
          [ 0.7402,  0.7054,  0.6182,  ...,  0.2696,  0.1825,  0.1651],
          ...,
          [ 1.5942,  1.5768,  1.6291,  ...,  1.1062,  0.9842,  0.8797],
          [ 1.6117,  1.5768,  1.6117,  ...,  1.1585,  1.0888,  0.9842],
          [ 1.6117,  1.5768,  1.6117,  ...,  1.1585,  1.1585,  1.1411]]],


        [[[-1.3644, -1.3815, -1.3815,  ..., -0.9020, -0.9020, -0.9020],
          [-1.3130, -1.2788, -1.3815,  ..., -0.8678, -0.8678, -0.8507],
          [-1.2959, -1.2274, -1.3130,  ..., -0.8507, -0.8678, -0.8507],
          ...,
          [ 1.5639,  1.5468,  1.5468,  ...,  1.4612,  1.4612,  1.4783],
          [ 1.5639,  1.5639,  1.5639,  ...,  1.5125,  1.4954,  1.5125],
          [ 1.5639,  1.5468,  1.5639,  ...,  1.5639,  1.5468,  1.5468]],

         [[-1.2304, -1.2479, -1.2479,  ..., -0.7577, -0.7577, -0.7577],
          [-1.1779, -1.1429, -1.2479,  ..., -0.7227, -0.7227, -0.7052],
          [-1.1779, -1.0903, -1.1779,  ..., -0.7052, -0.7402, -0.7052],
          ...,
          [ 1.7108,  1.6933,  1.6933,  ...,  1.6232,  1.6232,  1.6408],
          [ 1.7108,  1.7108,  1.7108,  ...,  1.6758,  1.6583,  1.6758],
          [ 1.7108,  1.6933,  1.7108,  ...,  1.7283,  1.7108,  1.7108]],

         [[-1.0550, -1.0376, -1.0376,  ..., -0.5844, -0.5495, -0.5495],
          [-1.0027, -0.9156, -1.0376,  ..., -0.5495, -0.4798, -0.4973],
          [-0.9678, -0.8981, -0.9853,  ..., -0.5147, -0.4798, -0.4798],
          ...,
          [ 2.1694,  2.1520,  2.1520,  ...,  2.0474,  2.0474,  2.0648],
          [ 2.1694,  2.1694,  2.1694,  ...,  2.0997,  2.0823,  2.0997],
          [ 2.1694,  2.1520,  2.1694,  ...,  2.1520,  2.1346,  2.1346]]]])
[19:57:17.167505] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:17.184348] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:18.492179] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:18.703440] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.703701] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.704127] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.704592] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.705059] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.705528] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.705991] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.706457] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.706927] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.707391] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.707861] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.708332] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.708795] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.709261] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.709732] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.710200] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.710659] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.711128] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.711594] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:18.767048] INFO: samples:  tensor([[[[-0.1828, -0.1999, -0.1999,  ..., -0.5082, -0.5596, -0.5767],
          [-0.1657, -0.1999, -0.1828,  ..., -0.5767, -0.5253, -0.4911],
          [-0.1828, -0.1828, -0.1828,  ..., -0.5253, -0.4568, -0.4054],
          ...,
          [ 0.2624,  0.1939,  0.1426,  ..., -0.3541, -0.9020, -0.7822],
          [ 0.1768,  0.1939,  0.1939,  ..., -0.5424, -0.6452, -0.5767],
          [ 0.1768,  0.1254,  0.1083,  ..., -0.6452, -0.6623, -0.6109]],

         [[-0.1450, -0.1275, -0.1275,  ..., -0.4426, -0.4601, -0.4776],
          [-0.1275, -0.1275, -0.1099,  ..., -0.4601, -0.4251, -0.3901],
          [-0.1099, -0.1099, -0.1099,  ..., -0.3901, -0.3550, -0.3025],
          ...,
          [ 0.3803,  0.2927,  0.2402,  ..., -0.4251, -0.8277, -0.7227],
          [ 0.3102,  0.3277,  0.3277,  ..., -0.6702, -0.7577, -0.6877],
          [ 0.3277,  0.2927,  0.2402,  ..., -0.8102, -0.7752, -0.7227]],

         [[ 0.0082,  0.0082,  0.0082,  ..., -0.3404, -0.3230, -0.3230],
          [ 0.0256,  0.0082,  0.0256,  ..., -0.3753, -0.2881, -0.2358],
          [ 0.0256,  0.0256,  0.0256,  ..., -0.3230, -0.2184, -0.1487],
          ...,
          [ 0.3916,  0.3219,  0.2696,  ..., -0.3753, -0.9156, -0.8633],
          [ 0.3568,  0.3742,  0.3742,  ..., -0.6715, -0.8110, -0.7587],
          [ 0.4091,  0.3219,  0.2871,  ..., -0.8458, -0.8458, -0.7936]]],


        [[[ 1.5982,  0.6049,  0.0398,  ..., -2.1179, -2.1179, -2.1179],
          [ 2.0605,  0.4851,  1.2557,  ..., -2.1179, -2.1179, -2.1179],
          [ 2.1804,  0.9988,  0.6221,  ..., -2.1179, -2.1179, -2.1179],
          ...,
          [-1.9467, -1.8097, -1.8439,  ...,  1.4440,  0.7248,  1.7352],
          [-2.1008, -1.9809, -1.7583,  ...,  0.8447, -1.0562,  1.1529],
          [-0.4397, -1.7925, -1.3987,  ...,  0.5536, -1.0048,  1.4612]],

         [[ 1.7633,  0.7479,  0.1702,  ..., -2.0357, -2.0357, -2.0357],
          [ 2.2360,  0.6254,  1.4132,  ..., -2.0357, -2.0357, -2.0357],
          [ 2.3585,  1.1506,  0.7654,  ..., -2.0357, -2.0357, -2.0357],
          ...,
          [-1.8606, -1.7206, -1.7556,  ...,  1.6057,  0.8704,  1.9034],
          [-2.0182, -1.8957, -1.6681,  ...,  0.9930, -0.9503,  1.3081],
          [-0.3200, -1.7031, -1.3004,  ...,  0.6954, -0.8978,  1.6232]],

         [[ 1.9777,  0.9668,  0.3916,  ..., -1.8044, -1.8044, -1.8044],
          [ 2.4483,  0.8448,  1.6291,  ..., -1.8044, -1.8044, -1.8044],
          [ 2.5703,  1.3677,  0.9842,  ..., -1.8044, -1.8044, -1.8044],
          ...,
          [-1.6302, -1.4907, -1.5256,  ...,  1.8208,  1.0888,  2.1171],
          [-1.7870, -1.6650, -1.4384,  ...,  1.2108, -0.7238,  1.5245],
          [-0.0964, -1.4733, -1.0724,  ...,  0.9145, -0.6715,  1.8383]]],


        [[[-0.9020, -0.9020, -0.9192,  ..., -0.9363, -0.9363, -0.9534],
          [-1.0390, -0.8335, -1.0562,  ..., -0.9363, -0.9363, -0.9534],
          [-0.9020, -0.7650, -1.1247,  ..., -0.9020, -0.9192, -0.9705],
          ...,
          [-0.2171, -0.1999, -0.2342,  ..., -1.0048, -1.0219, -1.0219],
          [-0.2342, -0.2684, -0.2856,  ..., -0.9877, -0.9877, -1.0390],
          [-0.3369, -0.2856, -0.2342,  ..., -1.0048, -1.0219, -1.0733]],

         [[-1.8081, -1.7906, -1.3880,  ..., -1.1779, -1.1779, -1.1954],
          [-1.8081, -1.7206, -1.6331,  ..., -1.1779, -1.1779, -1.1954],
          [-1.6681, -1.7031, -1.7556,  ..., -1.1604, -1.1779, -1.2304],
          ...,
          [-0.5826, -0.5476, -0.5826,  ..., -1.0553, -1.1078, -1.0903],
          [-0.6352, -0.6176, -0.6001,  ..., -1.0728, -1.0903, -1.0903],
          [-0.6877, -0.6352, -0.5476,  ..., -1.0728, -1.1253, -1.1253]],

         [[-1.6127, -1.6476, -1.2641,  ..., -1.3513, -1.3687, -1.3861],
          [-1.6476, -1.5430, -1.4907,  ..., -1.3687, -1.3687, -1.3861],
          [-1.5081, -1.4907, -1.5953,  ..., -1.3513, -1.3687, -1.4210],
          ...,
          [-0.9504, -0.9330, -0.9678,  ..., -1.1073, -1.1073, -1.0724],
          [-1.0027, -0.9853, -1.0027,  ..., -1.0898, -1.0898, -1.0898],
          [-1.0724, -1.0201, -0.9678,  ..., -1.1073, -1.1247, -1.1073]]],


        ...,


        [[[ 2.0948,  2.0948,  2.0948,  ...,  2.1119,  2.0948,  2.0948],
          [ 2.0948,  2.0948,  2.0948,  ...,  2.1462,  2.1290,  2.0948],
          [ 2.0948,  2.0948,  2.0948,  ...,  2.1119,  2.1462,  2.1290],
          ...,
          [-1.6384, -1.3987, -1.3130,  ...,  1.5297,  1.6495,  1.9064],
          [-1.6898, -1.3130, -1.2445,  ..., -1.6213, -1.1589, -0.8164],
          [-0.3369, -1.6213, -1.4158,  ..., -0.5253, -0.2856,  0.0569]],

         [[ 2.2885,  2.2885,  2.2885,  ...,  2.2535,  2.2360,  2.2360],
          [ 2.2885,  2.2885,  2.2885,  ...,  2.2885,  2.2710,  2.2360],
          [ 2.2885,  2.2885,  2.2885,  ...,  2.2535,  2.2885,  2.2710],
          ...,
          [-1.5280, -1.3880, -1.2829,  ...,  1.6933,  1.8158,  2.0784],
          [-1.6331, -1.3004, -1.1779,  ..., -1.7381, -1.2829, -0.9678],
          [-0.0924, -1.4405, -1.3004,  ..., -0.2675,  0.0126,  0.3627]],

         [[ 2.5703,  2.5703,  2.5703,  ...,  2.5529,  2.5354,  2.5354],
          [ 2.5703,  2.5703,  2.5703,  ...,  2.5877,  2.5703,  2.5354],
          [ 2.5703,  2.5703,  2.5703,  ...,  2.5529,  2.5877,  2.5703],
          ...,
          [-1.5604, -1.4559, -1.3861,  ...,  1.8905,  2.0474,  2.2217],
          [-1.2467, -1.1770, -1.1073,  ..., -1.6302, -1.1073, -0.7238],
          [-0.2010, -1.5256, -1.2816,  ..., -0.4624, -0.1661,  0.2696]]],


        [[[ 0.9817,  0.1083,  0.5364,  ...,  0.4337,  1.3413,  2.0777],
          [ 0.9988,  1.1529,  0.7762,  ...,  0.8276,  1.5810,  2.1119],
          [ 0.2624, -0.1143,  0.4508,  ...,  0.8276,  0.8789,  1.6324],
          ...,
          [-1.2103, -1.1075, -1.0562,  ...,  1.0673,  0.9646,  1.2899],
          [-1.2445, -1.0390, -0.9534,  ...,  0.9132,  1.0159,  1.6495],
          [-1.1932, -1.0904, -1.0562,  ...,  1.1015,  0.8789,  1.3242]],

         [[ 0.2402, -0.5826, -0.0749,  ...,  0.5553,  1.4832,  2.2360],
          [ 0.2752,  0.5378,  0.1176,  ...,  0.9405,  1.7633,  2.3060],
          [-0.5126, -0.7927, -0.3375,  ...,  0.9055,  1.0455,  1.8158],
          ...,
          [-1.4930, -1.4055, -1.3354,  ...,  1.1155,  1.0630,  1.3782],
          [-1.5280, -1.3354, -1.2479,  ...,  0.9580,  1.0980,  1.7458],
          [-1.5455, -1.4405, -1.4230,  ...,  1.1331,  0.9580,  1.4307]],

         [[ 0.1128, -0.7238, -0.2707,  ...,  0.9668,  1.8208,  2.5006],
          [ 0.1825,  0.3219, -0.1487,  ...,  1.2980,  2.0648,  2.5877],
          [-0.7238, -0.9678, -0.5670,  ...,  1.2457,  1.3328,  2.0997],
          ...,
          [-1.6127, -1.5081, -1.4036,  ...,  1.3677,  1.2805,  1.5768],
          [-1.5953, -1.4036, -1.3513,  ...,  1.1585,  1.3154,  1.9603],
          [-1.6127, -1.4907, -1.5256,  ...,  1.2631,  1.1062,  1.5594]]],


        [[[ 0.6563,  0.8276,  0.9988,  ..., -1.5528, -1.6727, -1.6727],
          [ 0.8447,  0.9303,  1.0331,  ..., -1.5870, -1.6555, -1.6555],
          [ 1.2899,  1.2728,  1.2728,  ..., -1.5870, -1.6555, -1.6555],
          ...,
          [-0.5424, -0.5253, -0.5082,  ..., -0.4054, -0.4739, -0.6109],
          [-0.5424, -0.5253, -0.5082,  ..., -0.4739, -0.5596, -0.6794],
          [-0.5082, -0.4911, -0.4911,  ..., -0.5424, -0.6109, -0.6281]],

         [[ 0.0826,  0.2577,  0.4328,  ..., -1.8782, -1.9832, -1.9307],
          [ 0.2752,  0.3452,  0.4503,  ..., -1.9132, -1.9657, -1.9132],
          [ 0.7304,  0.6954,  0.6954,  ..., -1.8431, -1.9132, -1.8782],
          ...,
          [-1.2304, -1.2129, -1.2129,  ..., -1.0553, -1.1429, -1.2829],
          [-1.2304, -1.2129, -1.1954,  ..., -1.1078, -1.2129, -1.3529],
          [-1.2304, -1.1954, -1.1779,  ..., -1.1779, -1.2479, -1.2829]],

         [[-0.6367, -0.4450, -0.2532,  ..., -1.7522, -1.7522, -1.6824],
          [-0.4624, -0.3404, -0.1661,  ..., -1.7696, -1.6999, -1.6476],
          [ 0.0082,  0.0605,  0.1302,  ..., -1.7696, -1.7522, -1.7173],
          ...,
          [-1.6302, -1.5953, -1.5779,  ..., -1.4210, -1.4907, -1.6302],
          [-1.6302, -1.5953, -1.5604,  ..., -1.4733, -1.5779, -1.6999],
          [-1.6302, -1.5779, -1.5430,  ..., -1.5430, -1.6302, -1.6650]]]])
[19:57:18.773271] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:18.790045] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:20.096941] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True, False,  ...,  True,  True,  True],
        [ True,  True, False,  ..., False,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:20.308302] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.308565] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.308989] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.309455] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.309925] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.310390] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.310867] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.311330] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.311824] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.312280] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.312750] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.313221] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.313680] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.314145] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.314610] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.315082] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.315547] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.316017] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.316484] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:20.376783] INFO: samples:  tensor([[[[-1.4500, -1.4672, -1.4672,  ...,  0.5364,  0.4851,  0.4851],
          [-1.4672, -1.4843, -1.4672,  ...,  0.5193,  0.4679,  0.4679],
          [-1.4672, -1.4672, -1.4500,  ...,  0.5022,  0.4679,  0.4679],
          ...,
          [ 1.9578,  1.9578,  1.9407,  ...,  0.3481,  0.3481,  0.3481],
          [ 1.9235,  1.9235,  1.8893,  ...,  0.3652,  0.3652,  0.3652],
          [ 1.9064,  1.8893,  1.8550,  ...,  0.3309,  0.3309,  0.3309]],

         [[-1.6506, -1.6681, -1.6681,  ...,  0.3277,  0.2752,  0.2752],
          [-1.6681, -1.6856, -1.6681,  ...,  0.3102,  0.2577,  0.2577],
          [-1.6681, -1.6856, -1.6681,  ...,  0.2927,  0.2577,  0.2577],
          ...,
          [ 2.1134,  2.1134,  2.1134,  ...,  0.1527,  0.1527,  0.1527],
          [ 2.0609,  2.0609,  2.0609,  ...,  0.1702,  0.1702,  0.1702],
          [ 2.0434,  2.0259,  2.0259,  ...,  0.1352,  0.1352,  0.1352]],

         [[-1.5430, -1.5604, -1.5604,  ...,  0.4265,  0.3742,  0.3742],
          [-1.5604, -1.5779, -1.5604,  ...,  0.4091,  0.3568,  0.3393],
          [-1.5604, -1.5604, -1.5430,  ...,  0.3568,  0.3393,  0.3219],
          ...,
          [ 2.4831,  2.4831,  2.4831,  ...,  0.1128,  0.1128,  0.1128],
          [ 2.4831,  2.4657,  2.4483,  ...,  0.1302,  0.1302,  0.1302],
          [ 2.4831,  2.4308,  2.4134,  ...,  0.0953,  0.0953,  0.0953]]],


        [[[-0.7479, -0.5596, -0.7650,  ...,  0.2624,  0.4166,  0.1939],
          [-0.9534, -0.8678, -0.9877,  ..., -0.2684,  0.5364,  0.4337],
          [-0.8335, -0.8335, -0.7479,  ..., -0.4911,  0.3652,  0.4508],
          ...,
          [-0.3369,  0.1939,  0.4508,  ...,  0.7077,  0.4679,  0.2111],
          [ 0.3994,  0.5707,  0.4508,  ...,  0.6049,  0.3138, -0.0287],
          [ 0.5536,  0.6049,  0.3652,  ...,  0.5536,  0.4508,  0.0741]],

         [[-0.6001, -0.4601, -0.6702,  ...,  0.3978,  0.5728,  0.3627],
          [-0.7577, -0.6877, -0.8627,  ..., -0.2150,  0.6604,  0.6254],
          [-0.6001, -0.6527, -0.6001,  ..., -0.3550,  0.5378,  0.6254],
          ...,
          [-0.3375,  0.1877,  0.4503,  ...,  0.8179,  0.5728,  0.3452],
          [ 0.4153,  0.6078,  0.5203,  ...,  0.7479,  0.4678,  0.1702],
          [ 0.5903,  0.6604,  0.4328,  ...,  0.6954,  0.5903,  0.1877]],

         [[-0.5495, -0.3578, -0.6018,  ...,  0.6705,  0.8099,  0.4614],
          [-0.5670, -0.6367, -0.8458,  ...,  0.1128,  0.8622,  0.7576],
          [-0.3230, -0.4450, -0.3055,  ..., -0.0092,  0.6531,  0.8099],
          ...,
          [-0.1138,  0.3916,  0.5834,  ...,  0.8797,  0.5311,  0.1825],
          [ 0.4962,  0.6879,  0.5485,  ...,  0.7925,  0.4962,  0.1999],
          [ 0.6008,  0.6705,  0.4962,  ...,  0.7402,  0.6531,  0.3045]]],


        [[[ 1.1700,  1.2557,  1.2385,  ...,  1.5982,  1.5982,  1.6495],
          [ 1.1872,  1.3413,  1.1529,  ...,  1.5468,  1.5468,  1.6324],
          [ 1.5810,  1.4954,  1.3927,  ...,  1.5468,  1.4954,  1.5639],
          ...,
          [ 0.4679,  0.4851,  0.4851,  ...,  1.8722,  1.8722,  1.9920],
          [ 0.4166,  0.4851,  0.5193,  ...,  1.9578,  1.9235,  2.0605],
          [ 0.3823,  0.4508,  0.4851,  ...,  1.9235,  1.9578,  2.0605]],

         [[-1.0028, -0.9153, -0.9153,  ...,  0.7479,  0.7479,  0.8004],
          [-0.6877, -0.6352, -0.7752,  ...,  0.6954,  0.6954,  0.7829],
          [ 0.0301, -0.1275, -0.3375,  ...,  0.6604,  0.6604,  0.7304],
          ...,
          [ 0.8529,  0.8880,  0.8880,  ...,  2.0259,  1.9559,  2.0784],
          [ 0.8004,  0.8880,  0.9055,  ...,  2.0784,  2.0084,  2.1485],
          [ 0.7829,  0.8529,  0.8354,  ...,  2.0434,  2.0434,  2.1310]],

         [[-0.3927, -0.3055, -0.2881,  ...,  0.6182,  0.6182,  0.6705],
          [-0.2532, -0.1312, -0.2707,  ...,  0.5311,  0.5659,  0.6531],
          [ 0.2696,  0.1651,  0.0082,  ...,  0.4962,  0.5311,  0.6008],
          ...,
          [ 1.5245,  1.5071,  1.4897,  ...,  2.0648,  2.0648,  2.2391],
          [ 1.4722,  1.5071,  1.4897,  ...,  2.1346,  2.1171,  2.3263],
          [ 1.4025,  1.4722,  1.4374,  ...,  2.0997,  2.1868,  2.3437]]],


        ...,


        [[[ 0.8276, -1.5357, -1.8782,  ..., -1.4672, -1.2959, -1.2788],
          [ 1.4612, -0.9363, -1.9980,  ..., -1.5870, -1.2959, -1.2617],
          [ 1.8550, -0.5424, -1.8782,  ..., -1.3815, -1.3302, -1.3130],
          ...,
          [-0.9534, -0.9020, -0.9363,  ..., -1.7925, -2.0323, -1.8610],
          [-1.2959, -0.9192, -0.8507,  ..., -1.9467, -1.8782, -1.8439],
          [-1.3815, -1.0390, -1.0048,  ..., -2.1179, -1.8953, -1.8782]],

         [[ 0.6779, -1.8081, -2.0007,  ..., -1.1954, -1.2479, -1.2304],
          [ 1.4132, -1.3880, -2.0007,  ..., -1.0378, -1.1604, -1.2829],
          [ 2.1660, -0.9153, -2.0357,  ..., -0.9503, -1.2479, -1.1253],
          ...,
          [-1.5805, -1.6856, -1.5105,  ..., -1.8957, -1.8782, -1.8957],
          [-1.5455, -1.5805, -1.6155,  ..., -1.8957, -1.8606, -1.9132],
          [-1.5105, -1.5980, -1.5980,  ..., -1.8782, -1.8957, -1.7731]],

         [[ 0.5311, -1.1421, -1.5081,  ..., -0.9678, -1.2467, -1.0201],
          [ 1.0888, -1.0376, -1.5604,  ..., -0.9678, -1.2119, -0.9678],
          [ 1.5768, -0.7064, -1.5604,  ..., -0.8807, -1.2293, -1.1247],
          ...,
          [-1.6650, -1.5779, -1.7347,  ..., -1.7870, -1.6476, -1.7347],
          [-1.6824, -1.5953, -1.6999,  ..., -1.6824, -1.7173, -1.6650],
          [-1.8044, -1.6999, -1.6302,  ..., -1.6824, -1.7173, -1.6824]]],


        [[[-1.1589, -1.1932, -1.2445,  ..., -0.9877, -1.0904, -1.0219],
          [-1.1589, -1.2103, -1.2445,  ..., -1.0048, -1.0562, -1.0562],
          [-1.1932, -1.2274, -1.2274,  ..., -0.9877, -1.0390, -1.0562],
          ...,
          [-1.0733, -1.0390, -1.0048,  ..., -0.5082, -0.4397, -0.4397],
          [-1.1075, -1.0733, -1.0390,  ..., -0.4226, -0.3883, -0.4226],
          [-1.1075, -1.0733, -1.0390,  ..., -0.4054, -0.3712, -0.3883]],

         [[-1.0728, -1.0728, -1.0728,  ..., -0.8452, -0.8102, -0.8627],
          [-1.0728, -1.0728, -1.1253,  ..., -0.7927, -0.8102, -0.8627],
          [-1.0903, -1.0903, -1.1078,  ..., -0.8102, -0.8102, -0.8277],
          ...,
          [-1.2129, -1.2304, -1.1604,  ..., -0.2850, -0.2675, -0.2675],
          [-1.2129, -1.1954, -1.1604,  ..., -0.2150, -0.2150, -0.1975],
          [-1.2129, -1.1954, -1.1253,  ..., -0.1800, -0.1800, -0.1625]],

         [[-0.7064, -0.7064, -0.7587,  ..., -0.4275, -0.4450, -0.4275],
          [-0.7064, -0.7238, -0.7936,  ..., -0.4101, -0.4275, -0.4624],
          [-0.7238, -0.7413, -0.7936,  ..., -0.4101, -0.4450, -0.4973],
          ...,
          [-1.2293, -1.2467, -1.1770,  ..., -0.7936, -0.7936, -0.8110],
          [-1.2641, -1.2467, -1.1596,  ..., -0.7761, -0.7587, -0.7761],
          [-1.2641, -1.2467, -1.1596,  ..., -0.7761, -0.7413, -0.7064]]],


        [[[-1.7754, -1.8097, -1.8097,  ..., -1.5870, -1.5528, -1.6042],
          [-1.8097, -1.8097, -1.8097,  ..., -1.6042, -1.6555, -1.6384],
          [-1.8097, -1.8097, -1.7925,  ..., -1.6727, -1.6898, -1.6898],
          ...,
          [-1.7925, -1.7754, -1.8268,  ...,  2.0092,  2.0263,  2.0092],
          [-1.8097, -1.7925, -1.8097,  ...,  2.0263,  2.0434,  2.0092],
          [-1.8097, -1.8097, -1.8097,  ...,  1.9920,  2.0092,  2.0092]],

         [[-1.4930, -1.5280, -1.5105,  ..., -0.9328, -0.9678, -1.0378],
          [-1.4580, -1.4930, -1.5105,  ..., -1.0028, -1.0728, -1.1078],
          [-1.4580, -1.4930, -1.4930,  ..., -1.0903, -1.1604, -1.1779],
          ...,
          [-1.5105, -1.4930, -1.4755,  ...,  1.6583,  1.6408,  1.6232],
          [-1.5280, -1.5105, -1.4930,  ...,  1.6232,  1.6583,  1.6232],
          [-1.5280, -1.5280, -1.5280,  ...,  1.6057,  1.6232,  1.6232]],

         [[-1.3164, -1.3513, -1.3513,  ..., -1.1596, -1.1596, -1.2119],
          [-1.3513, -1.3861, -1.3513,  ..., -1.1596, -1.1944, -1.2119],
          [-1.3513, -1.3687, -1.3339,  ..., -1.2119, -1.2293, -1.2467],
          ...,
          [-1.3861, -1.3687, -1.4210,  ...,  2.2566,  2.2566,  2.2391],
          [-1.4036, -1.3861, -1.4210,  ...,  2.2566,  2.2740,  2.2391],
          [-1.4036, -1.4036, -1.4384,  ...,  2.2217,  2.2391,  2.2391]]]])
[19:57:20.383425] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:20.401331] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:21.708932] INFO: mask:  tensor([[ True,  True, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True, False,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:21.920408] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.920677] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.921096] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.921561] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.922032] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.922506] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.922976] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.923442] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.923912] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.924381] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.924849] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.925322] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.925791] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.926260] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.926748] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.927201] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.927661] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.928126] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.928602] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:21.990351] INFO: samples:  tensor([[[[-0.4226, -0.3883, -0.3369,  ...,  1.8208,  1.8208,  1.6324],
          [-0.4054, -0.3541, -0.3369,  ...,  1.8037,  1.8550,  2.1290],
          [-0.4739, -0.4226, -0.4397,  ...,  1.8550,  0.9817,  1.1187],
          ...,
          [ 1.5810,  1.6838,  1.8379,  ...,  2.2489,  2.2489,  2.2489],
          [ 1.9578,  1.9235,  1.9064,  ...,  2.2489,  2.2489,  2.2489],
          [ 1.7694,  1.7865,  1.7865,  ...,  2.2489,  2.2489,  2.2489]],

         [[ 0.0476,  0.0651,  0.0651,  ..., -1.7206, -1.5980, -1.8081],
          [ 0.0651,  0.0301, -0.0224,  ..., -2.0357, -1.9132, -1.6331],
          [-0.0574, -0.0224, -0.0574,  ..., -1.5455, -2.0357, -1.9657],
          ...,
          [ 1.8859,  2.0084,  2.1660,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.2885,  2.2360,  2.2010,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.0959,  2.0784,  2.0784,  ...,  2.4286,  2.4286,  2.4286]],

         [[-0.3753, -0.3753, -0.3230,  ...,  1.2457,  1.1237,  0.9319],
          [-0.3230, -0.2532, -0.2707,  ...,  1.2980,  1.1062,  1.1934],
          [-0.4275, -0.2881, -0.3055,  ...,  1.7337,  0.5659,  0.1128],
          ...,
          [ 2.1520,  2.2566,  2.3611,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.5703,  2.5529,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.4308,  2.4308,  2.4308,  ...,  2.6400,  2.6400,  2.6400]]],


        [[[ 0.6563,  0.6563,  0.6049,  ...,  0.6392,  0.6392,  0.6392],
          [ 0.6221,  0.6563,  0.6392,  ...,  0.6392,  0.6392,  0.6392],
          [ 0.5707,  0.6221,  0.6392,  ...,  0.6734,  0.6392,  0.6392],
          ...,
          [-0.0458, -0.3712,  0.0912,  ...,  0.7762,  0.8104,  0.8104],
          [-1.3473, -1.6213, -1.3130,  ...,  0.7419,  0.7933,  0.7933],
          [-1.6042, -1.6213, -1.3130,  ...,  0.8104,  0.7933,  0.7933]],

         [[ 0.9055,  0.9055,  0.9230,  ...,  0.9580,  0.9930,  0.9930],
          [ 0.9055,  0.9055,  0.9055,  ...,  0.9580,  0.9930,  0.9930],
          [ 0.9405,  0.9230,  0.9055,  ...,  0.9580,  0.9930,  0.9930],
          ...,
          [ 0.3102, -0.1099,  0.2402,  ...,  1.0980,  1.0805,  1.0805],
          [-1.0903, -1.3880, -1.0903,  ...,  1.0805,  1.0805,  1.0805],
          [-1.4755, -1.5455, -1.2304,  ...,  1.0630,  1.0805,  1.0805]],

         [[ 1.5420,  1.5594,  1.5768,  ...,  1.6465,  1.6291,  1.6291],
          [ 1.5768,  1.5768,  1.5768,  ...,  1.6117,  1.6291,  1.6291],
          [ 1.5942,  1.5768,  1.5768,  ...,  1.6117,  1.6291,  1.6291],
          ...,
          [ 0.0082, -0.3927, -0.0267,  ...,  1.7163,  1.6814,  1.6988],
          [-1.2119, -1.4384, -1.2293,  ...,  1.7163,  1.6814,  1.6988],
          [-1.4210, -1.4907, -1.2641,  ...,  1.7337,  1.6988,  1.6988]]],


        [[[ 0.5707,  0.5707,  0.5707,  ...,  0.5707,  0.5707,  0.5707],
          [ 0.5878,  0.5878,  0.5878,  ...,  0.6049,  0.6049,  0.6049],
          [ 0.6049,  0.6049,  0.5878,  ...,  0.6049,  0.6049,  0.6049],
          ...,
          [ 0.8104,  0.8789,  0.8961,  ...,  1.1358,  0.8447,  0.5878],
          [ 0.8104,  0.8447,  0.8618,  ...,  1.3070,  1.0673,  0.7933],
          [ 0.8104,  0.8104,  0.8104,  ...,  1.4612,  1.2385,  0.9988]],

         [[-0.0224, -0.0049, -0.0049,  ..., -0.0224, -0.0049, -0.0049],
          [-0.0224, -0.0224, -0.0049,  ..., -0.0224, -0.0224, -0.0224],
          [-0.0224, -0.0224, -0.0049,  ..., -0.0224, -0.0224, -0.0224],
          ...,
          [ 0.6779,  0.7479,  0.7654,  ...,  1.1681,  0.8179,  0.5378],
          [ 0.6779,  0.7129,  0.7304,  ...,  1.3782,  1.0805,  0.7829],
          [ 0.6604,  0.6604,  0.6779,  ...,  1.5532,  1.2731,  1.0105]],

         [[-0.5495, -0.5495, -0.5670,  ..., -0.5670, -0.5495, -0.5495],
          [-0.6018, -0.6018, -0.6193,  ..., -0.5844, -0.5844, -0.5844],
          [-0.5844, -0.6018, -0.6018,  ..., -0.5844, -0.5844, -0.5844],
          ...,
          [ 0.6705,  0.7402,  0.7576,  ...,  1.4200,  1.0017,  0.6182],
          [ 0.6705,  0.7054,  0.7228,  ...,  1.6640,  1.2805,  0.8797],
          [ 0.6705,  0.6705,  0.6705,  ...,  1.8731,  1.5594,  1.1759]]],


        ...,


        [[[ 1.5297,  1.2385,  1.2899,  ...,  2.1804,  2.2318,  2.1975],
          [ 1.2043,  1.1358,  1.1529,  ...,  2.1975,  2.1975,  2.2147],
          [ 1.2043,  1.3242,  1.1529,  ...,  2.1804,  2.1804,  2.1975],
          ...,
          [-0.6109, -0.6109, -0.5938,  ...,  0.2967,  0.2796,  0.2453],
          [-0.6794, -0.6623, -0.7137,  ...,  0.2282,  0.3309,  0.2967],
          [-0.6452, -0.7137, -0.8335,  ...,  0.2624,  0.1939,  0.1768]],

         [[ 1.0105,  0.8179,  0.8704,  ...,  1.0455,  1.0630,  0.9930],
          [ 0.7129,  0.6954,  0.7129,  ...,  1.0105,  0.9405,  0.9230],
          [ 0.6954,  0.8704,  0.7129,  ...,  0.8880,  0.8704,  0.8529],
          ...,
          [-1.5980, -1.7031, -1.7206,  ..., -0.8277, -0.8277, -0.8277],
          [-1.7206, -1.6856, -1.6506,  ..., -0.9153, -0.7927, -0.7752],
          [-1.7556, -1.7031, -1.6155,  ..., -0.8452, -0.8978, -0.9153]],

         [[-0.6193, -0.7413, -0.6715,  ..., -0.3404, -0.3927, -0.4450],
          [-0.7936, -0.7587, -0.8284,  ..., -0.3230, -0.4450, -0.5147],
          [-0.7238, -0.4450, -0.7064,  ..., -0.4101, -0.5147, -0.5321],
          ...,
          [-1.6476, -1.5604, -1.6650,  ..., -1.7173, -1.7870, -1.6999],
          [-1.6650, -1.7347, -1.7870,  ..., -1.7696, -1.7522, -1.6824],
          [-1.5953, -1.6999, -1.7347,  ..., -1.6999, -1.8044, -1.7347]]],


        [[[-0.5938, -0.5596, -0.5082,  ...,  1.4783,  1.4783,  1.4954],
          [-0.6623, -0.6452, -0.5767,  ...,  1.4783,  1.4783,  1.4954],
          [-0.7308, -0.7137, -0.6623,  ...,  1.4954,  1.4954,  1.5125],
          ...,
          [ 1.4440,  1.4440,  1.4440,  ...,  2.0948,  2.1290,  2.1633],
          [ 1.4440,  1.4269,  1.4440,  ...,  2.1119,  2.1290,  2.1633],
          [ 1.4269,  1.4098,  1.4269,  ...,  2.1290,  2.1633,  2.1804]],

         [[-0.9503, -0.9328, -0.8978,  ...,  1.4132,  1.4132,  1.4132],
          [-0.9678, -0.9503, -0.9328,  ...,  1.4132,  1.4132,  1.4132],
          [-1.0028, -1.0028, -0.9678,  ...,  1.4307,  1.4307,  1.4307],
          ...,
          [ 1.6408,  1.6408,  1.6408,  ...,  1.5357,  1.5707,  1.6057],
          [ 1.6408,  1.6232,  1.6408,  ...,  1.5357,  1.5532,  1.5882],
          [ 1.6232,  1.6057,  1.6232,  ...,  1.5182,  1.5532,  1.5707]],

         [[-0.5321, -0.4973, -0.4624,  ...,  1.7511,  1.7511,  1.7511],
          [-0.5321, -0.5147, -0.4798,  ...,  1.7511,  1.7511,  1.7511],
          [-0.5670, -0.5495, -0.5147,  ...,  1.7860,  1.7860,  1.7860],
          ...,
          [ 2.0648,  2.0648,  2.0648,  ...,  1.3328,  1.3677,  1.4200],
          [ 2.0648,  2.0474,  2.0648,  ...,  1.3328,  1.3677,  1.4025],
          [ 2.0474,  2.0300,  2.0474,  ...,  1.3328,  1.3677,  1.3851]]],


        [[[-0.4226, -0.2342, -0.2513,  ..., -2.0152, -2.0665, -1.9467],
          [-0.4054, -0.3541, -0.1657,  ..., -1.4158, -1.6213, -1.6898],
          [-0.5424, -0.3198,  0.6049,  ..., -0.8849, -1.1247, -1.1589],
          ...,
          [ 1.4098,  1.3755,  1.3755,  ...,  0.7077,  0.6906,  0.6221],
          [ 1.3584,  1.4269,  1.4612,  ...,  0.6049,  0.5364,  0.5193],
          [ 1.3927,  1.3413,  1.3755,  ...,  0.5707,  0.5022,  0.5193]],

         [[-0.6001, -0.4601, -0.6352,  ..., -1.8431, -1.9132, -1.7731],
          [-0.3901, -0.8452, -0.6001,  ..., -1.4055, -1.5630, -1.7556],
          [-1.0553, -1.0203,  0.1352,  ..., -0.6527, -0.6877, -0.9153],
          ...,
          [ 1.6057,  1.6408,  1.6583,  ...,  0.8354,  0.7829,  0.8179],
          [ 1.5532,  1.5532,  1.5707,  ...,  0.7654,  0.7304,  0.7129],
          [ 1.5357,  1.5707,  1.5882,  ...,  0.6779,  0.6429,  0.6604]],

         [[-0.4450, -0.3578, -0.6541,  ..., -1.4559, -1.4907, -1.4036],
          [-0.6193, -0.7936, -0.3404,  ..., -1.1073, -1.2816, -1.4907],
          [-1.1770, -0.7413,  0.4962,  ..., -0.0790, -0.1487, -0.2532],
          ...,
          [ 1.9428,  1.9428,  1.9951,  ...,  1.4200,  1.4200,  1.3851],
          [ 1.9254,  1.9254,  1.8905,  ...,  1.3154,  1.2980,  1.3851],
          [ 1.8208,  1.8731,  1.9603,  ...,  1.2631,  1.2805,  1.2631]]]])
[19:57:21.997667] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:22.015699] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:23.321538] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True, False,  True],
        [ True,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:23.533108] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.533381] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.533804] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.534274] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.534771] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.535218] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.535684] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.536153] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.536626] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.537094] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.537558] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.538030] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.538497] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.538969] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.539436] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.539911] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.540371] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.540843] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.541315] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:23.607478] INFO: samples:  tensor([[[[-1.0904, -1.1075, -1.0904,  ..., -2.1008, -2.1008, -2.1008],
          [-1.0904, -1.1075, -1.0904,  ..., -2.1008, -2.1008, -2.1008],
          [-1.0904, -1.1075, -1.0904,  ..., -2.1008, -2.1008, -2.1008],
          ...,
          [-1.3473, -0.9363, -0.5082,  ..., -1.6555, -1.7925, -1.9467],
          [-1.4672, -1.1418, -0.6452,  ..., -1.3987, -1.5870, -1.8439],
          [-1.3815, -1.0219, -0.5082,  ..., -1.2445, -1.4843, -1.7754]],

         [[-0.8452, -0.8627, -0.8452,  ..., -2.0182, -2.0182, -2.0182],
          [-0.8452, -0.8627, -0.8452,  ..., -2.0182, -2.0182, -2.0182],
          [-0.8452, -0.8627, -0.8452,  ..., -2.0182, -2.0182, -2.0182],
          ...,
          [-1.3529, -1.0378, -0.7402,  ..., -1.6506, -1.7731, -1.8957],
          [-1.4930, -1.2479, -0.8627,  ..., -1.4230, -1.5630, -1.8081],
          [-1.4055, -1.1253, -0.7402,  ..., -1.2479, -1.4580, -1.7381]],

         [[-1.4559, -1.5081, -1.5081,  ..., -1.8044, -1.8044, -1.8044],
          [-1.4559, -1.5081, -1.5081,  ..., -1.7870, -1.7870, -1.7870],
          [-1.4733, -1.5081, -1.5081,  ..., -1.7870, -1.7870, -1.7870],
          ...,
          [-1.3861, -1.1596, -0.9678,  ..., -1.7347, -1.7522, -1.7696],
          [-1.5081, -1.3687, -1.1073,  ..., -1.5430, -1.6302, -1.7347],
          [-1.4210, -1.2467, -0.9853,  ..., -1.4559, -1.5779, -1.7347]]],


        [[[ 2.2489,  2.1804,  1.9407,  ...,  2.2489,  2.1975,  2.0777],
          [ 2.1462,  1.9578,  1.8722,  ...,  2.2489,  2.2318,  2.1290],
          [ 1.8550,  1.8379,  1.9749,  ...,  2.0948,  2.2318,  2.2147],
          ...,
          [ 1.7523,  1.7352,  1.6838,  ...,  2.0948,  1.8550,  1.6667],
          [ 1.7352,  1.7523,  1.7352,  ...,  1.9749,  1.8208,  1.6838],
          [ 1.7009,  1.7694,  1.7523,  ...,  1.8208,  1.7009,  1.6153]],

         [[ 2.4286,  2.3761,  2.0434,  ...,  2.2710,  2.1485,  1.9734],
          [ 2.2360,  2.0084,  1.9559,  ...,  2.2010,  2.1660,  2.0434],
          [ 1.8859,  1.8508,  2.0609,  ...,  2.0084,  2.1485,  2.1485],
          ...,
          [ 1.8158,  1.7983,  1.7458,  ...,  2.1310,  1.9209,  1.7458],
          [ 1.7983,  1.8158,  1.7983,  ...,  1.9909,  1.8859,  1.7458],
          [ 1.7633,  1.8333,  1.8158,  ...,  1.8508,  1.7633,  1.6758]],

         [[ 2.5006,  2.1346,  1.7511,  ...,  2.1346,  1.9777,  1.8208],
          [ 2.0997,  1.8383,  1.7860,  ...,  2.0997,  2.0300,  1.8731],
          [ 1.7163,  1.6988,  1.9254,  ...,  1.8905,  1.9951,  1.9603],
          ...,
          [ 1.6465,  1.6291,  1.5768,  ...,  1.8557,  1.6640,  1.5942],
          [ 1.6291,  1.6465,  1.6291,  ...,  1.7511,  1.6640,  1.6117],
          [ 1.5942,  1.6640,  1.6465,  ...,  1.5942,  1.5245,  1.5420]]],


        [[[-0.3198, -0.6452, -0.7137,  ...,  1.2214,  1.2214,  1.2899],
          [-0.6109, -0.7308, -0.9363,  ...,  0.6734,  0.8789,  0.8789],
          [-1.0219, -0.9363, -0.9705,  ..., -0.3541, -0.1314,  0.1083],
          ...,
          [-2.1008, -2.1008, -2.1008,  ..., -0.1486, -0.4226,  0.3823],
          [-2.1008, -2.0837, -2.1008,  ..., -0.2513, -0.3369, -0.4397],
          [-2.1179, -2.1008, -2.0665,  ...,  0.1939, -0.3541, -0.5253]],

         [[-0.8102, -1.0553, -1.0903,  ...,  1.0630,  1.0455,  1.1155],
          [-0.9503, -1.1078, -1.2654,  ...,  0.4853,  0.6429,  0.6429],
          [-1.2479, -1.2304, -1.3179,  ..., -0.3901, -0.1800,  0.0301],
          ...,
          [-2.0182, -2.0182, -2.0182,  ..., -0.1099, -0.3375,  0.5203],
          [-2.0182, -1.9832, -2.0182,  ..., -0.1625, -0.2325, -0.3375],
          [-2.0357, -2.0007, -1.9832,  ...,  0.3102, -0.2675, -0.4601]],

         [[-0.7413, -1.0027, -1.0376,  ...,  0.5311,  0.6705,  0.7576],
          [-0.9853, -1.1073, -1.2467,  ..., -0.2881,  0.0082,  0.0779],
          [-1.3164, -1.2467, -1.2816,  ..., -1.2641, -0.9330, -0.6715],
          ...,
          [-1.7870, -1.7696, -1.7870,  ...,  0.5136,  0.2696,  1.0539],
          [-1.7696, -1.7347, -1.7870,  ...,  0.4439,  0.3045,  0.1825],
          [-1.7522, -1.8044, -1.7347,  ...,  0.8448,  0.2696,  0.0605]]],


        ...,


        [[[-0.6281, -0.2513, -0.5938,  ..., -1.2959, -1.1760, -1.0904],
          [-0.5767, -0.4397, -0.5596,  ..., -1.1932, -1.0562, -0.7650],
          [-0.5082,  0.4337,  0.2796,  ..., -1.0390, -0.9363, -0.6623],
          ...,
          [ 1.5468,  1.1872,  0.7248,  ...,  0.5536,  1.2043,  1.8208],
          [ 0.4166,  0.7248,  0.0569,  ...,  1.4440,  0.8618,  1.1872],
          [ 0.0227,  0.1083, -0.5082,  ...,  1.4783,  0.5022,  0.2453]],

         [[-0.8452, -0.3901, -0.4776,  ..., -1.0203, -0.9678, -0.9678],
          [-0.8102, -0.6352, -0.5301,  ..., -0.9153, -0.8452, -0.6176],
          [-0.8102,  0.2227,  0.2927,  ..., -0.8102, -0.7402, -0.5126],
          ...,
          [ 1.3431,  0.9930,  0.5028,  ...,  0.4853,  1.0980,  1.7458],
          [ 0.1001,  0.5028, -0.1625,  ...,  1.4482,  0.7304,  1.0280],
          [-0.3200, -0.1625, -0.7402,  ...,  1.4132,  0.4153,  0.1702]],

         [[-0.7587, -0.3230, -0.4973,  ..., -0.7936, -0.7761, -0.8110],
          [-0.6367, -0.4624, -0.4450,  ..., -0.7238, -0.6890, -0.4798],
          [-0.5147,  0.4788,  0.4614,  ..., -0.6193, -0.5844, -0.3927],
          ...,
          [ 1.6117,  1.2108,  0.7925,  ...,  0.7576,  1.3154,  1.8383],
          [ 0.4439,  0.7402,  0.1128,  ...,  1.6988,  0.9494,  1.1934],
          [-0.0441,  0.0779, -0.4450,  ...,  1.6291,  0.6008,  0.3568]]],


        [[[ 2.2489,  2.2489,  2.2489,  ..., -1.4843, -1.6898, -1.6213],
          [ 2.2489,  2.2489,  2.2489,  ..., -1.5185, -1.6213, -1.6727],
          [ 2.2489,  2.2489,  2.2489,  ..., -1.2103, -1.5870, -1.6384],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489]],

         [[ 2.4286,  2.4286,  2.4286,  ..., -1.2479, -1.3880, -1.3529],
          [ 2.4286,  2.4286,  2.4286,  ..., -1.3179, -1.3354, -1.4055],
          [ 2.4286,  2.4286,  2.4286,  ..., -1.0553, -1.2654, -1.3529],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286]],

         [[ 2.6400,  2.6400,  2.6400,  ..., -0.6193, -0.8807, -0.6890],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.7761, -0.7587, -0.7413],
          [ 2.6400,  2.6400,  2.6400,  ..., -0.6367, -0.5844, -0.7413],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400]]],


        [[[ 0.7077,  0.7077,  0.7077,  ...,  0.6906,  0.7419,  0.6906],
          [ 0.7248,  0.7248,  0.7248,  ...,  0.6563,  0.7077,  0.6734],
          [ 0.7419,  0.7419,  0.7248,  ...,  0.6049,  0.6906,  0.6392],
          ...,
          [ 0.4679,  0.4679,  0.4851,  ..., -1.7925, -1.7925, -1.7925],
          [ 0.4679,  0.4851,  0.4851,  ..., -1.8097, -1.8097, -1.8097],
          [ 0.4508,  0.4851,  0.5022,  ..., -1.7069, -1.7240, -1.7412]],

         [[ 0.7654,  0.7654,  0.7654,  ..., -0.6176, -0.5826, -0.5476],
          [ 0.7829,  0.7829,  0.7829,  ..., -0.6176, -0.5651, -0.5476],
          [ 0.8004,  0.8004,  0.7829,  ..., -0.7402, -0.6352, -0.6176],
          ...,
          [ 0.5203,  0.5203,  0.5378,  ..., -1.6856, -1.6856, -1.6856],
          [ 0.5203,  0.5378,  0.5378,  ..., -1.7031, -1.7031, -1.7031],
          [ 0.5028,  0.5378,  0.5553,  ..., -1.5980, -1.6155, -1.6331]],

         [[ 0.9145,  0.9145,  0.9145,  ..., -0.4275, -0.3927, -0.3927],
          [ 0.9319,  0.9319,  0.9319,  ..., -0.4450, -0.3927, -0.3578],
          [ 0.9494,  0.9494,  0.9319,  ..., -0.5495, -0.4275, -0.4101],
          ...,
          [ 0.6879,  0.6879,  0.7054,  ..., -1.4210, -1.4210, -1.4210],
          [ 0.6879,  0.7054,  0.7054,  ..., -1.4384, -1.4384, -1.4384],
          [ 0.6705,  0.7054,  0.7228,  ..., -1.3339, -1.3513, -1.3687]]]])
[19:57:23.615982] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:23.633465] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:24.942779] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [False,  True,  True,  ...,  True,  True,  True],
        ...,
        [False,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:25.154359] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.154626] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.155053] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.155518] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.155983] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.156454] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.156919] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.157391] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.157858] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.158330] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.158822] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.159291] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.159757] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.160224] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.160694] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.161168] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.161627] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.162094] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.162566] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:25.224171] Epoch: [0]  [  20/2503]  eta: 2:56:28  lr: 0.000000  loss: 48.3412 (48.5159)  time: 1.6201  data: 0.0001  max mem: 3424
[19:57:25.224302] INFO: samples:  tensor([[[[ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],
          ...,
          [ 2.2489,  2.2489,  2.2489,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489],
          [ 2.2489,  2.2489,  2.2318,  ...,  2.2489,  2.2489,  2.2489]],

         [[ 2.4286,  2.4286,  2.4111,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4111,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4111,  ...,  2.4286,  2.4286,  2.4286],
          ...,
          [ 2.4286,  2.4286,  2.4286,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4111,  ...,  2.4286,  2.4286,  2.4286],
          [ 2.4286,  2.4286,  2.4111,  ...,  2.4286,  2.4286,  2.4286]],

         [[ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6400,  2.6400],
          ...,
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6226,  ...,  2.6400,  2.6400,  2.6400]]],


        [[[-0.9534, -0.8849, -0.7993,  ..., -0.6965, -0.7308, -0.4739],
          [-1.3644, -1.3815, -1.2274,  ..., -0.7650, -0.7308, -0.5082],
          [-1.5528, -1.5870, -1.5528,  ..., -0.7137, -0.7650, -0.5253],
          ...,
          [ 1.1872,  0.9474,  0.2967,  ..., -0.1314,  1.5810,  2.2489],
          [ 1.1872,  1.0331,  0.3652,  ..., -1.0219,  0.2967,  2.2147],
          [ 1.1015,  0.9646,  0.3652,  ..., -1.1075, -0.3541,  1.4612]],

         [[-0.6702, -0.6001, -0.5476,  ..., -0.4426, -0.6001, -0.3725],
          [-1.1604, -1.1429, -1.0028,  ..., -0.5126, -0.6001, -0.4076],
          [-1.4230, -1.4580, -1.4230,  ..., -0.4776, -0.6352, -0.4251],
          ...,
          [ 1.7108,  1.5182,  0.8880,  ..., -0.1099,  1.7108,  2.4286],
          [ 1.7283,  1.6408,  1.0455,  ..., -0.9328,  0.3978,  2.3936],
          [ 1.6758,  1.5707,  1.0105,  ..., -0.9853, -0.2675,  1.6232]],

         [[-0.8807, -0.8633, -0.8633,  ..., -0.6715, -0.7238, -0.7064],
          [-1.1770, -1.1944, -1.0898,  ..., -0.6715, -0.6890, -0.7413],
          [-1.2293, -1.2641, -1.2293,  ..., -0.7413, -0.7587, -0.5670],
          ...,
          [ 2.0300,  1.5768,  0.7054,  ..., -0.8981,  0.5834,  1.4200],
          [ 2.1520,  1.8034,  0.8971,  ..., -1.5953, -0.4101,  1.6988],
          [ 2.0823,  1.7163,  0.8797,  ..., -1.5953, -1.1596,  0.5136]]],


        [[[ 2.2147,  2.2489,  2.1804,  ..., -0.0629, -0.0629, -0.0629],
          [-0.6109,  1.0502,  2.0948,  ..., -0.0458, -0.0458, -0.0458],
          [-1.2274, -1.5528, -0.4739,  ..., -0.0287, -0.0458, -0.0458],
          ...,
          [ 0.6221,  0.4851,  0.3994,  ..., -2.0837, -2.0837, -2.0494],
          [ 0.4508,  0.2796,  0.2111,  ..., -2.0837, -2.0837, -2.0494],
          [ 0.3309,  0.2282,  0.3138,  ..., -2.1008, -2.1008, -2.0837]],

         [[ 2.2185,  2.2535,  2.2185,  ...,  0.8880,  0.8880,  0.8880],
          [-0.6527,  1.0980,  2.2010,  ...,  0.9055,  0.9055,  0.9055],
          [-1.4055, -1.7031, -0.8102,  ...,  0.9230,  0.9055,  0.9055],
          ...,
          [ 0.3803,  0.2402,  0.1527,  ..., -2.0182, -2.0182, -1.9832],
          [ 0.2052,  0.0301, -0.0399,  ..., -2.0182, -2.0182, -1.9832],
          [ 0.0826, -0.0224,  0.0651,  ..., -2.0357, -2.0357, -2.0182]],

         [[ 2.0474,  2.1171,  1.9777,  ...,  2.0474,  2.0474,  2.0474],
          [-0.4798,  1.0888,  2.0997,  ...,  2.0648,  2.0648,  2.0648],
          [-1.3513, -1.5953, -0.9504,  ...,  2.0823,  2.0648,  2.0648],
          ...,
          [ 0.3568,  0.2173,  0.1302,  ..., -1.6824, -1.6824, -1.6476],
          [ 0.1825,  0.0082, -0.0615,  ..., -1.7173, -1.6999, -1.6824],
          [ 0.0605, -0.0441,  0.0431,  ..., -1.7173, -1.7173, -1.6999]]],


        ...,


        [[[-0.4054, -0.3369, -0.3712,  ..., -0.4739, -0.3541, -0.3027],
          [-0.3883, -0.3541, -0.3541,  ..., -0.5082, -0.4054, -0.3712],
          [-0.3198, -0.2856, -0.2684,  ..., -0.3883, -0.4054, -0.4397],
          ...,
          [-0.2856, -0.2856, -0.5253,  ..., -0.3369, -0.5082, -0.1828],
          [-0.3198, -0.3541, -0.4739,  ..., -0.4226, -0.2342,  0.0398],
          [-0.3027, -0.4397, -0.3712,  ..., -0.3198, -0.1999, -0.3369]],

         [[-0.3901, -0.3725, -0.4601,  ..., -0.4776, -0.4076, -0.3550],
          [-0.5126, -0.4776, -0.4426,  ..., -0.5826, -0.4951, -0.4776],
          [-0.5126, -0.4426, -0.3725,  ..., -0.5476, -0.5651, -0.6001],
          ...,
          [-0.4951, -0.4251, -0.6352,  ..., -0.4426, -0.6001, -0.2325],
          [-0.5126, -0.5126, -0.5826,  ..., -0.5126, -0.5126, -0.2850],
          [-0.4951, -0.5826, -0.4776,  ..., -0.4076, -0.4776, -0.6527]],

         [[-0.9678, -0.9504, -1.0376,  ..., -0.9678, -0.9678, -0.9678],
          [-1.0376, -1.0201, -1.0027,  ..., -0.9504, -0.9678, -1.0027],
          [-1.0201, -0.9678, -0.9330,  ..., -0.8633, -0.9504, -1.0376],
          ...,
          [-0.8807, -0.7936, -1.0201,  ..., -1.0027, -1.0898, -0.7238],
          [-0.8981, -0.8807, -0.9678,  ..., -1.0724, -0.9330, -0.6715],
          [-0.8981, -0.9504, -0.8633,  ..., -0.9504, -0.9504, -1.1073]]],


        [[[ 0.1254,  0.1254,  0.1083,  ...,  0.6049,  0.5878,  0.5707],
          [ 0.1254,  0.0912,  0.1083,  ...,  0.6049,  0.5878,  0.6221],
          [ 0.1426,  0.1083,  0.0912,  ...,  0.6221,  0.6049,  0.6392],
          ...,
          [-0.6109, -0.5938, -0.5938,  ...,  0.5536,  0.5878,  0.5707],
          [-0.6109, -0.5596, -0.5596,  ...,  0.5364,  0.5707,  0.5536],
          [-0.6281, -0.5938, -0.5767,  ...,  0.5707,  0.5536,  0.5536]],

         [[ 0.1001,  0.1001,  0.0826,  ...,  0.6254,  0.5903,  0.5728],
          [ 0.1001,  0.0826,  0.1176,  ...,  0.6254,  0.6078,  0.6429],
          [ 0.1001,  0.1176,  0.1176,  ...,  0.6078,  0.6254,  0.6604],
          ...,
          [-0.6877, -0.6352, -0.5826,  ...,  0.5553,  0.5903,  0.5553],
          [-0.6527, -0.6176, -0.5826,  ...,  0.5378,  0.5728,  0.5728],
          [-0.6527, -0.6352, -0.6176,  ...,  0.5728,  0.5903,  0.5553]],

         [[ 0.0605,  0.0605,  0.0431,  ...,  0.5659,  0.5834,  0.5659],
          [ 0.0605,  0.0431,  0.0605,  ...,  0.5659,  0.5485,  0.5834],
          [ 0.0779,  0.0605,  0.0605,  ...,  0.5659,  0.5311,  0.5834],
          ...,
          [-0.5495, -0.5670, -0.6367,  ...,  0.5311,  0.6008,  0.6182],
          [-0.7238, -0.6715, -0.6367,  ...,  0.5485,  0.5834,  0.6182],
          [-0.7064, -0.6715, -0.6018,  ...,  0.5834,  0.6008,  0.5834]]],


        [[[ 0.2453,  0.1939,  0.1939,  ..., -0.1143, -0.0972, -0.1143],
          [ 0.1768,  0.2111,  0.2111,  ..., -0.0972, -0.0629, -0.1143],
          [ 0.2282,  0.2111,  0.1768,  ..., -0.0458, -0.0629, -0.0801],
          ...,
          [-0.8335, -0.7650, -0.7479,  ..., -0.2171,  0.0227,  0.1083],
          [-0.9192, -0.8335, -0.7822,  ...,  0.5878,  0.5022,  0.1083],
          [-0.8678, -0.8507, -0.8164,  ...,  0.1939,  0.0741, -0.2684]],

         [[-0.1975, -0.3025, -0.2675,  ..., -0.5126, -0.5126, -0.5476],
          [-0.2325, -0.2500, -0.2325,  ..., -0.5126, -0.4776, -0.5301],
          [-0.1800, -0.1975, -0.2150,  ..., -0.4426, -0.4776, -0.5301],
          ...,
          [-0.8277, -0.7402, -0.7402,  ..., -0.4776, -0.3725, -0.2325],
          [-0.9328, -0.7927, -0.6702,  ...,  0.1877, -0.0224, -0.4951],
          [-0.8452, -0.7577, -0.6527,  ..., -0.3901, -0.5126, -0.9503]],

         [[ 0.1302,  0.0779,  0.0431,  ..., -0.3230, -0.3055, -0.3578],
          [ 0.0256,  0.0431,  0.0256,  ..., -0.3404, -0.3055, -0.3578],
          [ 0.0082, -0.0267, -0.0441,  ..., -0.3230, -0.3404, -0.3578],
          ...,
          [-0.6715, -0.6367, -0.7064,  ..., -0.6367, -0.3578, -0.3927],
          [-0.8110, -0.7064, -0.6890,  ..., -0.1487, -0.1835, -0.5844],
          [-0.8110, -0.7587, -0.7413,  ..., -0.5670, -0.5844, -0.8458]]]])
[19:57:25.230338] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:25.247402] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:26.556705] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True, False],
        [ True,  True,  True,  ...,  True,  True,  True],
        [False,  True,  True,  ..., False,  True,  True],
        ...,
        [False,  True, False,  ...,  True,  True,  True],
        [ True,  True, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:26.768467] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.768767] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.769193] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.769650] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.770113] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.770582] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.771056] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.771523] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.771989] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.772458] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.772928] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.773404] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.773865] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.774333] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.774816] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.775292] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.775755] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.776222] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.776698] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:26.837201] INFO: samples:  tensor([[[[ 0.6392,  0.6563,  0.6563,  ...,  0.6392,  0.6221,  0.6221],
          [ 0.6563,  0.6563,  0.6563,  ...,  0.6563,  0.6392,  0.6392],
          [ 0.6563,  0.6734,  0.6734,  ...,  0.6392,  0.6221,  0.6392],
          ...,
          [-0.1143, -0.1143, -0.0972,  ..., -2.0494, -2.0323, -1.9809],
          [-0.0972, -0.0972, -0.0801,  ..., -2.0323, -2.0323, -2.0152],
          [-0.1657, -0.1486, -0.1486,  ..., -2.0323, -2.0152, -1.9980]],

         [[ 0.2052,  0.2227,  0.2227,  ...,  0.1702,  0.1877,  0.1702],
          [ 0.2227,  0.2227,  0.2227,  ...,  0.1877,  0.1702,  0.1877],
          [ 0.2227,  0.2402,  0.2402,  ...,  0.1702,  0.1702,  0.1702],
          ...,
          [-1.3179, -1.3179, -1.3004,  ..., -1.9657, -1.9657, -1.9307],
          [-1.3179, -1.3179, -1.3004,  ..., -1.9657, -1.9657, -1.9482],
          [-1.3179, -1.3179, -1.2829,  ..., -1.9482, -1.9482, -1.9482]],

         [[-0.4798, -0.4624, -0.4624,  ..., -0.6018, -0.6367, -0.6193],
          [-0.4450, -0.4450, -0.4450,  ..., -0.5670, -0.6193, -0.6018],
          [-0.4450, -0.4624, -0.4624,  ..., -0.5844, -0.6193, -0.6018],
          ...,
          [-1.4907, -1.4907, -1.4733,  ..., -1.8044, -1.8044, -1.7870],
          [-1.4907, -1.4907, -1.4733,  ..., -1.8044, -1.8044, -1.8044],
          [-1.4907, -1.4907, -1.4733,  ..., -1.8044, -1.8044, -1.8044]]],


        [[[-1.6727, -1.6898, -1.5014,  ..., -1.3130, -1.4672, -1.2274],
          [-1.7069, -1.6898, -1.6213,  ..., -1.3130, -1.3302, -1.2959],
          [-1.7069, -1.5185, -1.4672,  ..., -1.3130, -1.2274, -1.4843],
          ...,
          [-1.7069, -1.8610, -1.7412,  ..., -1.8610, -1.6384, -1.5870],
          [-1.6384, -1.6213, -1.7412,  ..., -1.8953, -1.5870, -1.7412],
          [-1.6727, -1.6555, -1.7583,  ..., -1.4672, -1.5014, -1.7240]],

         [[-1.6856, -1.7031, -1.4930,  ..., -1.4230, -1.5805, -1.3179],
          [-1.6681, -1.6331, -1.5980,  ..., -1.4230, -1.3529, -1.2829],
          [-1.5805, -1.4405, -1.5805,  ..., -1.3354, -1.2129, -1.4755],
          ...,
          [-1.6155, -1.7381, -1.7556,  ..., -1.7556, -1.5105, -1.4580],
          [-1.6506, -1.4930, -1.6856,  ..., -1.7906, -1.4930, -1.6506],
          [-1.7381, -1.6331, -1.6331,  ..., -1.6856, -1.5630, -1.7381]],

         [[-1.6650, -1.6824, -1.5779,  ..., -1.4384, -1.2816, -1.1944],
          [-1.7696, -1.6824, -1.5256,  ..., -1.5779, -1.5081, -1.4036],
          [-1.8044, -1.6302, -1.6824,  ..., -1.5953, -1.4559, -1.6476],
          ...,
          [-1.8044, -1.6999, -1.5430,  ..., -1.8044, -1.6650, -1.6999],
          [-1.6999, -1.6650, -1.7522,  ..., -1.7173, -1.4210, -1.5430],
          [-1.5953, -1.5953, -1.7696,  ..., -1.4210, -1.4733, -1.7173]]],


        [[[-1.5870, -1.4672, -1.5185,  ..., -1.5528, -1.5699, -1.5014],
          [-1.4672, -1.4329, -1.5528,  ..., -1.6213, -1.7069, -1.6898],
          [-1.5014, -1.4843, -1.5528,  ..., -1.6555, -1.5699, -1.5870],
          ...,
          [-1.4843, -1.2617, -1.2788,  ..., -1.7412, -1.7069, -1.7240],
          [-1.2617, -1.3815, -1.2959,  ..., -1.7069, -1.7240, -1.6555],
          [-1.3130, -1.3987, -1.2788,  ..., -1.6898, -1.7240, -1.7069]],

         [[-1.1954, -1.0553, -1.1253,  ..., -1.2479, -1.3354, -1.3179],
          [-1.0728, -1.0378, -1.1779,  ..., -1.1604, -1.2829, -1.3354],
          [-1.0903, -1.0728, -1.0553,  ..., -1.2129, -1.2829, -1.2654],
          ...,
          [-0.9678, -0.9328, -0.9153,  ..., -1.4755, -1.4405, -1.3880],
          [-0.9678, -0.9503, -0.9503,  ..., -1.4230, -1.4580, -1.3704],
          [-1.0378, -0.9853, -0.9328,  ..., -1.3880, -1.3880, -1.4055]],

         [[-0.0790,  0.0779,  0.0082,  ..., -0.2010, -0.2358, -0.2010],
          [ 0.0779,  0.1128, -0.0267,  ..., -0.1487, -0.2707, -0.2881],
          [ 0.0605,  0.0256, -0.0092,  ..., -0.2358, -0.2010, -0.2184],
          ...,
          [ 0.1302,  0.1302,  0.1476,  ..., -0.3578, -0.3404, -0.3055],
          [ 0.1825,  0.0605,  0.1302,  ..., -0.3404, -0.4101, -0.2707],
          [ 0.1302,  0.0605,  0.1128,  ..., -0.3753, -0.3927, -0.3230]]],


        ...,


        [[[ 0.6734,  0.7762,  0.8618,  ...,  1.1015,  1.0159,  1.0502],
          [ 0.8618,  0.7762,  0.8447,  ...,  1.2385,  1.1358,  0.9474],
          [ 1.3927,  1.1700,  0.8961,  ...,  1.2557,  1.1529,  0.7762],
          ...,
          [ 0.7591,  0.7077,  0.8447,  ...,  0.6392,  0.5536,  0.3652],
          [ 0.8961,  0.6734,  0.7591,  ...,  0.4337,  0.3481,  0.5878],
          [ 0.9817,  0.7419,  0.8104,  ...,  0.4166,  0.2967,  0.3652]],

         [[ 0.7129,  0.7829,  0.8704,  ...,  1.1856,  1.0455,  1.0805],
          [ 0.9055,  0.8354,  0.8704,  ...,  1.2906,  1.1681,  1.0105],
          [ 1.4657,  1.2556,  0.9580,  ...,  1.2381,  1.2206,  0.8529],
          ...,
          [ 0.7829,  0.7304,  0.8880,  ...,  0.7829,  0.7654,  0.6429],
          [ 0.9405,  0.7129,  0.7829,  ...,  0.6429,  0.6779,  0.9580],
          [ 0.9930,  0.7654,  0.8354,  ...,  0.6429,  0.6779,  0.7829]],

         [[ 0.6356,  0.6705,  0.7751,  ...,  1.1062,  0.9842,  0.8797],
          [ 0.9668,  0.8099,  0.8274,  ...,  1.1062,  1.1062,  0.8274],
          [ 1.6465,  1.3328,  0.9319,  ...,  1.0888,  1.1759,  0.7925],
          ...,
          [ 0.6182,  0.5834,  0.7576,  ...,  0.7054,  0.7751,  0.7576],
          [ 0.8099,  0.5834,  0.6356,  ...,  0.6008,  0.7054,  1.0714],
          [ 0.8448,  0.6531,  0.6356,  ...,  0.6182,  0.7751,  0.9668]]],


        [[[ 0.1597,  0.2111,  0.1768,  ...,  0.1426,  0.0912, -0.0116],
          [ 0.2111,  0.1597,  0.1768,  ...,  0.1254,  0.0912,  0.0398],
          [ 0.2111,  0.1768,  0.2453,  ...,  0.0912,  0.1083,  0.0741],
          ...,
          [-1.3987, -1.3302, -1.2788,  ..., -1.9980, -0.9020, -1.6213],
          [-1.3815, -1.3302, -1.3130,  ..., -1.5185, -1.7412, -1.8610],
          [-1.3815, -1.3644, -1.2788,  ..., -1.2788, -1.2959, -1.2103]],

         [[ 0.2927,  0.3277,  0.2927,  ...,  0.3277,  0.2577,  0.2927],
          [ 0.3102,  0.2927,  0.3277,  ...,  0.3102,  0.2577,  0.2752],
          [ 0.2752,  0.2752,  0.3277,  ...,  0.2752,  0.3102,  0.2752],
          ...,
          [-1.2654, -1.2304, -1.1604,  ..., -1.9657, -0.7577, -1.5980],
          [-1.2304, -1.1779, -1.1604,  ..., -1.4755, -1.5980, -1.8081],
          [-1.2304, -1.2129, -1.1253,  ..., -1.1429, -1.1604, -1.0903]],

         [[ 0.2871,  0.2696,  0.2522,  ...,  0.3219,  0.3219,  0.3393],
          [ 0.3045,  0.2696,  0.2696,  ...,  0.3045,  0.3045,  0.3742],
          [ 0.3045,  0.2522,  0.2696,  ...,  0.2348,  0.3219,  0.3742],
          ...,
          [-0.8458, -0.7064, -0.7587,  ..., -1.7173, -0.3055, -0.8284],
          [-0.7413, -0.7064, -0.7064,  ..., -1.0027, -1.2467, -1.1247],
          [-0.7587, -0.7064, -0.6367,  ..., -0.7238, -0.7238, -0.8110]]],


        [[[-0.1486,  0.1254,  0.1768,  ..., -0.0116, -0.0458, -0.1486],
          [ 0.2624,  0.2282,  0.1597,  ..., -0.2513, -0.1314, -0.0287],
          [ 0.2796,  0.6049,  0.5022,  ..., -0.0801, -0.0629, -0.1143],
          ...,
          [ 2.0092,  1.9064,  1.9749,  ...,  2.1290,  2.0263,  1.9407],
          [ 1.9407,  1.9578,  2.0777,  ...,  2.1975,  2.2147,  2.2147],
          [ 2.1633,  2.1633,  2.2489,  ...,  2.1462,  2.1633,  2.1633]],

         [[-0.1800,  0.0826,  0.0826,  ..., -0.0224, -0.1450, -0.3025],
          [ 0.1352,  0.0651,  0.0826,  ..., -0.3025, -0.2500, -0.1099],
          [ 0.2052,  0.4328,  0.3102,  ..., -0.1450, -0.1800, -0.1450],
          ...,
          [ 1.8683,  1.7633,  1.7808,  ...,  2.1660,  2.0784,  2.0259],
          [ 1.8859,  1.9384,  2.0959,  ...,  2.1835,  2.2360,  2.2710],
          [ 2.0784,  2.0609,  2.1134,  ...,  2.0959,  2.1134,  2.1485]],

         [[-0.2707, -0.1835, -0.0615,  ..., -0.2358, -0.4450, -0.4275],
          [ 0.0082, -0.1835, -0.0092,  ..., -0.4101, -0.4624, -0.3753],
          [-0.0441,  0.2348,  0.1476,  ..., -0.4101, -0.3578, -0.4101],
          ...,
          [ 1.8208,  1.7163,  1.7685,  ...,  2.1346,  2.0823,  2.0125],
          [ 1.7511,  1.7860,  1.9951,  ...,  2.1346,  2.2217,  2.2740],
          [ 1.9777,  1.9254,  2.0648,  ...,  2.0300,  2.0997,  2.1868]]]])
[19:57:26.843647] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:26.861928] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:28.168274] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True, False,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True, False,  True]], device='cuda:0')
[19:57:28.379605] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.379875] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.380315] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.380777] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.381247] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.381712] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.382175] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.382645] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.383119] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.383585] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.384049] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.384523] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.384991] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.385457] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.385928] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.386403] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.386873] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.387339] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.387814] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:28.447551] INFO: samples:  tensor([[[[ 1.4954,  1.4269,  1.3755,  ...,  1.1700,  1.1187,  1.0844],
          [ 1.4269,  1.3755,  1.3413,  ...,  1.1700,  1.1015,  1.0844],
          [ 1.4783,  1.3927,  1.3755,  ...,  1.1015,  1.0844,  1.1358],
          ...,
          [-1.5185, -1.4843, -1.2274,  ...,  0.9646,  0.8104,  0.7591],
          [-1.5357, -1.4672, -1.2445,  ...,  0.6906,  0.6563,  0.6906],
          [-1.5185, -1.4500, -1.2617,  ...,  0.6563,  0.5707,  0.6906]],

         [[ 1.3606,  1.3081,  1.2731,  ...,  1.0805,  1.0105,  0.9580],
          [ 1.2906,  1.2556,  1.2906,  ...,  1.0455,  0.9930,  0.9755],
          [ 1.3431,  1.2906,  1.3081,  ...,  0.9405,  0.9405,  1.0105],
          ...,
          [-1.5455, -1.5105, -1.3004,  ...,  0.9405,  0.8179,  0.7829],
          [-1.5630, -1.5105, -1.3179,  ...,  0.6954,  0.7304,  0.7479],
          [-1.5455, -1.4755, -1.3354,  ...,  0.6954,  0.6429,  0.7304]],

         [[ 1.7163,  1.6465,  1.5768,  ...,  1.4200,  1.3502,  1.2980],
          [ 1.6465,  1.5942,  1.5768,  ...,  1.3851,  1.3677,  1.3328],
          [ 1.6814,  1.6291,  1.6117,  ...,  1.2980,  1.3502,  1.3851],
          ...,
          [-1.1944, -1.1596, -0.9330,  ...,  1.0888,  0.9319,  0.8797],
          [-1.2119, -1.1596, -0.9504,  ...,  0.8622,  0.8622,  0.8797],
          [-1.1944, -1.1247, -0.9678,  ...,  0.8448,  0.7925,  0.8971]]],


        [[[ 0.6221,  0.6563,  0.5878,  ...,  1.1358,  1.0844,  1.2043],
          [ 0.6734,  0.5022,  0.1768,  ...,  1.5982,  1.4954,  1.3413],
          [-0.1486, -0.3883, -0.6452,  ...,  1.9578,  1.8893,  2.0434],
          ...,
          [-1.1075, -1.1418, -0.9534,  ..., -0.7650, -0.7822, -0.7993],
          [-0.9192, -1.0562, -0.8507,  ..., -0.7993, -0.8335, -0.7822],
          [-0.9877, -0.9192, -1.0390,  ..., -0.6452, -0.6794, -0.6623]],

         [[ 1.4307,  1.4482,  1.4482,  ...,  2.0259,  1.9384,  2.0609],
          [ 1.5532,  1.4657,  1.3081,  ...,  2.3235,  2.2010,  2.0434],
          [ 1.1506,  0.9230,  0.6954,  ...,  2.3585,  2.4111,  2.4286],
          ...,
          [ 0.2752,  0.3102,  0.3277,  ...,  0.5203,  0.5028,  0.5028],
          [ 0.3277,  0.4153,  0.3627,  ...,  0.5028,  0.5553,  0.5378],
          [ 0.3627,  0.3627,  0.3277,  ...,  0.5203,  0.5203,  0.5028]],

         [[ 1.4025,  1.5071,  1.4374,  ...,  2.1868,  2.0474,  2.0997],
          [ 1.6291,  1.6291,  1.3328,  ...,  2.4483,  2.3437,  2.2914],
          [ 1.0888,  0.7402,  0.5485,  ...,  2.5703,  2.6226,  2.6400],
          ...,
          [ 0.0082,  0.0256,  0.0605,  ...,  0.2522,  0.3045,  0.1999],
          [-0.0092,  0.1128,  0.0605,  ...,  0.2348,  0.2348,  0.1128],
          [ 0.1302,  0.0431,  0.0256,  ...,  0.2348,  0.1302,  0.0953]]],


        [[[ 0.1426, -0.1657, -1.0390,  ...,  0.3652,  0.4166,  0.4679],
          [ 0.1768, -0.1828, -0.7993,  ...,  0.3652,  0.4679,  0.5364],
          [ 0.8618, -0.4911, -0.7650,  ...,  0.3481,  0.4166,  0.5364],
          ...,
          [-1.0048, -1.0219, -0.8678,  ..., -0.7650, -0.5253, -0.7137],
          [-1.0562, -1.0048, -0.7993,  ..., -0.6965, -0.6452, -0.6109],
          [-1.0562, -0.7650, -0.7822,  ..., -0.7993, -0.5767, -0.4226]],

         [[ 0.3102, -0.0049, -0.9328,  ...,  0.3627,  0.4678,  0.5378],
          [ 0.3102, -0.0749, -0.8102,  ...,  0.3627,  0.5028,  0.6078],
          [ 0.9930, -0.4601, -0.8452,  ...,  0.3978,  0.5203,  0.6078],
          ...,
          [-0.8102, -0.8452, -0.7227,  ..., -0.5651, -0.3200, -0.5126],
          [-0.8803, -0.8452, -0.6527,  ..., -0.4951, -0.4426, -0.4076],
          [-0.8803, -0.6001, -0.6702,  ..., -0.6001, -0.3725, -0.2150]],

         [[ 0.4788,  0.1476, -0.7587,  ...,  0.3045,  0.4788,  0.5834],
          [ 0.5659,  0.1999, -0.4798,  ...,  0.3045,  0.5136,  0.6705],
          [ 1.2980, -0.2010, -0.5844,  ...,  0.3219,  0.4439,  0.5834],
          ...,
          [-0.6018, -0.6541, -0.5495,  ..., -0.4101, -0.1661, -0.3578],
          [-0.6193, -0.5670, -0.4101,  ..., -0.3404, -0.2881, -0.2532],
          [-0.6193, -0.3578, -0.3927,  ..., -0.4450, -0.2184, -0.0615]]],


        ...,


        [[[ 1.6153,  1.5810,  1.5639,  ...,  1.5639,  0.6734,  1.0844],
          [ 1.6153,  1.5297,  1.5468,  ...,  1.6495,  1.6495,  0.5022],
          [ 1.5810,  1.5639,  1.4783,  ...,  1.3070,  1.8722,  0.9132],
          ...,
          [-0.1314, -0.2684, -0.3541,  ...,  0.5193,  0.6906,  0.8447],
          [-0.1143, -0.0458, -0.4054,  ...,  0.7077,  0.6049,  0.6563],
          [-0.3369, -0.3027, -0.4054,  ...,  0.6906,  0.9474,  0.2967]],

         [[ 1.7458,  1.7633,  1.7983,  ...,  1.6057,  0.5903,  1.0630],
          [ 1.7808,  1.6758,  1.6933,  ...,  1.8158,  1.7283,  0.6254],
          [ 1.6758,  1.7108,  1.6583,  ...,  1.4657,  1.9384,  1.0455],
          ...,
          [ 0.1176, -0.0399, -0.1625,  ...,  0.5728,  0.7654,  0.8880],
          [ 0.1001,  0.1702, -0.2150,  ...,  0.7479,  0.6604,  0.7129],
          [-0.1275, -0.0924, -0.2325,  ...,  0.6954,  0.9930,  0.3452]],

         [[ 2.0125,  1.9603,  1.9951,  ...,  1.8034,  0.9145,  1.1062],
          [ 1.9951,  1.8731,  1.8905,  ...,  1.8905,  1.9428,  0.8448],
          [ 1.8905,  1.8905,  1.8731,  ...,  1.6291,  2.1346,  1.3154],
          ...,
          [ 0.1999,  0.0605, -0.0615,  ...,  0.7228,  0.8971,  1.0539],
          [ 0.2173,  0.2871, -0.1138,  ...,  0.8448,  0.7751,  0.8622],
          [-0.0092,  0.0256, -0.0964,  ...,  0.7402,  1.0888,  0.5136]]],


        [[[-0.3027, -0.5253, -0.7137,  ..., -1.3302, -1.3302, -1.3302],
          [-0.3541, -0.6109, -0.7479,  ..., -1.2959, -1.3130, -1.2788],
          [-0.3883, -0.6109, -0.7822,  ..., -1.2445, -1.2788, -1.2617],
          ...,
          [-0.1486, -0.1143, -0.1657,  ..., -0.6281, -1.2103, -1.6555],
          [ 0.0227,  0.1083,  0.0912,  ..., -0.9192, -1.3987, -1.7240],
          [ 0.0569,  0.1768,  0.2111,  ..., -0.8507, -1.0733, -1.2274]],

         [[ 0.7654,  0.6078,  0.4328,  ..., -0.4601, -0.3725, -0.2850],
          [ 0.7129,  0.5553,  0.3803,  ..., -0.3901, -0.3025, -0.2325],
          [ 0.6954,  0.5553,  0.3627,  ..., -0.2850, -0.2325, -0.1625],
          ...,
          [ 0.5028,  0.5378,  0.5728,  ...,  0.2052, -0.4426, -0.9328],
          [ 0.6078,  0.7304,  0.7654,  ..., -0.2150, -0.7402, -1.0203],
          [ 0.5728,  0.7304,  0.8179,  ..., -0.2850, -0.5651, -0.7402]],

         [[ 0.1302, -0.2010, -0.2532,  ..., -1.4559, -1.4907, -1.5953],
          [ 0.0431, -0.2707, -0.2532,  ..., -1.4733, -1.5256, -1.6127],
          [-0.0441, -0.2707, -0.2707,  ..., -1.4733, -1.5256, -1.6302],
          ...,
          [-0.6018, -0.5495, -0.5495,  ..., -0.6367, -1.0550, -1.4559],
          [-0.5147, -0.3578, -0.3753,  ..., -0.8981, -1.3164, -1.5604],
          [-0.5147, -0.3404, -0.3055,  ..., -0.8458, -1.1073, -1.2816]]],


        [[[-1.7925, -1.2959, -0.7479,  ..., -0.7308, -0.7308, -0.7137],
          [-1.3815, -0.8678, -0.4568,  ..., -0.7137, -0.7137, -0.7137],
          [-1.3644, -1.0048, -0.6965,  ..., -0.7137, -0.7137, -0.7137],
          ...,
          [-2.0665, -2.0837, -2.0323,  ...,  1.5125,  1.6667,  1.7009],
          [-2.0665, -2.0665, -2.0152,  ...,  1.4269,  1.6495,  1.7352],
          [-2.0665, -2.0494, -2.0323,  ...,  1.3070,  1.5810,  1.7352]],

         [[-1.6331, -1.0903, -0.4776,  ..., -1.4055, -1.4055, -1.3880],
          [-1.2129, -0.6352, -0.1975,  ..., -1.3880, -1.3880, -1.3880],
          [-1.1954, -0.8277, -0.5301,  ..., -1.3880, -1.3880, -1.3880],
          ...,
          [-2.0182, -2.0182, -2.0182,  ...,  1.7633,  1.9559,  1.9909],
          [-2.0182, -2.0007, -2.0007,  ...,  1.7283,  1.9209,  2.0084],
          [-2.0182, -2.0182, -2.0182,  ...,  1.5532,  1.8158,  1.9384]],

         [[-1.0376, -0.3578,  0.1999,  ..., -1.6999, -1.7173, -1.6999],
          [-0.7064, -0.1487,  0.2871,  ..., -1.6999, -1.6999, -1.6999],
          [-0.7413, -0.4275,  0.0082,  ..., -1.6999, -1.6999, -1.6999],
          ...,
          [-1.7870, -1.8044, -1.8044,  ...,  2.3088,  2.3960,  2.5354],
          [-1.7870, -1.8044, -1.7870,  ...,  2.1868,  2.5006,  2.5180],
          [-1.7870, -1.8044, -1.8044,  ...,  2.0823,  2.2914,  2.4308]]]])
[19:57:28.453616] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:28.469953] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:29.779289] INFO: mask:  tensor([[False,  True,  True,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [False, False,  True,  ...,  True,  True,  True],
        ...,
        [ True, False,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True, False],
        [False,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:29.990821] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.991092] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.991515] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.991985] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.992448] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.992912] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.993378] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.993846] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.994314] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.994791] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.995257] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.995729] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.996193] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.996661] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.997129] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.997597] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.998061] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.998527] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:29.999003] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:30.057178] INFO: samples:  tensor([[[[ 1.1015,  1.5297,  1.7523,  ...,  2.2489,  1.8550, -0.8849],
          [ 0.8789,  0.9303,  1.0502,  ...,  2.2489,  2.1804,  1.7009],
          [ 0.7419,  1.3070,  2.0092,  ...,  2.1975,  2.1633,  2.2489],
          ...,
          [ 1.9578,  1.9578,  1.9578,  ...,  1.6324,  1.6324,  1.6324],
          [ 1.9578,  1.9578,  1.9578,  ...,  1.6324,  1.6324,  1.6324],
          [ 1.9578,  1.9578,  1.9578,  ...,  1.6324,  1.6324,  1.6324]],

         [[ 1.2031,  1.6408,  1.8508,  ...,  2.4286,  1.9909, -0.9328],
          [ 0.9580,  1.0105,  1.1331,  ...,  2.3761,  2.3410,  1.7808],
          [ 0.8704,  1.4482,  2.1485,  ...,  2.3410,  2.4286,  2.3761],
          ...,
          [ 2.0959,  2.0959,  2.0959,  ...,  1.7983,  1.7983,  1.7983],
          [ 2.0959,  2.0959,  2.0959,  ...,  1.7983,  1.7983,  1.7983],
          [ 2.0959,  2.0959,  2.0959,  ...,  1.7983,  1.7983,  1.7983]],

         [[ 1.3328,  1.7860,  2.0474,  ...,  2.6400,  2.1868, -0.6890],
          [ 1.1237,  1.2108,  1.3677,  ...,  2.5354,  2.5180,  1.9428],
          [ 1.0191,  1.6291,  2.3786,  ...,  2.4831,  2.5354,  2.6400],
          ...,
          [ 2.3263,  2.3263,  2.3263,  ...,  2.0125,  2.0125,  2.0125],
          [ 2.3263,  2.3263,  2.3263,  ...,  2.0125,  2.0125,  2.0125],
          [ 2.3263,  2.3263,  2.3263,  ...,  2.0125,  2.0125,  2.0125]]],


        [[[-0.9192, -1.0048, -1.1760,  ..., -1.2617, -0.8507, -0.7137],
          [-1.2959, -1.1760, -1.1760,  ..., -1.1418, -0.7993, -0.7822],
          [-1.2274, -1.3130, -1.1589,  ..., -1.1589, -0.7308, -0.6452],
          ...,
          [-0.5596, -0.5938, -0.6794,  ..., -1.0904, -1.4329, -1.4500],
          [-0.5424, -0.6281, -0.6452,  ..., -0.1828, -0.2684, -0.6965],
          [-0.5596, -0.6452, -0.7137,  ..., -0.4397, -0.3541, -0.2342]],

         [[-0.4601, -0.6001, -0.6877,  ..., -1.4930, -1.2304, -1.1429],
          [-0.8102, -0.7927, -0.7577,  ..., -1.4580, -1.1954, -1.1954],
          [-0.7927, -0.9153, -0.7402,  ..., -1.5455, -1.1429, -1.0378],
          ...,
          [-0.5476, -0.5826, -0.6176,  ..., -1.1078, -1.5280, -1.5805],
          [-0.4951, -0.5651, -0.5826,  ..., -0.0749, -0.1800, -0.6702],
          [-0.4601, -0.5476, -0.6176,  ..., -0.3200, -0.2325, -0.0924]],

         [[-0.6715, -0.7761, -0.9156,  ..., -1.3861, -1.3513, -1.4036],
          [-1.0376, -0.9678, -0.9330,  ..., -1.2467, -1.2641, -1.4907],
          [-1.0027, -1.1596, -0.9330,  ..., -1.2641, -1.2467, -1.2816],
          ...,
          [-0.1487, -0.1835, -0.2010,  ..., -0.8807, -1.2816, -1.3339],
          [-0.0964, -0.1661, -0.1661,  ...,  0.1128,  0.0082, -0.4798],
          [-0.0964, -0.1835, -0.2184,  ..., -0.1312, -0.0441,  0.0605]]],


        [[[-2.0323, -2.1008, -2.1179,  ..., -1.8610, -1.7583, -1.7583],
          [-2.0837, -2.1179, -2.1179,  ..., -1.8610, -1.7583, -1.7925],
          [-2.1008, -2.1008, -2.1179,  ..., -1.9295, -1.7925, -1.8782],
          ...,
          [-1.7412, -1.7240, -1.7069,  ..., -1.2103, -1.2274, -1.2103],
          [-1.7583, -1.7754, -1.7240,  ..., -1.1932, -1.2103, -1.2274],
          [-1.7583, -1.7583, -1.7412,  ..., -1.1932, -1.2274, -1.2103]],

         [[-1.9307, -1.9832, -1.9832,  ..., -1.7031, -1.6506, -1.6506],
          [-1.9832, -2.0007, -1.9832,  ..., -1.7031, -1.6506, -1.6331],
          [-2.0007, -1.9832, -2.0007,  ..., -1.7731, -1.6856, -1.6681],
          ...,
          [-1.6506, -1.6506, -1.6331,  ..., -1.0903, -1.1253, -1.1253],
          [-1.6506, -1.6681, -1.6331,  ..., -1.1253, -1.1429, -1.1604],
          [-1.6506, -1.6681, -1.6506,  ..., -1.1253, -1.1779, -1.1779]],

         [[-1.5604, -1.5953, -1.6302,  ..., -1.3339, -1.6127, -1.6302],
          [-1.5604, -1.5779, -1.6127,  ..., -1.3513, -1.6476, -1.6302],
          [-1.5953, -1.5430, -1.6127,  ..., -1.4036, -1.6476, -1.6476],
          ...,
          [-1.6302, -1.6127, -1.5430,  ..., -1.1596, -1.2293, -1.2119],
          [-1.5604, -1.5779, -1.5779,  ..., -1.1421, -1.1770, -1.2641],
          [-1.6302, -1.6127, -1.5953,  ..., -1.1247, -1.1596, -1.1770]]],


        ...,


        [[[ 1.5639,  1.6495,  1.6495,  ...,  1.1872,  1.0502,  0.9132],
          [ 1.7009,  1.7865,  1.7180,  ...,  1.0331,  0.9303,  0.8618],
          [ 1.7865,  1.8037,  1.8550,  ...,  0.9303,  0.7077,  0.5707],
          ...,
          [ 0.3823,  0.5364,  0.6392,  ..., -1.2103, -1.1589, -1.1932],
          [ 0.3138,  0.2796,  0.4166,  ..., -1.3302, -1.3130, -1.2274],
          [ 0.2624,  0.3823,  0.4679,  ..., -1.0904, -1.0219, -0.9192]],

         [[ 1.6933,  1.6583,  1.6933,  ...,  0.4678,  0.3452,  0.2752],
          [ 1.7458,  1.7808,  1.8333,  ...,  0.3452,  0.2052,  0.1176],
          [ 1.7983,  1.8508,  1.9209,  ...,  0.2052,  0.2227,  0.1527],
          ...,
          [ 0.1877,  0.2927,  0.5028,  ..., -1.0378, -1.0728, -1.1253],
          [ 0.0126,  0.2227,  0.3803,  ..., -1.0553, -1.0553, -1.1078],
          [-0.0924, -0.0049,  0.2052,  ..., -1.0378, -1.0203, -1.0028]],

         [[ 1.9951,  2.0300,  2.0823,  ..., -0.1138, -0.3404, -0.4624],
          [ 2.0823,  2.1346,  2.1868,  ..., -0.1835, -0.3753, -0.4798],
          [ 2.0997,  2.1346,  2.2043,  ..., -0.3230, -0.4450, -0.4973],
          ...,
          [ 0.6182,  0.7402,  0.8448,  ..., -0.8110, -0.7936, -0.8110],
          [ 0.3045,  0.4091,  0.5834,  ..., -0.8284, -0.8110, -0.8110],
          [ 0.1128,  0.2522,  0.4439,  ..., -0.7761, -0.7064, -0.6367]]],


        [[[ 0.6049,  0.0227,  0.2282,  ..., -1.8953, -1.8782, -1.8782],
          [ 0.1939,  0.3138,  0.1254,  ..., -1.9124, -1.9124, -1.8782],
          [ 0.2282,  0.1254,  0.2624,  ..., -1.8610, -1.8953, -1.9124],
          ...,
          [-0.7650, -0.6109, -0.6623,  ...,  0.6563,  0.7591,  0.7933],
          [-0.6109, -0.6109, -0.6623,  ...,  0.6563,  0.8447,  0.8104],
          [-0.5767, -0.6794, -0.7137,  ...,  0.7077,  0.8276,  0.8276]],

         [[ 0.2577, -0.3200, -0.1099,  ..., -1.7556, -1.7731, -1.7906],
          [-0.1450, -0.0224, -0.1975,  ..., -1.7731, -1.7731, -1.7906],
          [-0.1975, -0.3025, -0.1625,  ..., -1.7906, -1.8256, -1.8256],
          ...,
          [-0.6352, -0.4951, -0.5476,  ...,  0.8880,  1.0105,  1.0455],
          [-0.4951, -0.4951, -0.5476,  ...,  0.7829,  0.9930,  0.9580],
          [-0.4601, -0.5651, -0.6001,  ...,  0.9230,  1.0455,  1.0805]],

         [[ 0.0605, -0.6367, -0.4450,  ..., -1.4036, -1.4559, -1.4210],
          [-0.3404, -0.1661, -0.3927,  ..., -1.4210, -1.4559, -1.4210],
          [-0.4101, -0.5147, -0.3753,  ..., -1.4210, -1.4559, -1.4559],
          ...,
          [-0.4798, -0.3055, -0.3578,  ...,  1.1759,  1.2457,  1.2631],
          [-0.3055, -0.3055, -0.3230,  ...,  1.0888,  1.2457,  1.2108],
          [-0.2707, -0.3753, -0.4101,  ...,  1.1411,  1.2457,  1.2631]]],


        [[[-1.9124, -1.9295, -1.9980,  ...,  0.1254,  0.3481, -0.4911],
          [-1.9467, -2.0323, -2.0665,  ..., -0.5424, -1.1247, -0.8335],
          [-2.0152, -2.0152, -2.0323,  ..., -0.0801,  0.1426, -0.2171],
          ...,
          [ 0.8447,  0.0912,  0.3823,  ..., -1.7240, -1.8439, -0.0972],
          [ 1.4440,  1.0673,  0.4679,  ..., -1.5185, -1.5185, -0.1143],
          [ 0.2796,  0.2282,  0.1083,  ..., -1.6555, -1.3302, -0.1143]],

         [[-2.0007, -1.9307, -2.0182,  ...,  0.1352,  0.1702, -1.1253],
          [-1.9482, -2.0182, -2.0007,  ..., -0.8978, -1.7031, -1.6506],
          [-2.0007, -1.9657, -1.9832,  ..., -0.0749,  0.0301, -0.7402],
          ...,
          [ 0.1877, -0.5476,  0.1877,  ..., -1.6155, -1.6331,  0.0476],
          [ 1.0280,  0.7479,  0.2577,  ..., -1.5280, -1.4755,  0.0826],
          [-0.1099,  0.0826, -0.1099,  ..., -1.3704, -1.1954, -0.0399]],

         [[-1.8044, -1.7522, -1.8044,  ...,  0.3045,  0.1128, -1.2990],
          [-1.7696, -1.7870, -1.7870,  ..., -0.6018, -1.4036, -1.6650],
          [-1.7696, -1.7173, -1.7347,  ...,  0.1825,  0.3742, -0.2881],
          ...,
          [-1.0550, -1.1596, -0.1661,  ..., -1.3339, -1.4733, -0.2184],
          [ 0.0605,  0.0431, -0.2707,  ..., -1.3687, -1.3513, -0.0267],
          [-0.4450, -0.2707, -0.5321,  ..., -1.2816, -1.2990, -0.2358]]]])
[19:57:30.063727] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:30.080499] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:31.388975] INFO: mask:  tensor([[ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        ...,
        [ True,  True,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True],
        [False, False,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:31.600432] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.600698] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.601119] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.601583] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.602045] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.602509] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.602985] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.603451] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.603922] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.604385] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.604854] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.605330] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.605796] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.606263] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.606745] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.607209] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.607669] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.608138] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.608609] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:31.666880] INFO: samples:  tensor([[[[-1.6042, -1.4843, -1.6384,  ..., -1.3473, -1.4843, -1.5185],
          [-1.5185, -1.5357, -1.5870,  ..., -1.2959, -1.4500, -1.7412],
          [-1.5357, -1.6898, -1.3987,  ..., -1.3302, -1.4329, -1.4500],
          ...,
          [-0.4054, -0.0972, -0.7479,  ..., -0.4397,  0.1939, -0.3541],
          [-0.0116, -0.4568, -1.3644,  ..., -0.5253, -0.2856, -0.2513],
          [-0.7479, -0.8335, -1.0219,  ...,  0.5364, -0.0972, -1.0048]],

         [[-1.2304, -1.3004, -1.3529,  ..., -1.0903, -1.2304, -1.2479],
          [-1.0903, -1.1253, -1.2654,  ..., -0.9153, -1.0028, -1.4230],
          [-1.1779, -1.2479, -1.1429,  ..., -1.1253, -1.1253, -1.0203],
          ...,
          [ 0.0476,  0.5378, -0.0049,  ..., -0.1450,  0.5203,  0.0476],
          [ 0.6429, -0.0749, -1.3704,  ..., -0.2675,  0.3102,  0.2927],
          [-0.4076, -0.3200, -0.6001,  ...,  0.5728, -0.2500, -0.2150]],

         [[-1.1944, -1.2119, -1.4036,  ..., -1.2990, -1.3687, -1.4559],
          [-1.2641, -1.3687, -1.4210,  ..., -1.1770, -1.4384, -1.5430],
          [-1.2816, -1.4733, -1.2293,  ..., -1.0027, -1.3339, -1.3687],
          ...,
          [-0.8458, -0.3230, -0.8633,  ..., -0.7413, -0.0790, -0.7936],
          [ 0.1128, -0.3927, -1.2990,  ..., -0.9330, -0.4973, -0.2358],
          [-0.7064, -1.0027, -1.1770,  ...,  0.0256, -0.5495, -0.9504]]],


        [[[-0.9705, -1.0562, -1.1075,  ..., -1.9809, -1.9467, -1.9638],
          [-0.9705, -1.0390, -1.0390,  ..., -2.0152, -1.9980, -1.9809],
          [-0.9020, -0.9020, -0.9363,  ..., -2.0323, -2.0494, -2.0494],
          ...,
          [-0.8507, -0.8849, -0.8678,  ..., -0.8507, -0.8678, -0.8849],
          [-0.8507, -0.8507, -0.8678,  ..., -0.7993, -0.8164, -0.7993],
          [-0.8335, -0.8507, -0.8849,  ..., -0.6623, -0.7308, -0.7137]],

         [[-0.8102, -0.8627, -0.8627,  ..., -1.9657, -1.9307, -1.9307],
          [-0.9153, -0.9503, -0.8627,  ..., -2.0007, -1.9657, -1.9307],
          [-0.9503, -0.9153, -0.8102,  ..., -2.0007, -1.9832, -1.9657],
          ...,
          [-0.8978, -0.9328, -0.9153,  ..., -0.9328, -0.9503, -0.9678],
          [-0.8452, -0.8452, -0.8452,  ..., -0.8978, -0.9153, -0.8978],
          [-0.8102, -0.8277, -0.8627,  ..., -0.8102, -0.8627, -0.8452]],

         [[-1.1073, -1.2641, -1.2990,  ..., -1.7173, -1.6824, -1.6824],
          [-0.9853, -1.0724, -1.0724,  ..., -1.7522, -1.7173, -1.6824],
          [-0.9156, -0.9853, -1.0376,  ..., -1.7522, -1.7522, -1.7347],
          ...,
          [-0.7064, -0.7413, -0.6715,  ..., -0.7936, -0.8110, -0.8110],
          [-0.6890, -0.6890, -0.6367,  ..., -0.7587, -0.7587, -0.7413],
          [-0.6890, -0.6890, -0.6715,  ..., -0.5844, -0.6367, -0.6367]]],


        [[[-0.4739, -0.4226, -0.3198,  ...,  0.1939,  0.1597,  0.1768],
          [-0.4397, -0.3541, -0.3541,  ...,  0.1083,  0.2282,  0.2453],
          [-0.5767, -0.4739, -0.2856,  ...,  0.1768,  0.0912,  0.0569],
          ...,
          [ 1.8550,  1.9064,  2.0092,  ...,  1.5810,  0.8618,  0.7248],
          [ 1.7352,  1.9749,  2.0605,  ...,  1.3070,  0.5536,  0.5364],
          [ 1.9578,  2.1975,  2.1975,  ...,  0.7933,  0.7248,  0.8104]],

         [[-0.6001, -0.5651, -0.4601,  ...,  0.0826,  0.0476,  0.0826],
          [-0.5826, -0.5126, -0.5126,  ...,  0.0126,  0.1352,  0.1527],
          [-0.7227, -0.6176, -0.4251,  ...,  0.0826, -0.0049, -0.0399],
          ...,
          [ 1.9909,  2.0259,  2.1134,  ...,  1.6933,  0.9930,  0.8354],
          [ 1.9384,  2.1660,  2.2185,  ...,  1.3957,  0.6429,  0.6078],
          [ 2.1310,  2.4111,  2.3936,  ...,  0.8704,  0.8354,  0.9055]],

         [[-0.9853, -0.9504, -0.8633,  ..., -0.5321, -0.5844, -0.6018],
          [-1.0027, -0.9156, -0.9156,  ..., -0.6367, -0.5670, -0.5321],
          [-1.1247, -1.0027, -0.8110,  ..., -0.6018, -0.6890, -0.7413],
          ...,
          [ 1.6640,  1.7163,  1.8034,  ...,  1.0714,  0.3045,  0.1128],
          [ 1.5942,  1.9428,  2.0648,  ...,  0.8797,  0.1651,  0.1825],
          [ 1.8383,  2.2391,  2.3263,  ...,  0.3916,  0.3916,  0.5311]]],


        ...,


        [[[-1.6555, -1.6213, -1.6213,  ..., -1.7412, -1.7240, -1.7412],
          [-1.6727, -1.6384, -1.6213,  ..., -1.7583, -1.7583, -1.7925],
          [-1.6555, -1.6213, -1.5699,  ..., -1.7925, -1.7754, -1.7412],
          ...,
          [ 2.2147,  2.2318,  2.2489,  ..., -0.9192, -0.9363, -0.9192],
          [ 2.2318,  2.2489,  2.2318,  ..., -0.9020, -0.9192, -0.9192],
          [ 2.2318,  2.2318,  2.2147,  ..., -0.9705, -0.9877, -0.9877]],

         [[-1.5280, -1.4930, -1.4930,  ..., -1.6506, -1.6331, -1.6155],
          [-1.5455, -1.5105, -1.4930,  ..., -1.6681, -1.6681, -1.6506],
          [-1.5280, -1.4930, -1.4405,  ..., -1.7031, -1.6856, -1.6155],
          ...,
          [ 2.4286,  2.4111,  2.4286,  ..., -0.2500, -0.2500, -0.2150],
          [ 2.4286,  2.4286,  2.4111,  ..., -0.2500, -0.2500, -0.2150],
          [ 2.4111,  2.4111,  2.3936,  ..., -0.3725, -0.3725, -0.3550]],

         [[-1.3339, -1.3164, -1.3164,  ..., -1.4384, -1.4036, -1.4210],
          [-1.3687, -1.3339, -1.3164,  ..., -1.4559, -1.4384, -1.4559],
          [-1.3513, -1.3164, -1.2641,  ..., -1.4907, -1.4559, -1.4036],
          ...,
          [ 2.6400,  2.6226,  2.6400,  ...,  0.2348,  0.2348,  0.2522],
          [ 2.6400,  2.6400,  2.6226,  ..., -0.0790, -0.0790, -0.0615],
          [ 2.6226,  2.6226,  2.6051,  ..., -0.2358, -0.2532, -0.2358]]],


        [[[ 0.8789,  0.8789,  0.8789,  ...,  0.7248,  0.7248,  0.7248],
          [ 0.8789,  0.8789,  0.8789,  ...,  0.7248,  0.7248,  0.7248],
          [ 0.8961,  0.8961,  0.8961,  ...,  0.7762,  0.7762,  0.7762],
          ...,
          [-0.8849, -1.1932, -1.5357,  ..., -0.2171, -0.1657, -0.2856],
          [-1.3815, -1.5699, -1.3987,  ..., -0.2684, -0.2171, -0.2513],
          [-1.5014, -1.5185, -1.5185,  ..., -0.2856, -0.1999, -0.1143]],

         [[ 1.7108,  1.7108,  1.7108,  ...,  1.5707,  1.5707,  1.5707],
          [ 1.7108,  1.7108,  1.7108,  ...,  1.5707,  1.5707,  1.5707],
          [ 1.7458,  1.7458,  1.7458,  ...,  1.6057,  1.6057,  1.6057],
          ...,
          [-0.9853, -1.2654, -1.6155,  ...,  0.2402,  0.3102,  0.1877],
          [-1.5105, -1.6155, -1.3880,  ...,  0.2927,  0.3452,  0.2927],
          [-1.5805, -1.5455, -1.5105,  ...,  0.2577,  0.3452,  0.4328]],

         [[ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6400,  2.6400,  2.6400,  ...,  2.6400,  2.6400,  2.6400],
          [ 2.6226,  2.6226,  2.6226,  ...,  2.6226,  2.6226,  2.6226],
          ...,
          [-0.8110, -1.0724, -1.4210,  ...,  0.8622,  0.9319,  0.8099],
          [-1.2641, -1.4210, -1.2119,  ...,  0.9668,  1.0191,  0.9668],
          [-1.3687, -1.3513, -1.3339,  ...,  0.9668,  1.0539,  1.1237]]],


        [[[-1.1418, -1.9467, -0.4226,  ..., -0.4054, -1.8439, -1.4843],
          [-1.8610, -0.9705,  0.6392,  ...,  0.4851, -1.6727, -1.6727],
          [-1.1932,  0.5707,  1.0673,  ...,  0.9988, -0.2856, -1.8439],
          ...,
          [-0.7479,  0.5878,  0.8961,  ..., -0.5082, -0.9705, -1.1760],
          [ 1.1187,  1.2385,  0.8789,  ..., -1.2617, -1.5185, -1.5185],
          [ 1.0331,  0.8276,  0.8789,  ..., -1.2959, -1.5014, -1.4158]],

         [[-1.1954, -1.6856,  0.5378,  ...,  0.7829, -1.1078, -0.9678],
          [-1.7556, -0.1800,  1.8859,  ...,  1.3431, -0.6702, -0.9503],
          [-0.5301,  1.5182,  1.5182,  ...,  1.4657,  0.9930, -0.9153],
          ...,
          [-0.9503,  0.1527,  0.5553,  ...,  0.0651, -0.4776, -0.6352],
          [ 1.3431,  1.6408,  1.1681,  ..., -0.7052, -1.0378, -1.0728],
          [ 1.0280,  0.8704,  1.3081,  ..., -0.7227, -0.9853, -1.0203]],

         [[-1.0724, -1.5604,  0.6356,  ...,  0.3568, -1.7347, -1.5953],
          [-1.5604, -0.0790,  2.0823,  ...,  1.5071, -1.1596, -1.5779],
          [-0.4624,  1.7163,  2.0823,  ...,  2.1868,  0.6356, -1.4559],
          ...,
          [-1.1073, -0.7064, -0.7238,  ..., -1.0201, -1.0724, -1.4559],
          [ 0.0431, -0.0267, -0.1487,  ..., -1.3861, -1.6476, -1.5953],
          [-0.2358, -0.3753, -0.3753,  ..., -1.1944, -1.7347, -1.4733]]]])
[19:57:31.673253] INFO: samples shape:  torch.Size([128, 3, 224, 224]) tensor(-2.1179, device='cuda:0') tensor(2.6400, device='cuda:0')
[19:57:31.690062] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[19:57:33.000459] INFO: mask:  tensor([[False,  True,  True,  ...,  True,  True, False],
        [ True, False,  True,  ...,  True,  True,  True],
        [ True,  True,  True,  ..., False,  True,  True],
        ...,
        [False,  True,  True,  ...,  True,  True,  True],
        [ True, False, False,  ...,  True,  True,  True],
        [ True,  True,  True,  ...,  True,  True,  True]], device='cuda:0')
[19:57:33.211888] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.212154] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.212580] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.213043] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.213510] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.213976] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.214446] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.216376] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.216825] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.217292] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.217756] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.218227] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.218691] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.219160] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.219630] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.220099] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.220564] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.221031] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
[19:57:33.221505] INFO: anchor.shape, rm_pred.shape, mask.shape:  torch.Size([128, 196, 768]) torch.Size([128, 196, 768]) torch.Size([128, 196])
WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448668 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448669 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448670 closing signal SIGINT
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448671 closing signal SIGINT
Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7ff1cce27430>
Traceback (most recent call last):
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1510, in __del__
    self._shutdown_workers()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1455, in _shutdown_workers
    self._worker_result_queue.put((None, None))
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/multiprocessing/queues.py", line 88, in put
    self._start_thread()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/multiprocessing/queues.py", line 173, in _start_thread
    self._thread.start()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/threading.py", line 857, in start
    self._started.wait()
KeyboardInterrupt: 
Traceback (most recent call last):
  File "main_ltrp.py", line 328, in <module>
    main(args)
  File "main_ltrp.py", line 286, in main
    train_stats = train_one_epoch(
  File "/home/s2/youngjoonjeong/github/ltrp/engine_pretrain.py", line 104, in train_one_epoch
    loss_scaler(loss, optimizer, parameters=model.parameters(),
  File "/home/s2/youngjoonjeong/github/ltrp/utils/misc.py", line 266, in __call__
    norm = get_grad_norm_(parameters)
  File "/home/s2/youngjoonjeong/github/ltrp/utils/misc.py", line 291, in get_grad_norm_
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
  File "/home/s2/youngjoonjeong/github/ltrp/utils/misc.py", line 291, in <listcomp>
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/functional.py", line 1451, in norm
    return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]
KeyboardInterrupt
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/multiprocessing/popen_fork.py", line 27, in poll
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448668 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448669 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448670 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 448671 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 448629 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 716, in run
    self._shutdown(e.sigval)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 193, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 330, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 707, in _close
    handler.proc.wait(time_to_wait)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/subprocess.py", line 1816, in _wait
    time.sleep(delay)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 448629 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/api.py", line 721, in run
    self._shutdown()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 193, in _shutdown
    self._pcontext.close(death_sig)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 330, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 707, in _close
    handler.proc.wait(time_to_wait)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/subprocess.py", line 1083, in wait
    return self._wait(timeout=timeout)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/subprocess.py", line 1816, in _wait
    time.sleep(delay)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 60, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 448629 got signal: 2
