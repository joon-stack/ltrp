/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24])
tensor(310.7826)
tensor(310.7826)tensor(310.7826)

tensor(310.7826)
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 0): env://, gpu 0
[20:03:52.718676] job dir: /home/s2/youngjoonjeong/github/ltrp
[20:03:52.718889] Namespace(accum_iter=1,
asymmetric=True,
batch_size=128,
blr=0.0003,
burning_in=0,
data_path='/shared/s2/lab01/dataset/imagenet/images',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=400,
focused_rank_k=10,
gpu=0,
img_metric='',
input_size=224,
list_mle_k=20,
local_rank=0,
log_dir=None,
low_shot='',
lr=None,
ltr_loss='list_mleEx',
mask_all=False,
mask_ratio=0.9,
min_lr=0.0,
model='ltrp_dinowm_and_vs',
model_ckpt='/shared/s2/lab01/youngjoonjeong/dino_wm_oc/outputs/wall_dinovits_full_nope/checkpoints/model_latest.pth',
norm_pix_loss=False,
num_workers=20,
output_dir='/shared/s2/lab01/youngjoonjeong/ltrp_test/output_dir',
pin_mem=True,
pretrained_from='',
rank=0,
rank_net_sigma=1,
rank_net_t=0.001,
resume='',
resume_from_mae='',
resume_score_net='',
save_ckpt_freq=20,
score_net='vit_small',
score_net_depth=12,
seed=0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.06,
world_size=4)
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
[20:03:52.829809] Loading wall dataset from self.data_path
[20:03:52.834157] [[42, 17, 30, 34, 20, 25, 0, 8, 36, 18, 28, 39, 3, 2, 41, 1, 37, 6, 14, 12, 11, 32, 43, 16, 5, 38, 31, 44, 26, 45, 15, 47, 4, 10, 21, 19, 33, 27, 13, 48, 46, 23, 22, 35, 7], [24, 49, 29, 40, 9]]
[20:03:52.835935] INFO: dataset_train len:  1845
[20:03:52.835961] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f3aa4834970>
[20:03:52.835974] Sampler len: 462
[20:03:52.836104] Sampler sample indices (first 10): [944, 1629, 102, 210, 1102, 15, 752, 1473, 134, 1414]
[20:03:52.836395] INFO: len(data_loader_train) = 4
[20:03:52.851109] INFO: snapshot_path:  /shared/s2/lab01/youngjoonjeong/dino_wm_oc/outputs/wall_dinovits_full_nope/checkpoints/model_latest.pth
Using cache found in /home/s2/youngjoonjeong/.cache/torch/hub/facebookresearch_dino_main
Using cache found in /home/s2/youngjoonjeong/.cache/torch/hub/facebookresearch_dino_main
Using cache found in /home/s2/youngjoonjeong/.cache/torch/hub/facebookresearch_dino_main
[20:03:53.674191] Checkpoint keys: dict_keys(['epoch', 'global_step', 'predictor', 'predictor_optimizer', 'action_encoder', 'proprio_encoder'])
[20:03:53.675184] Resuming from epoch 100: /shared/s2/lab01/youngjoonjeong/dino_wm_oc/outputs/wall_dinovits_full_nope/checkpoints/model_latest.pth
[20:03:53.675395] INFO: train_cfg.encoder:  {'_target_': 'models.dino.DinoEncoder', 'name': 'dino_vits16', 'feature_key': 'x_norm_patchtokens'}
Using cache found in /home/s2/youngjoonjeong/.cache/torch/hub/facebookresearch_dino_main
[20:03:54.041142] INFO: result.keys():  dict_keys(['predictor', 'action_encoder', 'proprio_encoder', 'epoch', 'encoder'])
[20:03:54.041233] INFO: result.action_encoder:  ProprioceptiveEmbedding(
  (patch_embed): Conv1d(10, 10, kernel_size=(1,), stride=(1,))
)
[20:03:54.045047] INFO: result.keys():  dict_keys(['predictor', 'action_encoder', 'proprio_encoder', 'epoch', 'encoder'])
[20:03:54.048967] num_action_repeat: 1
[20:03:54.048988] num_proprio_repeat: 1
[20:03:54.049015] proprio encoder: ProprioceptiveEmbedding(
  (patch_embed): Conv1d(2, 10, kernel_size=(1,), stride=(1,))
)
[20:03:54.049034] action encoder: ProprioceptiveEmbedding(
  (patch_embed): Conv1d(10, 10, kernel_size=(1,), stride=(1,))
)
[20:03:54.049045] proprio_dim: 10, after repeat: 10
[20:03:54.049056] action_dim: 10, after repeat: 10
[20:03:54.049066] emb_dim: 404
[20:03:54.049079] Model emb_dim:  404
[20:03:54.049154] INFO: self.encoder_image_size:  224
[20:03:54.049231] INFO: VWorldModelDrop initialized with plan_num_clusters:  -1
[20:03:54.050826] INFO: drop_patches_idx:  tensor([65])
[20:03:54.051404] get_score_net  ltrp_cluster_vs
[20:03:55.463323] score net is ltrp_cluster(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (pre_logits): Identity()
  (head): Linear(in_features=384, out_features=196, bias=True)
)
[20:03:55.464383] INFO: self.score_net ltrp_cluster(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate=none)
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (pre_logits): Identity()
  (head): Linear(in_features=384, out_features=196, bias=True)
)
[20:03:55.465848] Total trainable parameters: 21665476
[20:03:55.507301] Model = LearnToRankPatchWM(
  (dino_wm): VWorldModelDrop(
    (encoder): DinoEncoder(
      (base_model): VisionTransformer(
        (patch_embed): PatchEmbed(
          (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
        )
        (pos_drop): Dropout(p=0.0, inplace=False)
        (blocks): ModuleList(
          (0): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (6): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (7): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (8): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (9): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (10): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (11): Block(
            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (head): Identity()
      )
    )
    (proprio_encoder): ProprioceptiveEmbedding(
      (patch_embed): Conv1d(2, 10, kernel_size=(1,), stride=(1,))
    )
    (action_encoder): ProprioceptiveEmbedding(
      (patch_embed): Conv1d(10, 10, kernel_size=(1,), stride=(1,))
    )
    (predictor): ViTPredictorWithoutPE(
      (dropout): Dropout(p=0, inplace=False)
      (transformer): Transformer(
        (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0): ModuleList(
            (0): Attention(
              (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
              (to_qkv): Linear(in_features=404, out_features=3072, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=1024, out_features=404, bias=True)
                (1): Dropout(p=0.1, inplace=False)
              )
            )
            (1): FeedForward(
              (net): Sequential(
                (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=404, out_features=2048, bias=True)
                (2): GELU(approximate=none)
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(in_features=2048, out_features=404, bias=True)
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (1): ModuleList(
            (0): Attention(
              (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
              (to_qkv): Linear(in_features=404, out_features=3072, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=1024, out_features=404, bias=True)
                (1): Dropout(p=0.1, inplace=False)
              )
            )
            (1): FeedForward(
              (net): Sequential(
                (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=404, out_features=2048, bias=True)
                (2): GELU(approximate=none)
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(in_features=2048, out_features=404, bias=True)
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (2): ModuleList(
            (0): Attention(
              (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
              (to_qkv): Linear(in_features=404, out_features=3072, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=1024, out_features=404, bias=True)
                (1): Dropout(p=0.1, inplace=False)
              )
            )
            (1): FeedForward(
              (net): Sequential(
                (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=404, out_features=2048, bias=True)
                (2): GELU(approximate=none)
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(in_features=2048, out_features=404, bias=True)
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (3): ModuleList(
            (0): Attention(
              (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
              (to_qkv): Linear(in_features=404, out_features=3072, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=1024, out_features=404, bias=True)
                (1): Dropout(p=0.1, inplace=False)
              )
            )
            (1): FeedForward(
              (net): Sequential(
                (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=404, out_features=2048, bias=True)
                (2): GELU(approximate=none)
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(in_features=2048, out_features=404, bias=True)
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (4): ModuleList(
            (0): Attention(
              (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
              (to_qkv): Linear(in_features=404, out_features=3072, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=1024, out_features=404, bias=True)
                (1): Dropout(p=0.1, inplace=False)
              )
            )
            (1): FeedForward(
              (net): Sequential(
                (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=404, out_features=2048, bias=True)
                (2): GELU(approximate=none)
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(in_features=2048, out_features=404, bias=True)
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
          (5): ModuleList(
            (0): Attention(
              (norm): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
              (attend): Softmax(dim=-1)
              (dropout): Dropout(p=0.1, inplace=False)
              (to_qkv): Linear(in_features=404, out_features=3072, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=1024, out_features=404, bias=True)
                (1): Dropout(p=0.1, inplace=False)
              )
            )
            (1): FeedForward(
              (net): Sequential(
                (0): LayerNorm((404,), eps=1e-05, elementwise_affine=True)
                (1): Linear(in_features=404, out_features=2048, bias=True)
                (2): GELU(approximate=none)
                (3): Dropout(p=0.1, inplace=False)
                (4): Linear(in_features=2048, out_features=404, bias=True)
                (5): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
      )
    )
    (decoder_criterion): MSELoss()
    (emb_criterion): MSELoss()
  )
  (score_net): ltrp_cluster(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pre_logits): Identity()
    (head): Linear(in_features=384, out_features=196, bias=True)
  )
  (criterion): list_mleEx()
)
[20:03:55.507399] base lr: 3.00e-04
[20:03:55.507412] actual lr: 6.00e-04
[20:03:55.507422] accumulate grad iterations: 1
[20:03:55.507433] effective batch size: 512
[20:03:55.507450] ngpus_per_node  4
[20:03:55.523758] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0006
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0006
    maximize: False
    weight_decay: 0.06
)
[20:03:55.524038] Start training for 400 epochs
[20:04:18.368875] INFO: N, T, L, D:  128 2 196 384
[20:04:18.369084] INFO: ids_shuffle.shape:  torch.Size([128, 196])
[20:04:18.369162] INFO: ids_shuffle_expanded.shape:  torch.Size([128, 2, 196, 384])
[20:04:18.369191] x[visual] shape: torch.Size([128, 2, 196, 384])
[20:04:18.371526] ids_shuffle_expanded min: 0 max: 195
[20:04:18.372771] INFO: z_dct[visual].shape:  torch.Size([128, 2, 196, 384])
[20:04:18.372807] INFO: z_dct[proprio].shape:  torch.Size([128, 2, 10])
[20:04:18.385848] INFO: latent.shape,  mask.shape, ids_restore.shape torch.Size([128, 1, 196, 404]) torch.Size([128, 196]) torch.Size([128, 196])
[20:04:18.386183] INFO: x.shape:  torch.Size([128, 3, 224, 224])
[20:04:18.528271] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:18.528639] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:18.528698] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:18.528743] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:18.535175] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:18.543995] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:18.544364] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:18.544398] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:18.544442] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:18.575033] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:18.673592] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:18.674849] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:18.674899] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:18.674961] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:18.727367] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:18.826937] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:18.828230] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:18.828264] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:18.828306] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:18.880944] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:18.980515] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:18.981847] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:18.981877] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:18.981911] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.034523] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:19.134103] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:19.135436] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:19.135467] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:19.135501] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.188210] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:19.289280] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:19.290627] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:19.290659] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:19.290692] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.345012] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:19.448067] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:19.449348] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:19.449401] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:19.449459] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.503774] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:19.604634] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:19.605952] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:19.605983] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:19.606023] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.658648] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:19.758318] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:19.759651] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:19.759682] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:19.759716] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.812533] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:19.912489] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:19.913763] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:19.913814] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:19.913875] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:19.966534] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.066608] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:20.067892] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:20.067930] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:20.067976] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:20.120675] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.220360] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:20.221650] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:20.221693] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:20.221762] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:20.274999] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.376935] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:20.378205] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:20.378241] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:20.378287] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:20.432650] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.535175] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:20.536480] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:20.536519] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:20.536568] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:20.590094] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.690710] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:20.692010] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:20.692047] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:20.692088] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:20.745028] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.844914] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:20.846219] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:20.846252] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:20.846292] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:20.898989] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:20.999515] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:21.000825] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:21.000857] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:21.000896] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:21.054079] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:21.154194] INFO: z_dct['visual'].shape:  torch.Size([128, 2, 196, 384])
[20:04:21.155504] INFO: z.shape in encodeEx:  torch.Size([128, 2, 196, 404])
[20:04:21.155538] INFO: _latent.shape: in forward_mask_decoder   torch.Size([128, 2, 196, 404])
[20:04:21.155576] INFO: z_src.shape in forwardEx:  torch.Size([128, 1, 196, 404])
[20:04:21.208254] INFO: anchor.shape, each.shape: in forward_mask_decoder   torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404])
[20:04:21.208417] INFO: anchor_each.shape, rm_pred.shape, mask.shape:  torch.Size([128, 1, 196, 404]) torch.Size([128, 1, 196, 404]) torch.Size([128, 196])
[20:04:21.208448] INFO: mask:  tensor([[False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        ...,
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False],
        [False, False, False,  ..., False, False, False]], device='cuda:0')
[20:04:21.461852] INFO: anchor_each.shape, rm_pred_each.shape:  torch.Size([0, 404]) torch.Size([0, 404])
[20:04:21.461941] INFO: score.shape:  torch.Size([0, 404])
[20:04:21.462017] INFO: score_after.shape:  torch.Size([1])
Traceback (most recent call last):
  File "main_ltrp.py", line 328, in <module>
    main(args)
  File "main_ltrp.py", line 286, in main
    train_stats = train_one_epoch(
  File "/home/s2/youngjoonjeong/github/ltrp/engine_pretrain.py", line 48, in train_one_epoch
    loss = model(obs, act, mask_ratio=args.mask_ratio)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 167, in forward
    y_pred, y_true = self.forward_mask_decoder(imgs, acts, mask_ratio)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 156, in forward_mask_decoder
    y_true = self.img_metric(anchor_preds, rm_preds, mask)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/utils/metric.py", line 26, in l1
    assert score.shape == (anchor_each.shape[0], 1)
AssertionError
Traceback (most recent call last):
  File "main_ltrp.py", line 328, in <module>
    main(args)
  File "main_ltrp.py", line 286, in main
    train_stats = train_one_epoch(
  File "/home/s2/youngjoonjeong/github/ltrp/engine_pretrain.py", line 48, in train_one_epoch
    loss = model(obs, act, mask_ratio=args.mask_ratio)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 167, in forward
    y_pred, y_true = self.forward_mask_decoder(imgs, acts, mask_ratio)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 156, in forward_mask_decoder
    y_true = self.img_metric(anchor_preds, rm_preds, mask)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/utils/metric.py", line 26, in l1
    assert score.shape == (anchor_each.shape[0], 1)
AssertionError
Traceback (most recent call last):
  File "main_ltrp.py", line 328, in <module>
    main(args)
  File "main_ltrp.py", line 286, in main
    train_stats = train_one_epoch(
  File "/home/s2/youngjoonjeong/github/ltrp/engine_pretrain.py", line 48, in train_one_epoch
    loss = model(obs, act, mask_ratio=args.mask_ratio)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 167, in forward
    y_pred, y_true = self.forward_mask_decoder(imgs, acts, mask_ratio)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 156, in forward_mask_decoder
    y_true = self.img_metric(anchor_preds, rm_preds, mask)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/utils/metric.py", line 26, in l1
    assert score.shape == (anchor_each.shape[0], 1)
AssertionError
Traceback (most recent call last):
  File "main_ltrp.py", line 328, in <module>
    main(args)
  File "main_ltrp.py", line 286, in main
    train_stats = train_one_epoch(
  File "/home/s2/youngjoonjeong/github/ltrp/engine_pretrain.py", line 48, in train_one_epoch
    loss = model(obs, act, mask_ratio=args.mask_ratio)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 1008, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/parallel/distributed.py", line 969, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 167, in forward
    y_pred, y_true = self.forward_mask_decoder(imgs, acts, mask_ratio)
  File "/home/s2/youngjoonjeong/github/ltrp/models_ltrp.py", line 156, in forward_mask_decoder
    y_true = self.img_metric(anchor_preds, rm_preds, mask)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/s2/youngjoonjeong/github/ltrp/utils/metric.py", line 26, in l1
    assert score.shape == (anchor_each.shape[0], 1)
AssertionError
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 482131 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 482130) of binary: /home/s2/youngjoonjeong/anaconda3/envs/LTRP/bin/python
Traceback (most recent call last):
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py", line 193, in <module>
    main()
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py", line 189, in main
    launch(args)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launch.py", line 174, in launch
    run(args)
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/run.py", line 752, in run
    elastic_launch(
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/s2/youngjoonjeong/anaconda3/envs/LTRP/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_ltrp.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-04-14_20:04:27
  host      : b06
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 482132)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2025-04-14_20:04:27
  host      : b06
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 482133)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-14_20:04:27
  host      : b06
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 482130)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
